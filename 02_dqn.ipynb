{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from algorithms.deeprl.dqn.agent import DQN\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQN(env=env,\n",
    "            double=True,\n",
    "            duelling=True, \n",
    "            activation=F.relu, \n",
    "            optimizer=optim.RMSprop, \n",
    "            alpha=0.0001, \n",
    "            gamma=0.99, \n",
    "            epsilon_start=1, \n",
    "            epsilon_end=0.05, \n",
    "            epsilon_decay=0.000035,\n",
    "            tau=0.001,\n",
    "            memory_alpha=0.6,\n",
    "            memory_beta=0.1,\n",
    "            memory_beta_increment=0.00001,\n",
    "            memory_epsilon=0.00001,\n",
    "            max_memory_size=50000, \n",
    "            batch_size=64,\n",
    "            max_episodes=1000,\n",
    "            warmup=100,\n",
    "            replace_steps=1,\n",
    "            seed=1,\n",
    "            log=True,\n",
    "            dir='tmp', \n",
    "            name='LunarLander-DQN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "Episode: 100\n",
      "Step: 11184\n",
      "Evaluation Reward: -144.70303747710213\n",
      "Best Evaluation Reward: -71.96106489235407\n",
      "Train Mean: -158.32771693662028\n",
      "Eval Mean: -385.1191301778126\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 101\n",
      "Step: 11325\n",
      "Evaluation Reward: -109.85383112398502\n",
      "Best Evaluation Reward: -71.96106489235407\n",
      "Train Mean: -159.65660331222202\n",
      "Eval Mean: -376.86072090677766\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 102\n",
      "Step: 11442\n",
      "Evaluation Reward: -121.9680539714137\n",
      "Best Evaluation Reward: -71.96106489235407\n",
      "Train Mean: -158.3686356256125\n",
      "Eval Mean: -373.9989900607316\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 103\n",
      "Step: 11557\n",
      "Evaluation Reward: -183.28124323288665\n",
      "Best Evaluation Reward: -71.96106489235407\n",
      "Train Mean: -157.44604434901967\n",
      "Eval Mean: -367.12448777145056\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 104\n",
      "Step: 11730\n",
      "Evaluation Reward: -171.23914210320672\n",
      "Best Evaluation Reward: -71.96106489235407\n",
      "Train Mean: -157.94140881142397\n",
      "Eval Mean: -354.28534001964346\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 105\n",
      "Step: 11821\n",
      "Evaluation Reward: -147.7739228207132\n",
      "Best Evaluation Reward: -71.96106489235407\n",
      "Train Mean: -155.21933727434197\n",
      "Eval Mean: -347.12965409372896\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 106\n",
      "Step: 12042\n",
      "Evaluation Reward: -171.1956028161217\n",
      "Best Evaluation Reward: -71.96106489235407\n",
      "Train Mean: -154.68781972094928\n",
      "Eval Mean: -334.2123024456674\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 107\n",
      "Step: 12251\n",
      "Evaluation Reward: -161.68984858359124\n",
      "Best Evaluation Reward: -71.96106489235407\n",
      "Train Mean: -150.96505668625895\n",
      "Eval Mean: -330.8126041669913\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 108\n",
      "Step: 12375\n",
      "Evaluation Reward: -171.79776604684622\n",
      "Best Evaluation Reward: -71.96106489235407\n",
      "Train Mean: -148.56066337034653\n",
      "Eval Mean: -326.5679466595733\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 109\n",
      "Step: 12594\n",
      "Evaluation Reward: -273.5779401164126\n",
      "Best Evaluation Reward: -71.96106489235407\n",
      "Train Mean: -147.1467088042182\n",
      "Eval Mean: -325.9363831342321\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 110\n",
      "Step: 12758\n",
      "Evaluation Reward: -143.56575060340398\n",
      "Best Evaluation Reward: -71.96106489235407\n",
      "Train Mean: -146.21319301204963\n",
      "Eval Mean: -323.5775034213217\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 111\n",
      "Step: 12878\n",
      "Evaluation Reward: -137.94056398192228\n",
      "Best Evaluation Reward: -71.96106489235407\n",
      "Train Mean: -142.86006789348528\n",
      "Eval Mean: -319.5585808700299\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 112\n",
      "Step: 13072\n",
      "Evaluation Reward: -226.69365685572296\n",
      "Best Evaluation Reward: -71.96106489235407\n",
      "Train Mean: -142.13087511023716\n",
      "Eval Mean: -315.56497673532334\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 113\n",
      "Step: 13260\n",
      "Evaluation Reward: -147.96421834362636\n",
      "Best Evaluation Reward: -71.96106489235407\n",
      "Train Mean: -141.69013344575052\n",
      "Eval Mean: -311.6940013506939\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 114\n",
      "Step: 13439\n",
      "Evaluation Reward: -229.40621082864808\n",
      "Best Evaluation Reward: -71.96106489235407\n",
      "Train Mean: -139.7704785490819\n",
      "Eval Mean: -308.8078265254269\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 115\n",
      "Step: 14144\n",
      "Evaluation Reward: -356.6620677158108\n",
      "Best Evaluation Reward: -71.96106489235407\n",
      "Train Mean: -139.71324894038543\n",
      "Eval Mean: -308.66687991874113\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 116\n",
      "Step: 14489\n",
      "Evaluation Reward: -262.39051945232575\n",
      "Best Evaluation Reward: -71.96106489235407\n",
      "Train Mean: -138.79262498501967\n",
      "Eval Mean: -308.67191394545057\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 117\n",
      "Step: 14604\n",
      "Evaluation Reward: -308.3972351740664\n",
      "Best Evaluation Reward: -71.96106489235407\n",
      "Train Mean: -138.23942812596837\n",
      "Eval Mean: -309.77921437929\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 118\n",
      "Step: 14968\n",
      "Evaluation Reward: -157.89981858061753\n",
      "Best Evaluation Reward: -71.96106489235407\n",
      "Train Mean: -139.0084984540724\n",
      "Eval Mean: -308.2012966838761\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 119\n",
      "Step: 15067\n",
      "Evaluation Reward: -358.126960347903\n",
      "Best Evaluation Reward: -71.96106489235407\n",
      "Train Mean: -137.0476502157547\n",
      "Eval Mean: -306.6291811697194\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 120\n",
      "Step: 15180\n",
      "Evaluation Reward: -243.0662471088712\n",
      "Best Evaluation Reward: -71.96106489235407\n",
      "Train Mean: -134.60699971722588\n",
      "Eval Mean: -304.310197548772\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 121\n",
      "Step: 15293\n",
      "Evaluation Reward: -204.8124242656531\n",
      "Best Evaluation Reward: -71.96106489235407\n",
      "Train Mean: -134.05837892030792\n",
      "Eval Mean: -299.60892672257063\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 122\n",
      "Step: 15480\n",
      "Evaluation Reward: -177.97422029900824\n",
      "Best Evaluation Reward: -71.96106489235407\n",
      "Train Mean: -135.16992410097845\n",
      "Eval Mean: -294.349309826269\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 123\n",
      "Step: 15687\n",
      "Evaluation Reward: -121.67881924228735\n",
      "Best Evaluation Reward: -71.96106489235407\n",
      "Train Mean: -131.58824977044443\n",
      "Eval Mean: -289.56166631842785\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 124\n",
      "Step: 16132\n",
      "Evaluation Reward: -359.00913243622966\n",
      "Best Evaluation Reward: -71.96106489235407\n",
      "Train Mean: -130.87289158776358\n",
      "Eval Mean: -289.6817743275644\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 125\n",
      "Step: 16288\n",
      "Evaluation Reward: -206.0499918409095\n",
      "Best Evaluation Reward: -71.96106489235407\n",
      "Train Mean: -129.11108377426618\n",
      "Eval Mean: -285.9965447864683\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 126\n",
      "Step: 16478\n",
      "Evaluation Reward: -275.8378782463997\n",
      "Best Evaluation Reward: -71.96106489235407\n",
      "Train Mean: -126.58762397225395\n",
      "Eval Mean: -286.0482635761952\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 127\n",
      "Step: 16948\n",
      "Evaluation Reward: -72.68019120136226\n",
      "Best Evaluation Reward: -71.96106489235407\n",
      "Train Mean: -127.91304532884597\n",
      "Eval Mean: -282.75939322966\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 128\n",
      "Step: 17188\n",
      "Evaluation Reward: -124.03424743471413\n",
      "Best Evaluation Reward: -71.96106489235407\n",
      "Train Mean: -125.63174765313357\n",
      "Eval Mean: -281.77539201785277\n",
      "--------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "Episode: 129\n",
      "Step: 17579\n",
      "Evaluation Reward: -169.1963345554289\n",
      "Best Evaluation Reward: -71.96106489235407\n",
      "Train Mean: -126.28661453977226\n",
      "Eval Mean: -281.0405412442119\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 130\n",
      "Step: 17713\n",
      "Evaluation Reward: -222.55283223808385\n",
      "Best Evaluation Reward: -71.96106489235407\n",
      "Train Mean: -125.63861581555737\n",
      "Eval Mean: -279.6265765642427\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 131\n",
      "Step: 17918\n",
      "Evaluation Reward: -301.3007509735818\n",
      "Best Evaluation Reward: -71.96106489235407\n",
      "Train Mean: -123.97940139926096\n",
      "Eval Mean: -279.85736331195346\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 132\n",
      "Step: 18115\n",
      "Evaluation Reward: -315.9049824286585\n",
      "Best Evaluation Reward: -71.96106489235407\n",
      "Train Mean: -124.5788276692802\n",
      "Eval Mean: -281.19870281533343\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 133\n",
      "Step: 18386\n",
      "Evaluation Reward: -215.32240569120978\n",
      "Best Evaluation Reward: -71.96106489235407\n",
      "Train Mean: -124.79008722890904\n",
      "Eval Mean: -279.5698768975425\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 134\n",
      "Step: 18784\n",
      "Evaluation Reward: -178.132051360471\n",
      "Best Evaluation Reward: -71.96106489235407\n",
      "Train Mean: -125.16011055310244\n",
      "Eval Mean: -276.6536215366726\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 135\n",
      "Step: 19293\n",
      "Evaluation Reward: -165.37576707819065\n",
      "Best Evaluation Reward: -71.96106489235407\n",
      "Train Mean: -126.35622414407653\n",
      "Eval Mean: -275.47516125957304\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 136\n",
      "Step: 19971\n",
      "Evaluation Reward: -232.051677463959\n",
      "Best Evaluation Reward: -71.96106489235407\n",
      "Train Mean: -126.86923196260125\n",
      "Eval Mean: -275.2468555519426\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 137\n",
      "Step: 20218\n",
      "Evaluation Reward: -179.07697776637804\n",
      "Best Evaluation Reward: -71.96106489235407\n",
      "Train Mean: -126.54726580423015\n",
      "Eval Mean: -275.02666321536884\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 138\n",
      "Step: 21218\n",
      "Evaluation Reward: -150.7727124074043\n",
      "Best Evaluation Reward: -71.96106489235407\n",
      "Train Mean: -125.53203114829715\n",
      "Eval Mean: -273.5863533979332\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 139\n",
      "Step: 22218\n",
      "Evaluation Reward: -88.04104646052666\n",
      "Best Evaluation Reward: -71.96106489235407\n",
      "Train Mean: -124.04897293587977\n",
      "Eval Mean: -271.6444569815769\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 140\n",
      "Step: 23218\n",
      "Evaluation Reward: -99.5394697207003\n",
      "Best Evaluation Reward: -71.96106489235407\n",
      "Train Mean: -121.74621635844045\n",
      "Eval Mean: -267.5916982037959\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 141\n",
      "Step: 24218\n",
      "Evaluation Reward: -49.486316994559374\n",
      "Best Evaluation Reward: -49.486316994559374\n",
      "Train Mean: -119.8952477148972\n",
      "Eval Mean: -265.1867435336637\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 142\n",
      "Step: 25218\n",
      "Evaluation Reward: -84.35240053128429\n",
      "Best Evaluation Reward: -49.486316994559374\n",
      "Train Mean: -119.15608967920357\n",
      "Eval Mean: -263.4442169209185\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 143\n",
      "Step: 26144\n",
      "Evaluation Reward: -153.2865175743731\n",
      "Best Evaluation Reward: -49.486316994559374\n",
      "Train Mean: -120.74324201821348\n",
      "Eval Mean: -259.4463188888573\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 144\n",
      "Step: 27144\n",
      "Evaluation Reward: -72.43090679753881\n",
      "Best Evaluation Reward: -49.486316994559374\n",
      "Train Mean: -119.80033734279377\n",
      "Eval Mean: -259.04885627914086\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 145\n",
      "Step: 28144\n",
      "Evaluation Reward: -411.6254027094455\n",
      "Best Evaluation Reward: -49.486316994559374\n",
      "Train Mean: -118.3936873144137\n",
      "Eval Mean: -259.90182094575863\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 146\n",
      "Step: 29144\n",
      "Evaluation Reward: -13.68393648899503\n",
      "Best Evaluation Reward: -13.68393648899503\n",
      "Train Mean: -117.5968738651355\n",
      "Eval Mean: -254.90201018476347\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 147\n",
      "Step: 30144\n",
      "Evaluation Reward: -235.45542051482659\n",
      "Best Evaluation Reward: -13.68393648899503\n",
      "Train Mean: -117.56666504422415\n",
      "Eval Mean: -252.17680586055397\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 148\n",
      "Step: 31144\n",
      "Evaluation Reward: -222.0800178490787\n",
      "Best Evaluation Reward: -13.68393648899503\n",
      "Train Mean: -116.19476074094823\n",
      "Eval Mean: -251.22248501922343\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 149\n",
      "Step: 32058\n",
      "Evaluation Reward: -12.040977615253512\n",
      "Best Evaluation Reward: -12.040977615253512\n",
      "Train Mean: -114.48132698104455\n",
      "Eval Mean: -247.65078370496255\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 150\n",
      "Step: 33058\n",
      "Evaluation Reward: 104.5627571005023\n",
      "Best Evaluation Reward: 104.5627571005023\n",
      "Train Mean: -112.8175088308307\n",
      "Eval Mean: -240.9840171967641\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 151\n",
      "Step: 34058\n",
      "Evaluation Reward: -24.987810565491003\n",
      "Best Evaluation Reward: 104.5627571005023\n",
      "Train Mean: -111.83210098182404\n",
      "Eval Mean: -237.78580094136328\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 152\n",
      "Step: 35058\n",
      "Evaluation Reward: -445.17185551970925\n",
      "Best Evaluation Reward: 104.5627571005023\n",
      "Train Mean: -111.2373572580368\n",
      "Eval Mean: -239.01603644164967\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 153\n",
      "Step: 35439\n",
      "Evaluation Reward: -116.88773922895183\n",
      "Best Evaluation Reward: 104.5627571005023\n",
      "Train Mean: -112.51083522625642\n",
      "Eval Mean: -237.3539485746225\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 154\n",
      "Step: 36439\n",
      "Evaluation Reward: -237.30770920617718\n",
      "Best Evaluation Reward: 104.5627571005023\n",
      "Train Mean: -111.53056532966185\n",
      "Eval Mean: -232.26173869753222\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 155\n",
      "Step: 37058\n",
      "Evaluation Reward: -243.66737173322704\n",
      "Best Evaluation Reward: 104.5627571005023\n",
      "Train Mean: -111.845627343049\n",
      "Eval Mean: -227.1517143114288\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 156\n",
      "Step: 37639\n",
      "Evaluation Reward: -10.10975809991228\n",
      "Best Evaluation Reward: 104.5627571005023\n",
      "Train Mean: -112.47508547109457\n",
      "Eval Mean: -222.99284214549073\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 157\n",
      "Step: 38010\n",
      "Evaluation Reward: -16.365241355052966\n",
      "Best Evaluation Reward: 104.5627571005023\n",
      "Train Mean: -111.68138450464788\n",
      "Eval Mean: -217.19340431785125\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 158\n",
      "Step: 38449\n",
      "Evaluation Reward: -11.44937315191356\n",
      "Best Evaluation Reward: 104.5627571005023\n",
      "Train Mean: -111.07154590584094\n",
      "Eval Mean: -210.78053500539892\n",
      "--------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "Episode: 159\n",
      "Step: 39449\n",
      "Evaluation Reward: -157.90143220965672\n",
      "Best Evaluation Reward: 104.5627571005023\n",
      "Train Mean: -110.74706576791331\n",
      "Eval Mean: -207.99760235096892\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 160\n",
      "Step: 40207\n",
      "Evaluation Reward: -241.38440224987616\n",
      "Best Evaluation Reward: 104.5627571005023\n",
      "Train Mean: -113.3858470213634\n",
      "Eval Mean: -206.94340990561068\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 161\n",
      "Step: 41207\n",
      "Evaluation Reward: -19.462570915741967\n",
      "Best Evaluation Reward: 104.5627571005023\n",
      "Train Mean: -112.78755252771597\n",
      "Eval Mean: -203.08823869625877\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 162\n",
      "Step: 41682\n",
      "Evaluation Reward: -116.81507526031704\n",
      "Best Evaluation Reward: 104.5627571005023\n",
      "Train Mean: -111.06768948069926\n",
      "Eval Mean: -197.0949230061526\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 163\n",
      "Step: 42172\n",
      "Evaluation Reward: -70.91659166485573\n",
      "Best Evaluation Reward: 104.5627571005023\n",
      "Train Mean: -110.77493714959186\n",
      "Eval Mean: -194.72170490148525\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 164\n",
      "Step: 42493\n",
      "Evaluation Reward: -21.697036773420695\n",
      "Best Evaluation Reward: 104.5627571005023\n",
      "Train Mean: -110.95291133201876\n",
      "Eval Mean: -190.80536572426954\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 165\n",
      "Step: 43493\n",
      "Evaluation Reward: -20.982426840466648\n",
      "Best Evaluation Reward: 104.5627571005023\n",
      "Train Mean: -110.81702281542154\n",
      "Eval Mean: -188.16044709666016\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 166\n",
      "Step: 44493\n",
      "Evaluation Reward: -34.549132879126546\n",
      "Best Evaluation Reward: 104.5627571005023\n",
      "Train Mean: -110.22444504349012\n",
      "Eval Mean: -181.84126484851456\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 167\n",
      "Step: 44736\n",
      "Evaluation Reward: -132.4012987318727\n",
      "Best Evaluation Reward: 104.5627571005023\n",
      "Train Mean: -108.9799365041034\n",
      "Eval Mean: -180.20316761996358\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 168\n",
      "Step: 45725\n",
      "Evaluation Reward: -109.99680715624557\n",
      "Best Evaluation Reward: 104.5627571005023\n",
      "Train Mean: -110.41119584037408\n",
      "Eval Mean: -178.06136756028656\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 169\n",
      "Step: 45956\n",
      "Evaluation Reward: -73.18661117036683\n",
      "Best Evaluation Reward: 104.5627571005023\n",
      "Train Mean: -110.79002751865688\n",
      "Eval Mean: -175.12521921826556\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 170\n",
      "Step: 46956\n",
      "Evaluation Reward: -16.154439611823747\n",
      "Best Evaluation Reward: 104.5627571005023\n",
      "Train Mean: -107.99732502533762\n",
      "Eval Mean: -171.03108431539033\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 171\n",
      "Step: 47173\n",
      "Evaluation Reward: -112.01111835435191\n",
      "Best Evaluation Reward: 104.5627571005023\n",
      "Train Mean: -107.53038895325852\n",
      "Eval Mean: -168.96418409425505\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 172\n",
      "Step: 48173\n",
      "Evaluation Reward: -98.9200806769831\n",
      "Best Evaluation Reward: 104.5627571005023\n",
      "Train Mean: -107.03166823835798\n",
      "Eval Mean: -168.34451511511332\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 173\n",
      "Step: 49173\n",
      "Evaluation Reward: -41.25157357554953\n",
      "Best Evaluation Reward: 104.5627571005023\n",
      "Train Mean: -105.96528418247986\n",
      "Eval Mean: -165.6654606231006\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 174\n",
      "Step: 49910\n",
      "Evaluation Reward: 31.73125428059126\n",
      "Best Evaluation Reward: 104.5627571005023\n",
      "Train Mean: -105.884041280018\n",
      "Eval Mean: -161.6200932063092\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 175\n",
      "Step: 50910\n",
      "Evaluation Reward: -36.502503447355444\n",
      "Best Evaluation Reward: 104.5627571005023\n",
      "Train Mean: -103.81923802849464\n",
      "Eval Mean: -160.22933876918916\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 176\n",
      "Step: 51025\n",
      "Evaluation Reward: -55.500775880199065\n",
      "Best Evaluation Reward: 104.5627571005023\n",
      "Train Mean: -103.46617865914764\n",
      "Eval Mean: -158.90604286773282\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 177\n",
      "Step: 52025\n",
      "Evaluation Reward: 122.26573421374623\n",
      "Best Evaluation Reward: 122.26573421374623\n",
      "Train Mean: -103.34322797452724\n",
      "Eval Mean: -155.7440798561495\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 178\n",
      "Step: 53025\n",
      "Evaluation Reward: 104.4287095399004\n",
      "Best Evaluation Reward: 122.26573421374623\n",
      "Train Mean: -101.62864796470144\n",
      "Eval Mean: -153.39313307381946\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 179\n",
      "Step: 54025\n",
      "Evaluation Reward: 16.1148024726823\n",
      "Best Evaluation Reward: 122.26573421374623\n",
      "Train Mean: -100.56667185324427\n",
      "Eval Mean: -151.64934358276386\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 180\n",
      "Step: 55025\n",
      "Evaluation Reward: -15.926497104334373\n",
      "Best Evaluation Reward: 122.26573421374623\n",
      "Train Mean: -99.91836026439623\n",
      "Eval Mean: -149.18502994314412\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 181\n",
      "Step: 56025\n",
      "Evaluation Reward: -44.004304371667274\n",
      "Best Evaluation Reward: 122.26573421374623\n",
      "Train Mean: -98.04663816657185\n",
      "Eval Mean: -148.1320310352492\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 182\n",
      "Step: 57025\n",
      "Evaluation Reward: 12.488655574820918\n",
      "Best Evaluation Reward: 122.26573421374623\n",
      "Train Mean: -96.59339261174968\n",
      "Eval Mean: -146.03102647846245\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 183\n",
      "Step: 58025\n",
      "Evaluation Reward: 12.620796357735838\n",
      "Best Evaluation Reward: 122.26573421374623\n",
      "Train Mean: -94.96824409684237\n",
      "Eval Mean: -142.58311117836263\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 184\n",
      "Step: 59025\n",
      "Evaluation Reward: -20.184528716593796\n",
      "Best Evaluation Reward: 122.26573421374623\n",
      "Train Mean: -93.21363674679877\n",
      "Eval Mean: -140.96424708505666\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 185\n",
      "Step: 60025\n",
      "Evaluation Reward: 4.771475693431657\n",
      "Best Evaluation Reward: 122.26573421374623\n",
      "Train Mean: -92.99875524078203\n",
      "Eval Mean: -139.2293679911948\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 186\n",
      "Step: 61025\n",
      "Evaluation Reward: -14.901223112614954\n",
      "Best Evaluation Reward: 122.26573421374623\n",
      "Train Mean: -90.21962802429081\n",
      "Eval Mean: -137.91102085610362\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 187\n",
      "Step: 62025\n",
      "Evaluation Reward: -52.006993081453864\n",
      "Best Evaluation Reward: 122.26573421374623\n",
      "Train Mean: -89.80404757317734\n",
      "Eval Mean: -136.68910970579608\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 188\n",
      "Step: 63025\n",
      "Evaluation Reward: 26.003710566273757\n",
      "Best Evaluation Reward: 122.26573421374623\n",
      "Train Mean: -88.63993343040515\n",
      "Eval Mean: -134.84192445150927\n",
      "--------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "Episode: 189\n",
      "Step: 64025\n",
      "Evaluation Reward: -19.052304164873576\n",
      "Best Evaluation Reward: 122.26573421374623\n",
      "Train Mean: -87.59053819859896\n",
      "Eval Mean: -132.62130683390222\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 190\n",
      "Step: 65025\n",
      "Evaluation Reward: -28.24188131958605\n",
      "Best Evaluation Reward: 122.26573421374623\n",
      "Train Mean: -87.1931688670405\n",
      "Eval Mean: -130.78446779300575\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 191\n",
      "Step: 66025\n",
      "Evaluation Reward: -30.310160760745198\n",
      "Best Evaluation Reward: 122.26573421374623\n",
      "Train Mean: -87.05926286220436\n",
      "Eval Mean: -128.6365414073122\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 192\n",
      "Step: 67025\n",
      "Evaluation Reward: -42.33799856177611\n",
      "Best Evaluation Reward: 122.26573421374623\n",
      "Train Mean: -86.12773694124002\n",
      "Eval Mean: -128.3403107440064\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 193\n",
      "Step: 68025\n",
      "Evaluation Reward: -45.084568577593416\n",
      "Best Evaluation Reward: 122.26573421374623\n",
      "Train Mean: -84.95958117945382\n",
      "Eval Mean: -127.54401149540615\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 194\n",
      "Step: 69025\n",
      "Evaluation Reward: 184.09259578522293\n",
      "Best Evaluation Reward: 184.09259578522293\n",
      "Train Mean: -81.81785891111275\n",
      "Eval Mean: -124.01958065263472\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 195\n",
      "Step: 70025\n",
      "Evaluation Reward: -37.35676267585678\n",
      "Best Evaluation Reward: 184.09259578522293\n",
      "Train Mean: -80.83440917312132\n",
      "Eval Mean: -122.248842022643\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 196\n",
      "Step: 71025\n",
      "Evaluation Reward: -19.068150674251\n",
      "Best Evaluation Reward: 184.09259578522293\n",
      "Train Mean: -79.34584004357238\n",
      "Eval Mean: -120.14717912531343\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 197\n",
      "Step: 72025\n",
      "Evaluation Reward: 14.247730867535662\n",
      "Best Evaluation Reward: 184.09259578522293\n",
      "Train Mean: -78.54135229946205\n",
      "Eval Mean: -118.38252647825816\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 198\n",
      "Step: 73025\n",
      "Evaluation Reward: -35.84834267158485\n",
      "Best Evaluation Reward: 184.09259578522293\n",
      "Train Mean: -76.91581482590576\n",
      "Eval Mean: -117.66257315892148\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 199\n",
      "Step: 74025\n",
      "Evaluation Reward: 196.6595878558425\n",
      "Best Evaluation Reward: 196.6595878558425\n",
      "Train Mean: -75.83298400846097\n",
      "Eval Mean: -113.82896089215001\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 200\n",
      "Step: 75025\n",
      "Evaluation Reward: -6.433088270865632\n",
      "Best Evaluation Reward: 196.6595878558425\n",
      "Train Mean: -73.90076484376074\n",
      "Eval Mean: -112.44626140008764\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 201\n",
      "Step: 76025\n",
      "Evaluation Reward: 40.38584401563472\n",
      "Best Evaluation Reward: 196.6595878558425\n",
      "Train Mean: -70.83719945325386\n",
      "Eval Mean: -110.94386464869142\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 202\n",
      "Step: 77025\n",
      "Evaluation Reward: -26.865635080645575\n",
      "Best Evaluation Reward: 196.6595878558425\n",
      "Train Mean: -70.58613748230522\n",
      "Eval Mean: -109.99284045978375\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 203\n",
      "Step: 78025\n",
      "Evaluation Reward: 52.897944501119454\n",
      "Best Evaluation Reward: 196.6595878558425\n",
      "Train Mean: -69.84698352114442\n",
      "Eval Mean: -107.63104858244367\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 204\n",
      "Step: 79025\n",
      "Evaluation Reward: -47.77847201213348\n",
      "Best Evaluation Reward: 196.6595878558425\n",
      "Train Mean: -68.29959327770864\n",
      "Eval Mean: -106.39644188153294\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 205\n",
      "Step: 80025\n",
      "Evaluation Reward: 27.217690636724175\n",
      "Best Evaluation Reward: 196.6595878558425\n",
      "Train Mean: -67.85051006035006\n",
      "Eval Mean: -104.64652574695857\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 206\n",
      "Step: 81025\n",
      "Evaluation Reward: -17.597733707438046\n",
      "Best Evaluation Reward: 196.6595878558425\n",
      "Train Mean: -67.17480822012467\n",
      "Eval Mean: -103.11054705587173\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 207\n",
      "Step: 82025\n",
      "Evaluation Reward: 25.189938406589988\n",
      "Best Evaluation Reward: 196.6595878558425\n",
      "Train Mean: -66.48616981925954\n",
      "Eval Mean: -101.24174918596995\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 208\n",
      "Step: 83025\n",
      "Evaluation Reward: -18.888080662510415\n",
      "Best Evaluation Reward: 196.6595878558425\n",
      "Train Mean: -65.95649877317271\n",
      "Eval Mean: -99.7126523321266\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 209\n",
      "Step: 84025\n",
      "Evaluation Reward: -3.2991437603185574\n",
      "Best Evaluation Reward: 196.6595878558425\n",
      "Train Mean: -65.14448257209091\n",
      "Eval Mean: -97.00986436856564\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 210\n",
      "Step: 85025\n",
      "Evaluation Reward: -28.436883241556227\n",
      "Best Evaluation Reward: 196.6595878558425\n",
      "Train Mean: -64.6487872492535\n",
      "Eval Mean: -95.85857569494718\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 211\n",
      "Step: 86025\n",
      "Evaluation Reward: -29.19728788672291\n",
      "Best Evaluation Reward: 196.6595878558425\n",
      "Train Mean: -64.01755388341036\n",
      "Eval Mean: -94.77114293399518\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 212\n",
      "Step: 87025\n",
      "Evaluation Reward: 215.75180173937287\n",
      "Best Evaluation Reward: 215.75180173937287\n",
      "Train Mean: -62.261574650879794\n",
      "Eval Mean: -90.34668834804422\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 213\n",
      "Step: 88025\n",
      "Evaluation Reward: -27.568617997986074\n",
      "Best Evaluation Reward: 215.75180173937287\n",
      "Train Mean: -61.592389472345594\n",
      "Eval Mean: -89.14273234458783\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 214\n",
      "Step: 89025\n",
      "Evaluation Reward: 141.34745837418234\n",
      "Best Evaluation Reward: 215.75180173937287\n",
      "Train Mean: -60.95763687319771\n",
      "Eval Mean: -85.43519565255953\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 215\n",
      "Step: 90025\n",
      "Evaluation Reward: -1.3010285138250146\n",
      "Best Evaluation Reward: 215.75180173937287\n",
      "Train Mean: -59.49867331238328\n",
      "Eval Mean: -81.88158526053964\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 216\n",
      "Step: 91025\n",
      "Evaluation Reward: 121.96276196029159\n",
      "Best Evaluation Reward: 215.75180173937287\n",
      "Train Mean: -59.51737964729173\n",
      "Eval Mean: -78.03805244641347\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 217\n",
      "Step: 91838\n",
      "Evaluation Reward: -17.036432527226108\n",
      "Best Evaluation Reward: 215.75180173937287\n",
      "Train Mean: -57.02816994003671\n",
      "Eval Mean: -75.12444441994506\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 218\n",
      "Step: 92838\n",
      "Evaluation Reward: -15.57407105392408\n",
      "Best Evaluation Reward: 215.75180173937287\n",
      "Train Mean: -55.27349745862384\n",
      "Eval Mean: -73.70118694467813\n",
      "--------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "Episode: 219\n",
      "Step: 93838\n",
      "Evaluation Reward: 232.91821230248215\n",
      "Best Evaluation Reward: 232.91821230248215\n",
      "Train Mean: -55.206868145251\n",
      "Eval Mean: -67.79073521817426\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 220\n",
      "Step: 94838\n",
      "Evaluation Reward: -30.630145276904756\n",
      "Best Evaluation Reward: 232.91821230248215\n",
      "Train Mean: -55.08362242545566\n",
      "Eval Mean: -65.66637419985462\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 221\n",
      "Step: 95838\n",
      "Evaluation Reward: -72.43948334597479\n",
      "Best Evaluation Reward: 232.91821230248215\n",
      "Train Mean: -54.802144228691674\n",
      "Eval Mean: -64.34264479065783\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 222\n",
      "Step: 96838\n",
      "Evaluation Reward: 3.3237183534939865\n",
      "Best Evaluation Reward: 232.91821230248215\n",
      "Train Mean: -53.00756301354874\n",
      "Eval Mean: -62.529665404132814\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 223\n",
      "Step: 97838\n",
      "Evaluation Reward: -31.471490664828572\n",
      "Best Evaluation Reward: 232.91821230248215\n",
      "Train Mean: -52.987011544918275\n",
      "Eval Mean: -61.62759211835822\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 224\n",
      "Step: 98838\n",
      "Evaluation Reward: -22.85977602162215\n",
      "Best Evaluation Reward: 232.91821230248215\n",
      "Train Mean: -52.60804495107746\n",
      "Eval Mean: -58.26609855421215\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 225\n",
      "Step: 99838\n",
      "Evaluation Reward: 276.75712299171096\n",
      "Best Evaluation Reward: 276.75712299171096\n",
      "Train Mean: -53.03802448788253\n",
      "Eval Mean: -53.43802740588595\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 226\n",
      "Step: 100838\n",
      "Evaluation Reward: -67.51376956447018\n",
      "Best Evaluation Reward: 276.75712299171096\n",
      "Train Mean: -52.594680031687275\n",
      "Eval Mean: -51.35478631906664\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 227\n",
      "Step: 101838\n",
      "Evaluation Reward: 14.243144502070187\n",
      "Best Evaluation Reward: 276.75712299171096\n",
      "Train Mean: -50.203237600340344\n",
      "Eval Mean: -50.48555296203232\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 228\n",
      "Step: 102838\n",
      "Evaluation Reward: 29.042366500197947\n",
      "Best Evaluation Reward: 276.75712299171096\n",
      "Train Mean: -49.785994415668625\n",
      "Eval Mean: -48.9547868226832\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 229\n",
      "Step: 103838\n",
      "Evaluation Reward: 276.6543860235513\n",
      "Best Evaluation Reward: 276.75712299171096\n",
      "Train Mean: -48.041489200046584\n",
      "Eval Mean: -44.496279616893396\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 230\n",
      "Step: 104581\n",
      "Evaluation Reward: -29.872233647479813\n",
      "Best Evaluation Reward: 276.75712299171096\n",
      "Train Mean: -46.30774029061411\n",
      "Eval Mean: -42.56947363098736\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 231\n",
      "Step: 105581\n",
      "Evaluation Reward: -17.353250340398645\n",
      "Best Evaluation Reward: 276.75712299171096\n",
      "Train Mean: -44.75013314377628\n",
      "Eval Mean: -39.72999862465552\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 232\n",
      "Step: 105667\n",
      "Evaluation Reward: 203.517149163469\n",
      "Best Evaluation Reward: 276.75712299171096\n",
      "Train Mean: -43.03029532188166\n",
      "Eval Mean: -34.535777308734254\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 233\n",
      "Step: 106667\n",
      "Evaluation Reward: -6.985243064356382\n",
      "Best Evaluation Reward: 276.75712299171096\n",
      "Train Mean: -41.90177916659673\n",
      "Eval Mean: -32.45240568246572\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 234\n",
      "Step: 107667\n",
      "Evaluation Reward: 11.304434803083211\n",
      "Best Evaluation Reward: 276.75712299171096\n",
      "Train Mean: -39.86988136135257\n",
      "Eval Mean: -30.55804082083018\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 235\n",
      "Step: 108667\n",
      "Evaluation Reward: -28.686874495937722\n",
      "Best Evaluation Reward: 276.75712299171096\n",
      "Train Mean: -38.66523843764855\n",
      "Eval Mean: -29.19115189500765\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 236\n",
      "Step: 109667\n",
      "Evaluation Reward: 22.893873505984573\n",
      "Best Evaluation Reward: 276.75712299171096\n",
      "Train Mean: -36.83201509299878\n",
      "Eval Mean: -26.641696385308215\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 237\n",
      "Step: 110667\n",
      "Evaluation Reward: 128.03692308422012\n",
      "Best Evaluation Reward: 276.75712299171096\n",
      "Train Mean: -36.467229319910736\n",
      "Eval Mean: -23.57055737680223\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 238\n",
      "Step: 111667\n",
      "Evaluation Reward: 56.85163469899326\n",
      "Best Evaluation Reward: 276.75712299171096\n",
      "Train Mean: -35.194352541578745\n",
      "Eval Mean: -21.494313905738252\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 239\n",
      "Step: 112667\n",
      "Evaluation Reward: 12.273475970083485\n",
      "Best Evaluation Reward: 276.75712299171096\n",
      "Train Mean: -34.68126021063135\n",
      "Eval Mean: -20.491168681432153\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 240\n",
      "Step: 113667\n",
      "Evaluation Reward: -27.810121073015768\n",
      "Best Evaluation Reward: 276.75712299171096\n",
      "Train Mean: -34.74395753297352\n",
      "Eval Mean: -19.773875194955306\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 241\n",
      "Step: 114667\n",
      "Evaluation Reward: 103.78125133611141\n",
      "Best Evaluation Reward: 276.75712299171096\n",
      "Train Mean: -34.09756700576484\n",
      "Eval Mean: -18.241199511648603\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 242\n",
      "Step: 115667\n",
      "Evaluation Reward: -23.595758986377206\n",
      "Best Evaluation Reward: 276.75712299171096\n",
      "Train Mean: -34.16178866339899\n",
      "Eval Mean: -17.633633096199528\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 243\n",
      "Step: 116667\n",
      "Evaluation Reward: 127.01929670619853\n",
      "Best Evaluation Reward: 276.75712299171096\n",
      "Train Mean: -31.459061208519785\n",
      "Eval Mean: -14.830574953393814\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 244\n",
      "Step: 117667\n",
      "Evaluation Reward: -50.96204487683192\n",
      "Best Evaluation Reward: 276.75712299171096\n",
      "Train Mean: -31.371226596281517\n",
      "Eval Mean: -14.615886334186742\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 245\n",
      "Step: 118633\n",
      "Evaluation Reward: 169.26183666266792\n",
      "Best Evaluation Reward: 276.75712299171096\n",
      "Train Mean: -29.139639140722934\n",
      "Eval Mean: -8.807013940465605\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 246\n",
      "Step: 119426\n",
      "Evaluation Reward: 199.11233526301413\n",
      "Best Evaluation Reward: 276.75712299171096\n",
      "Train Mean: -27.514397513478407\n",
      "Eval Mean: -6.679051222945516\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 247\n",
      "Step: 119838\n",
      "Evaluation Reward: -4.390079114235422\n",
      "Best Evaluation Reward: 276.75712299171096\n",
      "Train Mean: -24.194393977169007\n",
      "Eval Mean: -4.368397808939602\n",
      "--------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "Episode: 248\n",
      "Step: 120037\n",
      "Evaluation Reward: 211.78673645030673\n",
      "Best Evaluation Reward: 276.75712299171096\n",
      "Train Mean: -25.656083498066906\n",
      "Eval Mean: -0.029730265945749467\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 249\n",
      "Step: 120393\n",
      "Evaluation Reward: 178.62497822013435\n",
      "Best Evaluation Reward: 276.75712299171096\n",
      "Train Mean: -21.40446209465349\n",
      "Eval Mean: 1.8769292924081296\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 250\n",
      "Step: 121393\n",
      "Evaluation Reward: 68.04409296118823\n",
      "Best Evaluation Reward: 276.75712299171096\n",
      "Train Mean: -21.548080543222927\n",
      "Eval Mean: 1.5117426510149883\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 251\n",
      "Step: 122337\n",
      "Evaluation Reward: 211.87250172208917\n",
      "Best Evaluation Reward: 276.75712299171096\n",
      "Train Mean: -19.632608350762332\n",
      "Eval Mean: 3.880345773890791\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 252\n",
      "Step: 123309\n",
      "Evaluation Reward: 21.35037077063446\n",
      "Best Evaluation Reward: 276.75712299171096\n",
      "Train Mean: -17.60631946031732\n",
      "Eval Mean: 8.545568036794227\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 253\n",
      "Step: 123950\n",
      "Evaluation Reward: 142.32005475060498\n",
      "Best Evaluation Reward: 276.75712299171096\n",
      "Train Mean: -14.04946022309071\n",
      "Eval Mean: 11.137645976589795\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 254\n",
      "Step: 124115\n",
      "Evaluation Reward: 210.2909387807271\n",
      "Best Evaluation Reward: 276.75712299171096\n",
      "Train Mean: -11.810169375219953\n",
      "Eval Mean: 15.61363245645884\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 255\n",
      "Step: 124970\n",
      "Evaluation Reward: 168.8504383307212\n",
      "Best Evaluation Reward: 276.75712299171096\n",
      "Train Mean: -7.889355050465579\n",
      "Eval Mean: 19.738810557098322\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 256\n",
      "Step: 125526\n",
      "Evaluation Reward: -1.9697471804900109\n",
      "Best Evaluation Reward: 276.75712299171096\n",
      "Train Mean: -3.0509625155795392\n",
      "Eval Mean: 19.820210666292542\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 257\n",
      "Step: 126164\n",
      "Evaluation Reward: 284.8529503211321\n",
      "Best Evaluation Reward: 284.8529503211321\n",
      "Train Mean: 0.7875225780796435\n",
      "Eval Mean: 22.832392583054396\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 258\n",
      "Step: 126827\n",
      "Evaluation Reward: 221.24749733933953\n",
      "Best Evaluation Reward: 284.8529503211321\n",
      "Train Mean: 4.415703429334593\n",
      "Eval Mean: 25.159361287966927\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 259\n",
      "Step: 127454\n",
      "Evaluation Reward: 214.1811679634046\n",
      "Best Evaluation Reward: 284.8529503211321\n",
      "Train Mean: 6.52125998943125\n",
      "Eval Mean: 28.88018728969754\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 260\n",
      "Step: 128081\n",
      "Evaluation Reward: 285.1937045827882\n",
      "Best Evaluation Reward: 285.1937045827882\n",
      "Train Mean: 12.055419811808047\n",
      "Eval Mean: 34.145968358024184\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 261\n",
      "Step: 129058\n",
      "Evaluation Reward: -33.19473399258571\n",
      "Best Evaluation Reward: 285.1937045827882\n",
      "Train Mean: 16.162749911073824\n",
      "Eval Mean: 34.00864672725575\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 262\n",
      "Step: 129647\n",
      "Evaluation Reward: 223.5447120179474\n",
      "Best Evaluation Reward: 285.1937045827882\n",
      "Train Mean: 19.60744964681212\n",
      "Eval Mean: 37.41224460003839\n",
      "--------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9b6c7a54f83c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/000_projects/RLSimpleBaselines/algorithms/deeprl/dqn/agent.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0mreward_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarmup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecrease_epsilon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/000_projects/RLSimpleBaselines/algorithms/deeprl/dqn/agent.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_online\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;31m# if np.isnan(weights).any():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/000_projects/RLSimpleBaselines/venv/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/000_projects/RLSimpleBaselines/venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/000_projects/RLSimpleBaselines/venv/lib/python3.8/site-packages/torch/optim/rmsprop.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             F.rmsprop(params_with_grad,\n\u001b[0m\u001b[1;32m    107\u001b[0m                       \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                       \u001b[0msquare_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/000_projects/RLSimpleBaselines/venv/lib/python3.8/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36mrmsprop\u001b[0;34m(params, grads, square_avgs, grad_avgs, momentum_buffer_list, lr, alpha, eps, weight_decay, momentum, centered)\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msquare_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m             \u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msquare_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmomentum\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(agent.eval_rewards_mean)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
