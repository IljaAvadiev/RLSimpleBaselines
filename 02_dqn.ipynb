{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from algorithms.deeprl.dqn.agent import DQN\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQN(env=env,\n",
    "            double=True,\n",
    "            duelling=True, \n",
    "            activation=F.relu, \n",
    "            optimizer=optim.RMSprop, \n",
    "            alpha=0.0001, \n",
    "            gamma=0.99, \n",
    "            epsilon_start=1, \n",
    "            epsilon_end=0.05, \n",
    "            epsilon_decay=0.000035,\n",
    "            tau=0.001,\n",
    "            memory_alpha=0.6,\n",
    "            memory_beta=0.1,\n",
    "            memory_beta_increment=0.00001,\n",
    "            memory_epsilon=0.00001,\n",
    "            max_memory_size=50000, \n",
    "            batch_size=128,\n",
    "            max_episodes=1000,\n",
    "            warmup=200,\n",
    "            replace_steps=1,\n",
    "            seed=1,\n",
    "            log=True,\n",
    "            dir='tmp', \n",
    "            name='LunarLander-DQN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "Episode: 100\n",
      "Step: 10305\n",
      "Evaluation Reward: -236.1849112875006\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -139.67364343513782\n",
      "Eval Mean: -278.42577128605603\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 101\n",
      "Step: 10517\n",
      "Evaluation Reward: -182.44870852815615\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -139.57070574979608\n",
      "Eval Mean: -275.45816109864694\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 102\n",
      "Step: 10644\n",
      "Evaluation Reward: -197.48905130177494\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -139.93362953109144\n",
      "Eval Mean: -270.67421491756545\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 103\n",
      "Step: 10744\n",
      "Evaluation Reward: -186.66327612661053\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -137.93730201445157\n",
      "Eval Mean: -263.99947355961683\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 104\n",
      "Step: 10845\n",
      "Evaluation Reward: -163.92272910432422\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -137.13363829997184\n",
      "Eval Mean: -260.5762347069064\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 105\n",
      "Step: 10977\n",
      "Evaluation Reward: -160.3951182907329\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -136.27360460683016\n",
      "Eval Mean: -253.506133424987\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 106\n",
      "Step: 11072\n",
      "Evaluation Reward: -148.53644785075244\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -135.87442765765508\n",
      "Eval Mean: -250.04908126012793\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 107\n",
      "Step: 11207\n",
      "Evaluation Reward: -182.68369220468452\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -133.66850522438207\n",
      "Eval Mean: -247.688727259192\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 108\n",
      "Step: 11285\n",
      "Evaluation Reward: -139.72499983290038\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -133.26155224907146\n",
      "Eval Mean: -247.59406583123663\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 109\n",
      "Step: 11475\n",
      "Evaluation Reward: -116.99741637757225\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -132.96399814696727\n",
      "Eval Mean: -242.70136193889982\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 110\n",
      "Step: 11606\n",
      "Evaluation Reward: -113.53652591452521\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -131.60579543276745\n",
      "Eval Mean: -238.3292285569319\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 111\n",
      "Step: 11720\n",
      "Evaluation Reward: -85.45787092049554\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -130.84772836692846\n",
      "Eval Mean: -234.7217982245047\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 112\n",
      "Step: 11836\n",
      "Evaluation Reward: -126.95496646594636\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -129.5480958073424\n",
      "Eval Mean: -231.19894297785598\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 113\n",
      "Step: 11925\n",
      "Evaluation Reward: -133.52305364406396\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -128.29589932285472\n",
      "Eval Mean: -228.7384328675392\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 114\n",
      "Step: 12089\n",
      "Evaluation Reward: -217.14836014115951\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -125.9226732279324\n",
      "Eval Mean: -229.71196640504414\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 115\n",
      "Step: 12216\n",
      "Evaluation Reward: -150.26515184036242\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -124.51964768960414\n",
      "Eval Mean: -228.8911063075761\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 116\n",
      "Step: 12386\n",
      "Evaluation Reward: -129.16152616827947\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -122.95301938248627\n",
      "Eval Mean: -228.78364522779628\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 117\n",
      "Step: 12570\n",
      "Evaluation Reward: -148.92021043627744\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -122.89795858173379\n",
      "Eval Mean: -228.5872176698166\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 118\n",
      "Step: 12705\n",
      "Evaluation Reward: -161.08470393189072\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -123.68969389760171\n",
      "Eval Mean: -228.77920514463702\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 119\n",
      "Step: 12966\n",
      "Evaluation Reward: -83.88196993793984\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -125.15241937136761\n",
      "Eval Mean: -225.44043378349747\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 120\n",
      "Step: 13074\n",
      "Evaluation Reward: -210.0559382085226\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -124.63218567618333\n",
      "Eval Mean: -223.69543329112383\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 121\n",
      "Step: 13188\n",
      "Evaluation Reward: -166.6199391953461\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -123.8074277453904\n",
      "Eval Mean: -221.59720928905932\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 122\n",
      "Step: 13334\n",
      "Evaluation Reward: -160.67176680039518\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -125.50689281654473\n",
      "Eval Mean: -220.9115684370722\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 123\n",
      "Step: 13653\n",
      "Evaluation Reward: -137.3284167474149\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -121.9136049834325\n",
      "Eval Mean: -221.00931330883338\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 124\n",
      "Step: 13734\n",
      "Evaluation Reward: -117.29416777052643\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -121.20980275851389\n",
      "Eval Mean: -219.78594020258737\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 125\n",
      "Step: 13880\n",
      "Evaluation Reward: -136.04699920402697\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -120.66702099234136\n",
      "Eval Mean: -219.88669149259616\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 126\n",
      "Step: 14002\n",
      "Evaluation Reward: -150.9208719548796\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -117.90197147769095\n",
      "Eval Mean: -217.72953267518278\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 127\n",
      "Step: 14085\n",
      "Evaluation Reward: -190.5140816112714\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -117.29627045962519\n",
      "Eval Mean: -215.226527746126\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 128\n",
      "Step: 14529\n",
      "Evaluation Reward: -155.3963447833773\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -117.04806024194768\n",
      "Eval Mean: -215.0064451797161\n",
      "--------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "Episode: 129\n",
      "Step: 14722\n",
      "Evaluation Reward: -181.94597165016933\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -116.48869865835873\n",
      "Eval Mean: -215.07215637005808\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 130\n",
      "Step: 14797\n",
      "Evaluation Reward: -153.1475880423027\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -115.49182319174308\n",
      "Eval Mean: -213.69000378171546\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 131\n",
      "Step: 15264\n",
      "Evaluation Reward: -162.87449391931594\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -114.28040454514075\n",
      "Eval Mean: -212.46265137911325\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 132\n",
      "Step: 16264\n",
      "Evaluation Reward: -177.18479821896096\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -113.43501233221924\n",
      "Eval Mean: -213.37590296991013\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 133\n",
      "Step: 16387\n",
      "Evaluation Reward: -148.48548926039825\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -112.78928763475797\n",
      "Eval Mean: -211.50337025929778\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 134\n",
      "Step: 16608\n",
      "Evaluation Reward: -186.05602799085213\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -111.9369216369012\n",
      "Eval Mean: -211.28681993750638\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 135\n",
      "Step: 16872\n",
      "Evaluation Reward: -140.14556235334308\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -108.11671518606617\n",
      "Eval Mean: -206.77176771598974\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 136\n",
      "Step: 17099\n",
      "Evaluation Reward: -147.29759984064793\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -106.38345685000975\n",
      "Eval Mean: -202.62401376416844\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 137\n",
      "Step: 17409\n",
      "Evaluation Reward: -164.80934481528885\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -105.0036927249345\n",
      "Eval Mean: -198.8331196491199\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 138\n",
      "Step: 17876\n",
      "Evaluation Reward: -281.6351048805798\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -104.10975073286136\n",
      "Eval Mean: -199.89489417952507\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 139\n",
      "Step: 18876\n",
      "Evaluation Reward: -108.38378287769473\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -103.43406226378983\n",
      "Eval Mean: -195.90309672282805\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 140\n",
      "Step: 19723\n",
      "Evaluation Reward: -90.16233548505505\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -105.05666705656093\n",
      "Eval Mean: -194.49873901705683\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 141\n",
      "Step: 20114\n",
      "Evaluation Reward: -111.87402725286573\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -103.78667654335763\n",
      "Eval Mean: -188.78255030291587\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 142\n",
      "Step: 21114\n",
      "Evaluation Reward: -120.83729679382878\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -103.42710107332634\n",
      "Eval Mean: -186.4689827707235\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 143\n",
      "Step: 22114\n",
      "Evaluation Reward: -123.93625064873069\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -103.42775142388942\n",
      "Eval Mean: -185.0337486044773\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 144\n",
      "Step: 23114\n",
      "Evaluation Reward: -20.86781765914569\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -101.4087334131557\n",
      "Eval Mean: -183.1865516285391\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 145\n",
      "Step: 24114\n",
      "Evaluation Reward: -49.885897517161055\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -100.36250557112088\n",
      "Eval Mean: -181.2766557465512\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 146\n",
      "Step: 25114\n",
      "Evaluation Reward: -91.84780681782696\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -101.27556557868725\n",
      "Eval Mean: -180.53292713591188\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 147\n",
      "Step: 26114\n",
      "Evaluation Reward: -141.7225482394678\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -100.98371288809238\n",
      "Eval Mean: -176.73374158664407\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 148\n",
      "Step: 27114\n",
      "Evaluation Reward: -14.175755716849595\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -100.73297102159124\n",
      "Eval Mean: -172.23858620061904\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 149\n",
      "Step: 27548\n",
      "Evaluation Reward: -49.045119596785746\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -101.8439077498641\n",
      "Eval Mean: -170.7259376160513\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 150\n",
      "Step: 28548\n",
      "Evaluation Reward: -105.91096212154044\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -101.74980247637504\n",
      "Eval Mean: -169.39442796926184\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 151\n",
      "Step: 29548\n",
      "Evaluation Reward: -250.22406818266188\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -101.59191258071256\n",
      "Eval Mean: -169.0498222169244\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 152\n",
      "Step: 30548\n",
      "Evaluation Reward: -148.5910814592914\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -100.8266770412296\n",
      "Eval Mean: -168.22778310481502\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 153\n",
      "Step: 31548\n",
      "Evaluation Reward: -59.02978636941886\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -99.85563140473727\n",
      "Eval Mean: -166.23921689228527\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 154\n",
      "Step: 32253\n",
      "Evaluation Reward: -46.50309726278314\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -101.45553256656393\n",
      "Eval Mean: -165.50542089771955\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 155\n",
      "Step: 33253\n",
      "Evaluation Reward: -50.828313189268904\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -98.75556166721368\n",
      "Eval Mean: -164.1013314057866\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 156\n",
      "Step: 34253\n",
      "Evaluation Reward: -47.40380662228169\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -98.53083576279776\n",
      "Eval Mean: -162.13911030134636\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 157\n",
      "Step: 35253\n",
      "Evaluation Reward: -48.2571783842716\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -98.22756242153011\n",
      "Eval Mean: -159.28421512420712\n",
      "--------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "Episode: 158\n",
      "Step: 36253\n",
      "Evaluation Reward: -94.48192643760638\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -98.93196539871693\n",
      "Eval Mean: -157.9792624362289\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 159\n",
      "Step: 37253\n",
      "Evaluation Reward: -16.052394206277388\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -98.53473306501297\n",
      "Eval Mean: -154.20382985791215\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 160\n",
      "Step: 38253\n",
      "Evaluation Reward: -18.072535845710053\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -98.70595517856056\n",
      "Eval Mean: -152.60619411817433\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 161\n",
      "Step: 39253\n",
      "Evaluation Reward: -23.31583462735358\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -97.262925780529\n",
      "Eval Mean: -152.46435505386117\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 162\n",
      "Step: 40253\n",
      "Evaluation Reward: -30.654076496091694\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -95.93024930164988\n",
      "Eval Mean: -149.54644562947863\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 163\n",
      "Step: 41253\n",
      "Evaluation Reward: -45.70769182191394\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -95.4031493240852\n",
      "Eval Mean: -146.93195057153756\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 164\n",
      "Step: 42253\n",
      "Evaluation Reward: -22.080813517147448\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -93.15873355214829\n",
      "Eval Mean: -144.41873291684112\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 165\n",
      "Step: 43253\n",
      "Evaluation Reward: 12.541999093979138\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -91.5673226941358\n",
      "Eval Mean: -141.14615694374805\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 166\n",
      "Step: 44253\n",
      "Evaluation Reward: -27.98135055790294\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -90.56332649603002\n",
      "Eval Mean: -140.753772317766\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 167\n",
      "Step: 45253\n",
      "Evaluation Reward: -16.286822013372664\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -89.34742311618436\n",
      "Eval Mean: -137.5056206729006\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 168\n",
      "Step: 46253\n",
      "Evaluation Reward: -224.68522887837275\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -87.93725129460744\n",
      "Eval Mean: -137.60067240924207\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 169\n",
      "Step: 47253\n",
      "Evaluation Reward: -57.95093380300604\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -86.70287372625087\n",
      "Eval Mean: -135.09350618516044\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 170\n",
      "Step: 48253\n",
      "Evaluation Reward: -155.08877689198798\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -86.2088328414659\n",
      "Eval Mean: -135.30602485300892\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 171\n",
      "Step: 49253\n",
      "Evaluation Reward: -77.0055710012378\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -85.13442047179758\n",
      "Eval Mean: -133.3887049147982\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 172\n",
      "Step: 50253\n",
      "Evaluation Reward: -72.52928516002311\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -82.50909926715029\n",
      "Eval Mean: -131.682738327952\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 173\n",
      "Step: 51253\n",
      "Evaluation Reward: -45.81393791874057\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -82.96615723673317\n",
      "Eval Mean: -130.39874100884424\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 174\n",
      "Step: 52252\n",
      "Evaluation Reward: -79.76310643452413\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -84.7536232275865\n",
      "Eval Mean: -133.67961171545778\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 175\n",
      "Step: 53199\n",
      "Evaluation Reward: -253.0987178557718\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -85.49428142165647\n",
      "Eval Mean: -135.67566165737114\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 176\n",
      "Step: 54199\n",
      "Evaluation Reward: -229.18307414514774\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -84.49040030152253\n",
      "Eval Mean: -136.03332290060672\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 177\n",
      "Step: 55199\n",
      "Evaluation Reward: -143.82580436752693\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -83.37347829563888\n",
      "Eval Mean: -136.22161499836056\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 178\n",
      "Step: 55960\n",
      "Evaluation Reward: -224.399237423562\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -84.42426672704309\n",
      "Eval Mean: -135.1227395921001\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 179\n",
      "Step: 56753\n",
      "Evaluation Reward: -228.78141753212506\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -84.49021084504348\n",
      "Eval Mean: -136.13649616841752\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 180\n",
      "Step: 57753\n",
      "Evaluation Reward: -241.66634363628293\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -83.50831809013327\n",
      "Eval Mean: -136.60428188447383\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 181\n",
      "Step: 58459\n",
      "Evaluation Reward: -185.78376212722327\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -85.13096415891388\n",
      "Eval Mean: -137.56246007842665\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 182\n",
      "Step: 58613\n",
      "Evaluation Reward: -200.36940458537106\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -83.38169079135722\n",
      "Eval Mean: -137.2449276244774\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 183\n",
      "Step: 59205\n",
      "Evaluation Reward: -217.00117495340325\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -84.01700482021467\n",
      "Eval Mean: -137.27945144718174\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 184\n",
      "Step: 59871\n",
      "Evaluation Reward: -237.7809676850493\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -84.91353295472146\n",
      "Eval Mean: -137.73795706027747\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 185\n",
      "Step: 60019\n",
      "Evaluation Reward: -43.54848317330527\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -84.55021182217081\n",
      "Eval Mean: -136.23175172850736\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 186\n",
      "Step: 60355\n",
      "Evaluation Reward: -30.169868571483626\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -83.61602396215767\n",
      "Eval Mean: -135.24802842551537\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 187\n",
      "Step: 61137\n",
      "Evaluation Reward: -85.51734505009365\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -83.95185928308321\n",
      "Eval Mean: -133.233908670362\n",
      "--------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "Episode: 188\n",
      "Step: 62137\n",
      "Evaluation Reward: 89.24897974925454\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -84.78491773237484\n",
      "Eval Mean: -130.35430278771372\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 189\n",
      "Step: 62729\n",
      "Evaluation Reward: -103.96491428004181\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -86.6701999204916\n",
      "Eval Mean: -129.24569522453237\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 190\n",
      "Step: 63729\n",
      "Evaluation Reward: -95.84161637210481\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -85.66913808854399\n",
      "Eval Mean: -128.62718754612413\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 191\n",
      "Step: 64269\n",
      "Evaluation Reward: -86.21820329396303\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -86.70422272486279\n",
      "Eval Mean: -128.46478312966917\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 192\n",
      "Step: 64726\n",
      "Evaluation Reward: -134.30735386258\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -88.02858020742872\n",
      "Eval Mean: -127.97956455555396\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 193\n",
      "Step: 65234\n",
      "Evaluation Reward: -203.7324336921739\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -88.82091977159354\n",
      "Eval Mean: -129.036780223117\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 194\n",
      "Step: 65714\n",
      "Evaluation Reward: -42.98332470783195\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -85.99958571462345\n",
      "Eval Mean: -127.28411855752925\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 195\n",
      "Step: 66174\n",
      "Evaluation Reward: -228.89405134427705\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -85.92248754396756\n",
      "Eval Mean: -127.19638114357562\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 196\n",
      "Step: 66503\n",
      "Evaluation Reward: -372.32373111592653\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -85.90718770939729\n",
      "Eval Mean: -128.26243999478473\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 197\n",
      "Step: 66928\n",
      "Evaluation Reward: -71.3160297808025\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -87.5076627007734\n",
      "Eval Mean: -126.8897823956994\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 198\n",
      "Step: 67490\n",
      "Evaluation Reward: -140.71213322696516\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -90.280682877853\n",
      "Eval Mean: -127.87308594719123\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 199\n",
      "Step: 68490\n",
      "Evaluation Reward: -107.07717151382407\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -89.15881101857856\n",
      "Eval Mean: -127.2707400081537\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 200\n",
      "Step: 69490\n",
      "Evaluation Reward: -200.94145307675404\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -89.14210492555121\n",
      "Eval Mean: -126.91830542604623\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 201\n",
      "Step: 70020\n",
      "Evaluation Reward: -101.02074587976546\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -88.09760517781842\n",
      "Eval Mean: -126.10402579956231\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 202\n",
      "Step: 70522\n",
      "Evaluation Reward: -130.5631250488759\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -84.69869663153857\n",
      "Eval Mean: -125.4347665370333\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 203\n",
      "Step: 71333\n",
      "Evaluation Reward: -22.834036788730092\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -86.21034912155054\n",
      "Eval Mean: -123.7964741436545\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 204\n",
      "Step: 72056\n",
      "Evaluation Reward: -54.79701093293694\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -87.52348444405008\n",
      "Eval Mean: -122.70521696194062\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 205\n",
      "Step: 72316\n",
      "Evaluation Reward: 121.42729040343096\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -86.98823578980883\n",
      "Eval Mean: -119.886992874999\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 206\n",
      "Step: 73316\n",
      "Evaluation Reward: -95.11352220849955\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -86.72803872801856\n",
      "Eval Mean: -119.35276361857649\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 207\n",
      "Step: 74316\n",
      "Evaluation Reward: -59.89610754397711\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -86.79497168993684\n",
      "Eval Mean: -118.12488777196938\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 208\n",
      "Step: 75316\n",
      "Evaluation Reward: -68.41565341035543\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -86.77890327818602\n",
      "Eval Mean: -117.41179430774395\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 209\n",
      "Step: 75770\n",
      "Evaluation Reward: -100.010004695024\n",
      "Best Evaluation Reward: 248.32396422683289\n",
      "Train Mean: -86.58306592671838\n",
      "Eval Mean: -117.24192019091848\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 210\n",
      "Step: 76032\n",
      "Evaluation Reward: 279.88023725509925\n",
      "Best Evaluation Reward: 279.88023725509925\n",
      "Train Mean: -86.10692768589428\n",
      "Eval Mean: -113.30775255922222\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 211\n",
      "Step: 77032\n",
      "Evaluation Reward: 192.29346595426014\n",
      "Best Evaluation Reward: 279.88023725509925\n",
      "Train Mean: -86.50213374634565\n",
      "Eval Mean: -110.53023919047467\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 212\n",
      "Step: 78032\n",
      "Evaluation Reward: 271.04526843091656\n",
      "Best Evaluation Reward: 279.88023725509925\n",
      "Train Mean: -86.30311499223374\n",
      "Eval Mean: -106.55023684150606\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 213\n",
      "Step: 79032\n",
      "Evaluation Reward: -89.23875457349044\n",
      "Best Evaluation Reward: 279.88023725509925\n",
      "Train Mean: -86.72704609629764\n",
      "Eval Mean: -106.10739385080032\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 214\n",
      "Step: 79485\n",
      "Evaluation Reward: -66.30427005464001\n",
      "Best Evaluation Reward: 279.88023725509925\n",
      "Train Mean: -84.09412945896527\n",
      "Eval Mean: -104.59895294993512\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 215\n",
      "Step: 80485\n",
      "Evaluation Reward: 192.90213467018032\n",
      "Best Evaluation Reward: 279.88023725509925\n",
      "Train Mean: -84.81434409510067\n",
      "Eval Mean: -101.16728008482967\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 216\n",
      "Step: 81193\n",
      "Evaluation Reward: 264.26456307986587\n",
      "Best Evaluation Reward: 279.88023725509925\n",
      "Train Mean: -87.02994178863672\n",
      "Eval Mean: -97.23301919234824\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 217\n",
      "Step: 82193\n",
      "Evaluation Reward: 190.43041680962244\n",
      "Best Evaluation Reward: 279.88023725509925\n",
      "Train Mean: -86.5023067158218\n",
      "Eval Mean: -93.83951291988924\n",
      "--------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "Episode: 218\n",
      "Step: 83193\n",
      "Evaluation Reward: -98.63272679940168\n",
      "Best Evaluation Reward: 279.88023725509925\n",
      "Train Mean: -86.55671432354411\n",
      "Eval Mean: -93.21499314856436\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 219\n",
      "Step: 84193\n",
      "Evaluation Reward: 13.717103975514862\n",
      "Best Evaluation Reward: 279.88023725509925\n",
      "Train Mean: -85.38649211280989\n",
      "Eval Mean: -92.23900240942982\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 220\n",
      "Step: 85193\n",
      "Evaluation Reward: -86.05045686461897\n",
      "Best Evaluation Reward: 279.88023725509925\n",
      "Train Mean: -85.49670381396223\n",
      "Eval Mean: -90.99894759599077\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 221\n",
      "Step: 85710\n",
      "Evaluation Reward: 200.0535243404456\n",
      "Best Evaluation Reward: 279.88023725509925\n",
      "Train Mean: -87.57655510272926\n",
      "Eval Mean: -87.33221296063287\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 222\n",
      "Step: 86035\n",
      "Evaluation Reward: -86.32541334993635\n",
      "Best Evaluation Reward: 279.88023725509925\n",
      "Train Mean: -82.4504176881352\n",
      "Eval Mean: -86.58874942612827\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 223\n",
      "Step: 86387\n",
      "Evaluation Reward: -70.95595513111516\n",
      "Best Evaluation Reward: 279.88023725509925\n",
      "Train Mean: -79.19869172358278\n",
      "Eval Mean: -85.9250248099653\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 224\n",
      "Step: 87387\n",
      "Evaluation Reward: -115.23087563049229\n",
      "Best Evaluation Reward: 279.88023725509925\n",
      "Train Mean: -79.82019232352125\n",
      "Eval Mean: -85.90439188856493\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 225\n",
      "Step: 88387\n",
      "Evaluation Reward: 240.0986356826929\n",
      "Best Evaluation Reward: 279.88023725509925\n",
      "Train Mean: -79.84075338487436\n",
      "Eval Mean: -82.14293553969773\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 226\n",
      "Step: 89387\n",
      "Evaluation Reward: -67.83795670757173\n",
      "Best Evaluation Reward: 279.88023725509925\n",
      "Train Mean: -80.3403189419564\n",
      "Eval Mean: -81.31210638722467\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 227\n",
      "Step: 90387\n",
      "Evaluation Reward: 276.1315440038069\n",
      "Best Evaluation Reward: 279.88023725509925\n",
      "Train Mean: -80.05825351311024\n",
      "Eval Mean: -76.64565013107386\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 228\n",
      "Step: 91387\n",
      "Evaluation Reward: -84.41548937743224\n",
      "Best Evaluation Reward: 279.88023725509925\n",
      "Train Mean: -79.63985669199258\n",
      "Eval Mean: -75.93584157701441\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 229\n",
      "Step: 92387\n",
      "Evaluation Reward: -101.95198386931075\n",
      "Best Evaluation Reward: 279.88023725509925\n",
      "Train Mean: -81.21763553854113\n",
      "Eval Mean: -75.13590169920583\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 230\n",
      "Step: 93387\n",
      "Evaluation Reward: -37.6579664169152\n",
      "Best Evaluation Reward: 279.88023725509925\n",
      "Train Mean: -81.3859121263494\n",
      "Eval Mean: -73.98100548295196\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 231\n",
      "Step: 93853\n",
      "Evaluation Reward: -254.74080537221522\n",
      "Best Evaluation Reward: 279.88023725509925\n",
      "Train Mean: -78.7118740192397\n",
      "Eval Mean: -74.89966859748095\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 232\n",
      "Step: 94177\n",
      "Evaluation Reward: 114.25020251460661\n",
      "Best Evaluation Reward: 279.88023725509925\n",
      "Train Mean: -76.212324104463\n",
      "Eval Mean: -71.98531859014527\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 233\n",
      "Step: 95177\n",
      "Evaluation Reward: -99.07256265222017\n",
      "Best Evaluation Reward: 279.88023725509925\n",
      "Train Mean: -76.46864116677281\n",
      "Eval Mean: -71.49118932406348\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 234\n",
      "Step: 95799\n",
      "Evaluation Reward: -46.38774168159717\n",
      "Best Evaluation Reward: 279.88023725509925\n",
      "Train Mean: -73.32613561918468\n",
      "Eval Mean: -70.09450646097096\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 235\n",
      "Step: 96289\n",
      "Evaluation Reward: -58.51790451093053\n",
      "Best Evaluation Reward: 279.88023725509925\n",
      "Train Mean: -73.74542420153676\n",
      "Eval Mean: -69.27822988254682\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 236\n",
      "Step: 97197\n",
      "Evaluation Reward: 281.0691865789917\n",
      "Best Evaluation Reward: 281.0691865789917\n",
      "Train Mean: -71.3681625193326\n",
      "Eval Mean: -64.99456201835044\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 237\n",
      "Step: 98123\n",
      "Evaluation Reward: -95.91151391415741\n",
      "Best Evaluation Reward: 281.0691865789917\n",
      "Train Mean: -72.04716701799401\n",
      "Eval Mean: -64.30558370933913\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 238\n",
      "Step: 98525\n",
      "Evaluation Reward: -94.36673665724493\n",
      "Best Evaluation Reward: 281.0691865789917\n",
      "Train Mean: -67.20547055173198\n",
      "Eval Mean: -62.432900027105774\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 239\n",
      "Step: 99525\n",
      "Evaluation Reward: -67.12659139294956\n",
      "Best Evaluation Reward: 281.0691865789917\n",
      "Train Mean: -66.77675217919268\n",
      "Eval Mean: -62.02032811225832\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 240\n",
      "Step: 100525\n",
      "Evaluation Reward: -45.50844490368824\n",
      "Best Evaluation Reward: 281.0691865789917\n",
      "Train Mean: -64.81088901168816\n",
      "Eval Mean: -61.573789206444644\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 241\n",
      "Step: 101525\n",
      "Evaluation Reward: -39.515628098459985\n",
      "Best Evaluation Reward: 281.0691865789917\n",
      "Train Mean: -64.50818377552274\n",
      "Eval Mean: -60.850205214900576\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 242\n",
      "Step: 101913\n",
      "Evaluation Reward: -61.22985712693663\n",
      "Best Evaluation Reward: 281.0691865789917\n",
      "Train Mean: -60.74684941827743\n",
      "Eval Mean: -60.25413081823166\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 243\n",
      "Step: 102913\n",
      "Evaluation Reward: -50.251273012224225\n",
      "Best Evaluation Reward: 281.0691865789917\n",
      "Train Mean: -60.48299539925898\n",
      "Eval Mean: -59.517281041866596\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 244\n",
      "Step: 103492\n",
      "Evaluation Reward: 63.25557089942453\n",
      "Best Evaluation Reward: 281.0691865789917\n",
      "Train Mean: -57.62952721049283\n",
      "Eval Mean: -58.6760471562809\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 245\n",
      "Step: 104492\n",
      "Evaluation Reward: -139.83678186345517\n",
      "Best Evaluation Reward: 281.0691865789917\n",
      "Train Mean: -57.03929598877942\n",
      "Eval Mean: -59.57555599974385\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 246\n",
      "Step: 105492\n",
      "Evaluation Reward: -57.20185107556277\n",
      "Best Evaluation Reward: 281.0691865789917\n",
      "Train Mean: -56.397003625243975\n",
      "Eval Mean: -59.2290964423212\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 247\n",
      "Step: 106492\n",
      "Evaluation Reward: 314.61238553694506\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: -56.95225440926633\n",
      "Eval Mean: -54.66574710455707\n",
      "--------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "Episode: 248\n",
      "Step: 107114\n",
      "Evaluation Reward: -64.04354799089809\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: -59.014625913931184\n",
      "Eval Mean: -55.16442502729755\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 249\n",
      "Step: 108114\n",
      "Evaluation Reward: -74.07038327889803\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: -57.24121778754972\n",
      "Eval Mean: -55.41467766411867\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 250\n",
      "Step: 108914\n",
      "Evaluation Reward: -57.30728232825408\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: -54.83264358755685\n",
      "Eval Mean: -54.92864086618581\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 251\n",
      "Step: 109914\n",
      "Evaluation Reward: 167.79194092488842\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: -55.139921397637266\n",
      "Eval Mean: -50.748480775110295\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 252\n",
      "Step: 110914\n",
      "Evaluation Reward: 158.2475032618354\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: -55.14392863368229\n",
      "Eval Mean: -47.68009492789903\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 253\n",
      "Step: 111501\n",
      "Evaluation Reward: -41.33566339030966\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: -52.02240394378195\n",
      "Eval Mean: -47.503153698107944\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 254\n",
      "Step: 112408\n",
      "Evaluation Reward: -67.20389476397597\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: -47.91844265558062\n",
      "Eval Mean: -47.71016167311987\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 255\n",
      "Step: 112557\n",
      "Evaluation Reward: 13.063984411669544\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: -47.79568428947603\n",
      "Eval Mean: -47.07123869711049\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 256\n",
      "Step: 113317\n",
      "Evaluation Reward: 242.02616171627068\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: -46.420811689566015\n",
      "Eval Mean: -44.17693901372496\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 257\n",
      "Step: 113487\n",
      "Evaluation Reward: 25.39865390559258\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: -46.27350843851258\n",
      "Eval Mean: -43.44038069082633\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 258\n",
      "Step: 114355\n",
      "Evaluation Reward: 14.809444032673866\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: -44.37697219885209\n",
      "Eval Mean: -42.34746698612352\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 259\n",
      "Step: 114994\n",
      "Evaluation Reward: 235.8324011851461\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: -41.70532332607553\n",
      "Eval Mean: -39.82861903220928\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 260\n",
      "Step: 115994\n",
      "Evaluation Reward: 72.99313265809032\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: -40.687962287783144\n",
      "Eval Mean: -38.917962347171276\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 261\n",
      "Step: 116741\n",
      "Evaluation Reward: 153.50296912747027\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: -38.12250251248549\n",
      "Eval Mean: -37.149774309623034\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 262\n",
      "Step: 117395\n",
      "Evaluation Reward: 219.31351235330317\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: -35.52446118235921\n",
      "Eval Mean: -34.65009842112909\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 263\n",
      "Step: 118087\n",
      "Evaluation Reward: -0.328438488468052\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: -33.255938904528634\n",
      "Eval Mean: -34.19630588779464\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 264\n",
      "Step: 118278\n",
      "Evaluation Reward: 221.59777977142738\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: -32.87501409877158\n",
      "Eval Mean: -31.75951995490889\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 265\n",
      "Step: 119249\n",
      "Evaluation Reward: 220.8559488442694\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: -30.938384820595758\n",
      "Eval Mean: -29.676380457405983\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 266\n",
      "Step: 120249\n",
      "Evaluation Reward: 240.33218530806835\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: -30.128857622879945\n",
      "Eval Mean: -26.993245098746275\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 267\n",
      "Step: 120872\n",
      "Evaluation Reward: -78.60044650394336\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: -27.249158548526236\n",
      "Eval Mean: -27.61638134365198\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 268\n",
      "Step: 121225\n",
      "Evaluation Reward: 211.12893713302842\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: -24.273763153169153\n",
      "Eval Mean: -23.258239683537962\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 269\n",
      "Step: 122225\n",
      "Evaluation Reward: -43.9657057660791\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: -23.850838731056147\n",
      "Eval Mean: -23.118387403168693\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 270\n",
      "Step: 122885\n",
      "Evaluation Reward: 11.464385835385386\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: -21.319506422280472\n",
      "Eval Mean: -21.452855775894964\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 271\n",
      "Step: 123180\n",
      "Evaluation Reward: -18.498007857322726\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: -20.9813981285454\n",
      "Eval Mean: -20.867780144455814\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 272\n",
      "Step: 124180\n",
      "Evaluation Reward: 14.40121616718913\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: -20.53409581249208\n",
      "Eval Mean: -19.998475131183692\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 273\n",
      "Step: 125092\n",
      "Evaluation Reward: 248.96020429016679\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: -17.977269293270098\n",
      "Eval Mean: -17.050733709094615\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 274\n",
      "Step: 125573\n",
      "Evaluation Reward: 239.82832640734637\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: -12.97998612144657\n",
      "Eval Mean: -13.854819380675911\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 275\n",
      "Step: 126150\n",
      "Evaluation Reward: -197.26543557325658\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: -8.276356826574132\n",
      "Eval Mean: -13.296486557850761\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 276\n",
      "Step: 126640\n",
      "Evaluation Reward: 223.732588638709\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: -5.332608994844339\n",
      "Eval Mean: -8.767329930012192\n",
      "--------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "Episode: 277\n",
      "Step: 126779\n",
      "Evaluation Reward: -11.319682884219162\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: -5.916185524882292\n",
      "Eval Mean: -7.442268715179114\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 278\n",
      "Step: 127536\n",
      "Evaluation Reward: 258.9551581121845\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: -1.0236405927782208\n",
      "Eval Mean: -2.6087247598216488\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 279\n",
      "Step: 128525\n",
      "Evaluation Reward: -44.643946373112286\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 2.7067876953388303\n",
      "Eval Mean: -0.7673500482315206\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 280\n",
      "Step: 129276\n",
      "Evaluation Reward: 161.85506493503988\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 4.885505128667973\n",
      "Eval Mean: 3.2678640374817087\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 281\n",
      "Step: 129457\n",
      "Evaluation Reward: -185.83131158243293\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 7.2133798418819755\n",
      "Eval Mean: 3.267388542929613\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 282\n",
      "Step: 129671\n",
      "Evaluation Reward: 225.8958331304869\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 5.0425200596063\n",
      "Eval Mean: 7.53004092008819\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 283\n",
      "Step: 130076\n",
      "Evaluation Reward: -1.667971158958892\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 9.388193521746476\n",
      "Eval Mean: 9.683372958032633\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 284\n",
      "Step: 130723\n",
      "Evaluation Reward: 140.59927058310757\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 13.510400199675876\n",
      "Eval Mean: 13.467175340714203\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 285\n",
      "Step: 131041\n",
      "Evaluation Reward: 163.20646286014394\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 13.412017387221812\n",
      "Eval Mean: 15.534724801048695\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 286\n",
      "Step: 132041\n",
      "Evaluation Reward: -70.05070800094997\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 15.303571316959895\n",
      "Eval Mean: 15.135916406754031\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 287\n",
      "Step: 133041\n",
      "Evaluation Reward: -2.657534464965128\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 17.123767263585925\n",
      "Eval Mean: 15.964514512605312\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 288\n",
      "Step: 133466\n",
      "Evaluation Reward: -55.123919591962505\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 17.417592671659456\n",
      "Eval Mean: 14.520785519193142\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 289\n",
      "Step: 133606\n",
      "Evaluation Reward: 133.47082980930273\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 18.091767274279945\n",
      "Eval Mean: 16.895142960086588\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 290\n",
      "Step: 134430\n",
      "Evaluation Reward: 198.0692654882373\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 19.562684319309113\n",
      "Eval Mean: 19.83425177869001\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 291\n",
      "Step: 135369\n",
      "Evaluation Reward: 198.3324203777658\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 22.83844645525246\n",
      "Eval Mean: 22.679758015407298\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 292\n",
      "Step: 136340\n",
      "Evaluation Reward: 291.3524584005034\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 26.956122510612193\n",
      "Eval Mean: 26.936356138038132\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 293\n",
      "Step: 137340\n",
      "Evaluation Reward: -40.79538781095835\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 28.608549600642792\n",
      "Eval Mean: 28.56572659685029\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 294\n",
      "Step: 138340\n",
      "Evaluation Reward: -27.24252434582734\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 27.436100690460318\n",
      "Eval Mean: 28.72313460047033\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 295\n",
      "Step: 138498\n",
      "Evaluation Reward: -60.37424372418386\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 29.663268950394777\n",
      "Eval Mean: 30.408332676671264\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 296\n",
      "Step: 139118\n",
      "Evaluation Reward: -113.12258263351194\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 32.816979617199166\n",
      "Eval Mean: 33.000344161495406\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 297\n",
      "Step: 140012\n",
      "Evaluation Reward: 265.1005743149846\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 38.279849027258535\n",
      "Eval Mean: 36.364510202453275\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 298\n",
      "Step: 140727\n",
      "Evaluation Reward: -191.77038627982859\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 43.24191761588144\n",
      "Eval Mean: 35.853927671924644\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 299\n",
      "Step: 140872\n",
      "Evaluation Reward: 203.0850122936423\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 41.83086390429798\n",
      "Eval Mean: 38.95554950999931\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 300\n",
      "Step: 141872\n",
      "Evaluation Reward: -39.20313262466385\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 43.348562186547866\n",
      "Eval Mean: 40.572932714520206\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 301\n",
      "Step: 142029\n",
      "Evaluation Reward: -50.81603875245762\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 44.42759525073337\n",
      "Eval Mean: 41.07497978579328\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 302\n",
      "Step: 142384\n",
      "Evaluation Reward: -280.81499866688375\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 44.092858665644044\n",
      "Eval Mean: 39.57246104961321\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 303\n",
      "Step: 142538\n",
      "Evaluation Reward: -117.84845681163918\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 44.130343704755944\n",
      "Eval Mean: 38.62231684938412\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 304\n",
      "Step: 142710\n",
      "Evaluation Reward: -35.46449674481237\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 45.62773452060073\n",
      "Eval Mean: 38.815641991265366\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 305\n",
      "Step: 143230\n",
      "Evaluation Reward: 249.05046185660714\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 49.92499874863509\n",
      "Eval Mean: 40.09187370579713\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 306\n",
      "Step: 143720\n",
      "Evaluation Reward: 151.75016639165005\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 48.031665344128825\n",
      "Eval Mean: 42.56051059179863\n",
      "--------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "Episode: 307\n",
      "Step: 144149\n",
      "Evaluation Reward: 77.33221045386102\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 50.7529900635877\n",
      "Eval Mean: 43.93279377177701\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 308\n",
      "Step: 144266\n",
      "Evaluation Reward: -25.108850803039445\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 51.04545439032312\n",
      "Eval Mean: 44.365861797850165\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 309\n",
      "Step: 145266\n",
      "Evaluation Reward: 201.35682891831644\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 52.00923093498519\n",
      "Eval Mean: 47.37953013398357\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 310\n",
      "Step: 145389\n",
      "Evaluation Reward: 12.880362572107645\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 50.80630112661873\n",
      "Eval Mean: 44.70953138715367\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 311\n",
      "Step: 146053\n",
      "Evaluation Reward: 256.3390974890759\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 53.805461100138906\n",
      "Eval Mean: 45.34998770250182\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 312\n",
      "Step: 146695\n",
      "Evaluation Reward: -69.52013652126487\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 55.675760496005644\n",
      "Eval Mean: 41.94433365297999\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 313\n",
      "Step: 147695\n",
      "Evaluation Reward: 237.2107186140801\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 57.96901348258767\n",
      "Eval Mean: 45.208828384855714\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 314\n",
      "Step: 148695\n",
      "Evaluation Reward: -97.92330413346676\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 56.338280150123275\n",
      "Eval Mean: 44.892638044067446\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 315\n",
      "Step: 149695\n",
      "Evaluation Reward: -58.0091269166517\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 58.479809412863844\n",
      "Eval Mean: 42.38352542819912\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 316\n",
      "Step: 149889\n",
      "Evaluation Reward: -128.99280684647636\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 58.94316347091458\n",
      "Eval Mean: 38.45095172893569\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 317\n",
      "Step: 150889\n",
      "Evaluation Reward: 236.51454549123974\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 60.355438969950995\n",
      "Eval Mean: 38.911793015751876\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 318\n",
      "Step: 151075\n",
      "Evaluation Reward: 114.36229699827936\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 60.6581767758358\n",
      "Eval Mean: 41.04174325372868\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 319\n",
      "Step: 151592\n",
      "Evaluation Reward: 196.23652756726966\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 64.40539681428434\n",
      "Eval Mean: 42.86693748964623\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 320\n",
      "Step: 152211\n",
      "Evaluation Reward: -85.49744127468219\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 67.33433133232343\n",
      "Eval Mean: 42.872467645545605\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 321\n",
      "Step: 152545\n",
      "Evaluation Reward: 200.45564012922196\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 71.81010492975933\n",
      "Eval Mean: 42.87648880343336\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 322\n",
      "Step: 152694\n",
      "Evaluation Reward: 178.5840994465774\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 68.65249409381747\n",
      "Eval Mean: 45.525583931398494\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 323\n",
      "Step: 152828\n",
      "Evaluation Reward: -188.9145306956953\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 65.59755958982255\n",
      "Eval Mean: 44.34599817575269\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 324\n",
      "Step: 152977\n",
      "Evaluation Reward: -66.14169206984315\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 65.97892895606243\n",
      "Eval Mean: 44.83689001135918\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 325\n",
      "Step: 153977\n",
      "Evaluation Reward: -66.24487036510067\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 67.04824911500256\n",
      "Eval Mean: 41.773454950881245\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 326\n",
      "Step: 154977\n",
      "Evaluation Reward: 247.01696305690186\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 69.06772489883114\n",
      "Eval Mean: 44.922004148525986\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 327\n",
      "Step: 155147\n",
      "Evaluation Reward: 133.3423625492407\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 68.72825433498511\n",
      "Eval Mean: 43.49411233398032\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 328\n",
      "Step: 156147\n",
      "Evaluation Reward: -112.28568659693244\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 70.01653481217119\n",
      "Eval Mean: 43.21541036178532\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 329\n",
      "Step: 157147\n",
      "Evaluation Reward: 233.14071082151474\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 71.91207997346291\n",
      "Eval Mean: 46.56633730869356\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 330\n",
      "Step: 158147\n",
      "Evaluation Reward: 205.7162118377195\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 72.38288485114167\n",
      "Eval Mean: 49.00007909123991\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 331\n",
      "Step: 158547\n",
      "Evaluation Reward: 174.25783354119952\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 72.62896676768068\n",
      "Eval Mean: 53.29006548037406\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 332\n",
      "Step: 159547\n",
      "Evaluation Reward: 198.28460353938027\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 71.31002402489719\n",
      "Eval Mean: 54.13040949062181\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 333\n",
      "Step: 160386\n",
      "Evaluation Reward: 236.26105144619416\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 73.60835159823279\n",
      "Eval Mean: 57.483745631605956\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 334\n",
      "Step: 160624\n",
      "Evaluation Reward: 126.074682327927\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 73.30343768530751\n",
      "Eval Mean: 59.208369871701194\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 335\n",
      "Step: 161624\n",
      "Evaluation Reward: 90.53410991311448\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 75.15345985732162\n",
      "Eval Mean: 60.69889001594164\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 336\n",
      "Step: 161759\n",
      "Evaluation Reward: 228.81721034248608\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 73.02944366200451\n",
      "Eval Mean: 60.176370253576586\n",
      "--------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "Episode: 337\n",
      "Step: 162759\n",
      "Evaluation Reward: 250.15028351458875\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 75.86491830556585\n",
      "Eval Mean: 63.63698822786405\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 338\n",
      "Step: 163422\n",
      "Evaluation Reward: -103.16992025930693\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 75.18457935861153\n",
      "Eval Mean: 63.54895639184343\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 339\n",
      "Step: 163889\n",
      "Evaluation Reward: 248.96416890360078\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 77.42741706697639\n",
      "Eval Mean: 66.70986399480893\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 340\n",
      "Step: 164705\n",
      "Evaluation Reward: 197.3617279772962\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 79.76886644686451\n",
      "Eval Mean: 69.13856572361878\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 341\n",
      "Step: 165286\n",
      "Evaluation Reward: 219.9261977381297\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 82.28370485398044\n",
      "Eval Mean: 71.73298398198467\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 342\n",
      "Step: 166286\n",
      "Evaluation Reward: -185.0573501316416\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 80.38955184608938\n",
      "Eval Mean: 70.49470905193762\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 343\n",
      "Step: 166631\n",
      "Evaluation Reward: 139.32427805809348\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 83.18707077543257\n",
      "Eval Mean: 72.3904645626408\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 344\n",
      "Step: 167631\n",
      "Evaluation Reward: 103.34358608574982\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 81.95254182036192\n",
      "Eval Mean: 72.79134471450406\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 345\n",
      "Step: 168631\n",
      "Evaluation Reward: 213.13037543634618\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 82.79560530313482\n",
      "Eval Mean: 76.32101628750208\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 346\n",
      "Step: 169218\n",
      "Evaluation Reward: 239.12550645075478\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 86.26432297340159\n",
      "Eval Mean: 79.28428986276525\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 347\n",
      "Step: 169771\n",
      "Evaluation Reward: 260.6144775551601\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 89.37647119940027\n",
      "Eval Mean: 78.74431078294741\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 348\n",
      "Step: 170739\n",
      "Evaluation Reward: 270.40293743372433\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 93.93451505738041\n",
      "Eval Mean: 82.08877563719363\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 349\n",
      "Step: 171185\n",
      "Evaluation Reward: 238.05249135648302\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 96.36852805377885\n",
      "Eval Mean: 85.21000438354741\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 350\n",
      "Step: 171639\n",
      "Evaluation Reward: 117.55858169308615\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 96.76709349219415\n",
      "Eval Mean: 86.95866302376085\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 351\n",
      "Step: 172639\n",
      "Evaluation Reward: 241.63238216810197\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 98.32054144061748\n",
      "Eval Mean: 87.69706743619298\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 352\n",
      "Step: 173492\n",
      "Evaluation Reward: 85.5899422501655\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 100.63711316297912\n",
      "Eval Mean: 86.97049182607628\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 353\n",
      "Step: 174492\n",
      "Evaluation Reward: 205.19500109112167\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 99.21354923839286\n",
      "Eval Mean: 89.4357984708906\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 354\n",
      "Step: 174946\n",
      "Evaluation Reward: 246.60967328645913\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 100.64831389352625\n",
      "Eval Mean: 92.57393415139495\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 355\n",
      "Step: 175881\n",
      "Evaluation Reward: 62.51194624669869\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 103.70041876225815\n",
      "Eval Mean: 93.06841376974523\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 356\n",
      "Step: 176881\n",
      "Evaluation Reward: 104.69981397406671\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 103.24845497910611\n",
      "Eval Mean: 91.69515029232319\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 357\n",
      "Step: 177465\n",
      "Evaluation Reward: 229.2649289877857\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 105.7319493920506\n",
      "Eval Mean: 93.7338130431451\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 358\n",
      "Step: 178465\n",
      "Evaluation Reward: 247.6221272801422\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 105.58942407017105\n",
      "Eval Mean: 96.06193987561979\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 359\n",
      "Step: 178762\n",
      "Evaluation Reward: 93.86199960636013\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 105.54531394897973\n",
      "Eval Mean: 94.64223585983193\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 360\n",
      "Step: 179010\n",
      "Evaluation Reward: 260.2749034516609\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 107.69359883570549\n",
      "Eval Mean: 96.51505356776765\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 361\n",
      "Step: 179715\n",
      "Evaluation Reward: 220.96551507301018\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 107.95871457339908\n",
      "Eval Mean: 97.18967902722302\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 362\n",
      "Step: 180715\n",
      "Evaluation Reward: 227.54678276291637\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 107.10241262957081\n",
      "Eval Mean: 97.27201173131917\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 363\n",
      "Step: 181715\n",
      "Evaluation Reward: 280.0402326781087\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 106.06012956512319\n",
      "Eval Mean: 100.07569844298494\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 364\n",
      "Step: 181851\n",
      "Evaluation Reward: -68.54146245777642\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 105.57604616739376\n",
      "Eval Mean: 97.17430602069291\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 365\n",
      "Step: 182851\n",
      "Evaluation Reward: 241.4031428766394\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 105.30616585148208\n",
      "Eval Mean: 97.3797779610166\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 366\n",
      "Step: 183631\n",
      "Evaluation Reward: 143.66891398517723\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 107.18305919887536\n",
      "Eval Mean: 96.41314524778768\n",
      "--------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "Episode: 367\n",
      "Step: 183807\n",
      "Evaluation Reward: 257.4901829020641\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 104.57686653788527\n",
      "Eval Mean: 99.77405154184775\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 368\n",
      "Step: 184807\n",
      "Evaluation Reward: 136.6875907504221\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 103.71860068473384\n",
      "Eval Mean: 99.02963807802169\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 369\n",
      "Step: 185807\n",
      "Evaluation Reward: 291.3412702848111\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 105.25488090161997\n",
      "Eval Mean: 102.38270783853059\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 370\n",
      "Step: 186587\n",
      "Evaluation Reward: 176.29306821213862\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 105.38462577391535\n",
      "Eval Mean: 104.03099466229813\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 371\n",
      "Step: 186732\n",
      "Evaluation Reward: -3.918241751778311\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 105.31240095405259\n",
      "Eval Mean: 104.17679232335357\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 372\n",
      "Step: 187332\n",
      "Evaluation Reward: 105.07429047680641\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 107.4665243911701\n",
      "Eval Mean: 105.08352306644976\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 373\n",
      "Step: 187543\n",
      "Evaluation Reward: 123.50822797768825\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 104.43408049995658\n",
      "Eval Mean: 103.82900330332497\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 374\n",
      "Step: 188164\n",
      "Evaluation Reward: 103.27434702541039\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 104.7845251486277\n",
      "Eval Mean: 102.46346350950562\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 375\n",
      "Step: 188343\n",
      "Evaluation Reward: 106.38836379054524\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 102.67649570741668\n",
      "Eval Mean: 105.50000150314364\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 376\n",
      "Step: 188735\n",
      "Evaluation Reward: 229.7517183563016\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 102.09095943828632\n",
      "Eval Mean: 105.56019280031957\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 377\n",
      "Step: 189735\n",
      "Evaluation Reward: 262.9107658651792\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 103.54924830320142\n",
      "Eval Mean: 108.30249728781354\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 378\n",
      "Step: 189987\n",
      "Evaluation Reward: 277.70230630281105\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 101.32862424398921\n",
      "Eval Mean: 108.4899687697198\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 379\n",
      "Step: 190987\n",
      "Evaluation Reward: 247.1072075707097\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 101.31790283514414\n",
      "Eval Mean: 111.40748030915802\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 380\n",
      "Step: 191734\n",
      "Evaluation Reward: 254.61398844601902\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 101.33242458712961\n",
      "Eval Mean: 112.33506954426781\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 381\n",
      "Step: 192728\n",
      "Evaluation Reward: 270.3255675596797\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 103.77927769069616\n",
      "Eval Mean: 116.89663833568893\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 382\n",
      "Step: 193728\n",
      "Evaluation Reward: 290.26945499875364\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 107.44177391318031\n",
      "Eval Mean: 117.5403745543716\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 383\n",
      "Step: 194728\n",
      "Evaluation Reward: 270.5139721527144\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 106.76265234999482\n",
      "Eval Mean: 120.26219398748833\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 384\n",
      "Step: 195370\n",
      "Evaluation Reward: 257.8632888815273\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 107.27306242702967\n",
      "Eval Mean: 121.43483417047253\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 385\n",
      "Step: 195955\n",
      "Evaluation Reward: 248.11807011566142\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 110.27538342177539\n",
      "Eval Mean: 122.2839502430277\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 386\n",
      "Step: 196271\n",
      "Evaluation Reward: 252.53483666075758\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 111.93561675535167\n",
      "Eval Mean: 125.50980568964479\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 387\n",
      "Step: 196757\n",
      "Evaluation Reward: 267.0239593365794\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 114.48908843942205\n",
      "Eval Mean: 128.20662062766024\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 388\n",
      "Step: 196923\n",
      "Evaluation Reward: 264.778321004085\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 115.66342100657467\n",
      "Eval Mean: 131.40564303362072\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 389\n",
      "Step: 197190\n",
      "Evaluation Reward: 260.6465572700954\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 117.14305090150461\n",
      "Eval Mean: 132.67740030822864\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 390\n",
      "Step: 198190\n",
      "Evaluation Reward: 135.2066639748175\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 116.72773873694483\n",
      "Eval Mean: 132.04877429309443\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 391\n",
      "Step: 199190\n",
      "Evaluation Reward: 252.09461132866372\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 116.73966570625552\n",
      "Eval Mean: 132.58639620260342\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 392\n",
      "Step: 200190\n",
      "Evaluation Reward: 144.64771096976182\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 115.98383395753861\n",
      "Eval Mean: 131.119348728296\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 393\n",
      "Step: 200694\n",
      "Evaluation Reward: 102.30331029896814\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 117.9337922532728\n",
      "Eval Mean: 132.55033570939528\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 394\n",
      "Step: 201384\n",
      "Evaluation Reward: 129.84634329767434\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 119.78250006106612\n",
      "Eval Mean: 134.1212243858303\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 395\n",
      "Step: 202384\n",
      "Evaluation Reward: 145.63967464039638\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 121.05157985209898\n",
      "Eval Mean: 136.1813635694761\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 396\n",
      "Step: 203384\n",
      "Evaluation Reward: 136.46598690161463\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 120.81225382361968\n",
      "Eval Mean: 138.67724926482734\n",
      "--------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "Episode: 397\n",
      "Step: 204384\n",
      "Evaluation Reward: 137.35868616035455\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 119.27350474129764\n",
      "Eval Mean: 137.39983038328106\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 398\n",
      "Step: 204819\n",
      "Evaluation Reward: 276.04343913972775\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 120.5877309605798\n",
      "Eval Mean: 142.0779686374766\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 399\n",
      "Step: 205819\n",
      "Evaluation Reward: 153.30864523470362\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 123.54127739118138\n",
      "Eval Mean: 141.58020496688724\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 400\n",
      "Step: 206605\n",
      "Evaluation Reward: 284.16101835318375\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 124.97515524118515\n",
      "Eval Mean: 144.81384647666567\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 401\n",
      "Step: 207022\n",
      "Evaluation Reward: 259.1925835173996\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 127.77728839242278\n",
      "Eval Mean: 147.91393269936424\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 402\n",
      "Step: 207771\n",
      "Evaluation Reward: 156.33813581992877\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 128.4167431912384\n",
      "Eval Mean: 152.28546404423238\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 403\n",
      "Step: 208124\n",
      "Evaluation Reward: 251.0318098635288\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 132.8797956621467\n",
      "Eval Mean: 155.97426671098407\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 404\n",
      "Step: 209124\n",
      "Evaluation Reward: 251.42106558687152\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 135.227243704557\n",
      "Eval Mean: 158.84312233430092\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 405\n",
      "Step: 210124\n",
      "Evaluation Reward: 243.93255110513192\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 133.88564891698306\n",
      "Eval Mean: 158.7919432267862\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 406\n",
      "Step: 210773\n",
      "Evaluation Reward: 114.27387932276426\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 139.11343741697937\n",
      "Eval Mean: 158.4171803560973\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 407\n",
      "Step: 211481\n",
      "Evaluation Reward: 151.78037393770887\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 139.99388849389865\n",
      "Eval Mean: 159.16166199093578\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 408\n",
      "Step: 212051\n",
      "Evaluation Reward: 102.57949961212663\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 143.1018444143302\n",
      "Eval Mean: 160.43854549508742\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 409\n",
      "Step: 212898\n",
      "Evaluation Reward: 270.7695933139238\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 145.4899939164065\n",
      "Eval Mean: 161.1326731390435\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 410\n",
      "Step: 213898\n",
      "Evaluation Reward: 295.01450207323853\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 148.66189250683826\n",
      "Eval Mean: 163.9540145340548\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 411\n",
      "Step: 214373\n",
      "Evaluation Reward: 168.81073148904036\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 149.12689489289951\n",
      "Eval Mean: 163.0787308740545\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 412\n",
      "Step: 214611\n",
      "Evaluation Reward: 281.58419227873736\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 150.2022120675199\n",
      "Eval Mean: 166.58977416205448\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 413\n",
      "Step: 215611\n",
      "Evaluation Reward: 255.72375485501772\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 150.0312120415557\n",
      "Eval Mean: 166.77490452446386\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 414\n",
      "Step: 216400\n",
      "Evaluation Reward: 262.33341282467165\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 151.75005685472834\n",
      "Eval Mean: 170.37747169404528\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 415\n",
      "Step: 217400\n",
      "Evaluation Reward: 286.65302731259453\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 152.4192049415369\n",
      "Eval Mean: 173.82409323633772\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 416\n",
      "Step: 218400\n",
      "Evaluation Reward: 279.88463766832615\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 155.02429148422294\n",
      "Eval Mean: 177.91286768148572\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 417\n",
      "Step: 218716\n",
      "Evaluation Reward: 294.5111835250044\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 156.6296364381202\n",
      "Eval Mean: 178.49283406182334\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 418\n",
      "Step: 218817\n",
      "Evaluation Reward: 44.22611405420827\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 157.5723461591714\n",
      "Eval Mean: 177.79147223238263\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 419\n",
      "Step: 219420\n",
      "Evaluation Reward: 244.24907470492718\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 157.69338417368076\n",
      "Eval Mean: 178.27159770375923\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 420\n",
      "Step: 219909\n",
      "Evaluation Reward: 248.50047211269873\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 158.70687370642065\n",
      "Eval Mean: 181.611576837633\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 421\n",
      "Step: 220909\n",
      "Evaluation Reward: 294.95465561474657\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 158.48806401101774\n",
      "Eval Mean: 182.55656699248826\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 422\n",
      "Step: 221201\n",
      "Evaluation Reward: 251.94640743994665\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 161.51075055407588\n",
      "Eval Mean: 183.29019007242198\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 423\n",
      "Step: 222201\n",
      "Evaluation Reward: 265.0467050463161\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 163.62566034849277\n",
      "Eval Mean: 187.8298024298421\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 424\n",
      "Step: 223201\n",
      "Evaluation Reward: 294.1275864085983\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 165.81703224203605\n",
      "Eval Mean: 191.43249521462656\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "Episode: 425\n",
      "Step: 224161\n",
      "Evaluation Reward: 278.3829072840938\n",
      "Best Evaluation Reward: 314.61238553694506\n",
      "Train Mean: 167.79680766564863\n",
      "Eval Mean: 194.87877299111847\n",
      "--------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-9b6c7a54f83c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Storage\\Projects\\RLSimpleBaselines\\algorithms\\deeprl\\dqn\\agent.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    170\u001b[0m                 \u001b[0mreward_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarmup\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecrease_epsilon\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace_steps\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Storage\\Projects\\RLSimpleBaselines\\algorithms\\deeprl\\dqn\\agent.py\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    133\u001b[0m                     dim=1, index=indices).detach() * torch.logical_not(terminals)\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m         \u001b[0monline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq_online\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0monline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Storage\\Projects\\RLSimpleBaselines\\algorithms\\deeprl\\dqn\\model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_layer_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1751\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1752\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1753\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1754\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1755\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(agent.eval_rewards_mean)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
