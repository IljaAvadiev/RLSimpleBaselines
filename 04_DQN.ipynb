{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network (DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook is based on \"Human-Level Control through deep reinforcement learning\" by Mnih et. al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from itertools import count\n",
    "import os\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replay buffer for memory replay\n",
    "class ReplayBuffer():\n",
    "    '''\n",
    "    Stores memories up to a maximum of mem_size. The memories can be batched to use in training.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, input_size, mem_size=10000, batch_size=64):\n",
    "        self.mem_size = mem_size\n",
    "        self.index = 0\n",
    "        self.batch_size=batch_size\n",
    "        \n",
    "        self.obs_memory = np.empty((self.mem_size, *input_size), dtype=np.float32)\n",
    "        self.action_memory = np.empty((self.mem_size), dtype=np.int64)\n",
    "        self.reward_memory = np.empty((self.mem_size), dtype=np.float32)\n",
    "        self.next_obs_memory = np.empty((self.mem_size, *input_size), dtype=np.float32)\n",
    "        self.terminal_memory = np.empty((self.mem_size), dtype=np.bool)\n",
    "        \n",
    "    def add_memory(self, obs, action, reward, next_obs, done):\n",
    "        self.obs_memory[self.index] = obs\n",
    "        self.action_memory[self.index] = action\n",
    "        self.reward_memory[self.index] = reward\n",
    "        self.next_obs_memory[self.index] = next_obs\n",
    "        self.terminal_memory[self.index] = done\n",
    "        self.index += 1\n",
    "        self.index %= self.mem_size\n",
    "\n",
    "    def get_memory_batch(self):\n",
    "        idxs = np.random.choice(len(self), self.batch_size, replace=False)\n",
    "        \n",
    "        obss = self.obs_memory[idxs]\n",
    "        actions = self.action_memory[idxs]\n",
    "        rewards = self.reward_memory[idxs]\n",
    "        next_obss = self.next_obs_memory[idxs]\n",
    "        dones = self.terminal_memory[idxs]\n",
    "        \n",
    "        return obss, actions, rewards, next_obss, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return min(self.index, self.mem_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function approximator of the Q-Function\n",
    "class QNN(nn.Module):\n",
    "    '''\n",
    "    A PyTorch based neural network approximator of the Q-function.\n",
    "    The network can include a flexible number of input, hidden and output nodes.\n",
    "    No CNN layers are included in this architecture.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, input_size, hidden_sizes, action_size, lr, save_dir, name):\n",
    "        super(QNN, self).__init__()\n",
    "        self.output_file = os.path.join(save_dir, name)\n",
    "        self.input_layer = nn.Linear(*input_size, hidden_sizes[0])\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]) \n",
    "                                            for i in range(len(hidden_sizes)-1)])\n",
    "        self.output_layer = nn.Linear(hidden_sizes[-1], action_size)\n",
    "        \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.loss = nn.MSELoss()\n",
    "        self.optimizer = optim.RMSprop(self.parameters(), lr=lr)\n",
    "        self.to(self.device)\n",
    "        \n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        x = F.relu(self.input_layer(x))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = F.relu(hidden_layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "        \n",
    "    \n",
    "    def save(self):\n",
    "        torch.save(self.state_dict(), self.output_file)\n",
    "    \n",
    "    def load(self):\n",
    "        self.load_state_dict(torch.load(self.output_file))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    '''\n",
    "    DQN Agent combining a memory buffer and separate online and target neural networks\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, input_size, hidden_sizes, action_size, min_epsilon, max_epsilon, epsilon_decay, \n",
    "                 gamma, lr, mem_size, batch_size, save_dir, name):\n",
    "        self.replay_buffer = ReplayBuffer(input_size, mem_size, batch_size)\n",
    "        self.qnn_target = QNN(input_size, hidden_sizes, action_size, lr, save_dir, name=name+'_target.pt')\n",
    "        self.qnn_online = QNN(input_size, hidden_sizes, action_size, lr, save_dir, name=name+'_online.pt')\n",
    "        self.replace_target_network()\n",
    "        \n",
    "        self.epsilon = max_epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.gamma = gamma\n",
    "        self.action_size = action_size\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def epsilon_greedy(self, obs):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            action = self.greedy(obs)\n",
    "        else:\n",
    "            action = np.random.choice(self.action_size)\n",
    "        return action\n",
    "    \n",
    "    def greedy(self, obs):\n",
    "        with torch.no_grad():\n",
    "            obs = torch.from_numpy(obs).to(self.qnn_online.device).float()\n",
    "            action = np.argmax(self.qnn_online.forward(obs).detach().numpy())\n",
    "            return action\n",
    "    \n",
    "    def decrement_epsilon(self):\n",
    "        if self.epsilon <= self.min_epsilon:\n",
    "            return\n",
    "        \n",
    "        epsilon = self.epsilon - self.epsilon_decay\n",
    "        self.epsilon = max(epsilon, self.min_epsilon)\n",
    "    \n",
    "    def add_memory(self, obs, action, reward, next_obs, done):\n",
    "        self.replay_buffer.add_memory(obs, action, reward, next_obs, done)\n",
    "    \n",
    "    def get_memory_batch(self):\n",
    "        obss, actions, rewards, next_obss, dones = self.replay_buffer.get_memory_batch()\n",
    "        device = self.qnn_online.device\n",
    "        obss = torch.from_numpy(obss).to(device)\n",
    "        actions = torch.from_numpy(actions).to(device)\n",
    "        rewards = torch.from_numpy(rewards).to(device)\n",
    "        next_obss = torch.from_numpy(next_obss).to(device)\n",
    "        dones = torch.from_numpy(dones).to(device)\n",
    "        return obss, actions, rewards, next_obss, dones\n",
    "        \n",
    "    def learn(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        self.qnn_online.optimizer.zero_grad()\n",
    "        obss, actions, rewards, next_obss, dones = self.get_memory_batch()\n",
    "        with torch.no_grad():\n",
    "            target = rewards + self.gamma * torch.max(self.qnn_target.forward(next_obss).detach(), dim=1)[0] \\\n",
    "                * torch.logical_not(dones)\n",
    "        target = target.unsqueeze(1)\n",
    "        online = self.qnn_online.forward(obss).gather(dim=1, index=actions.unsqueeze(1))\n",
    "\n",
    "        loss = self.qnn_online.loss(online, target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.qnn_online.parameters(), 1.0)\n",
    "        \n",
    "        self.qnn_online.optimizer.step()\n",
    "        self.decrement_epsilon()\n",
    "    \n",
    "    def save(self):\n",
    "        self.qnn_online.save()\n",
    "    \n",
    "    def load(self):\n",
    "        self.qnn_online.load()\n",
    "        \n",
    "    def replace_target_network(self):\n",
    "        self.qnn_target.load_state_dict(self.qnn_online.state_dict())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environments config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVS = [\n",
    "    {\n",
    "        'NAME': 'CartPole-v1',\n",
    "        'UNWRAPPED': False,\n",
    "        'SOLVED_REWARD': 475.0\n",
    "    },\n",
    "    {\n",
    "        'NAME': 'MountainCar-v0',\n",
    "        'UNWRAPPED': True,\n",
    "        'SOLVED_REWARD': -110.0\n",
    "    },\n",
    "    {\n",
    "        'NAME': 'LunarLander-v2',\n",
    "        'UNWRAPPED': False,\n",
    "        'SOLVED_REWARD': 200\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These parameters are used for all the environments defined above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZES = (512, 128)\n",
    "EPISODES = 100000\n",
    "MIN_EPSILON=0.3\n",
    "MAX_EPSILON=1\n",
    "EPSILON_DECAY=0.0001\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.00025\n",
    "MEMORY_SIZE = 1000000\n",
    "BATCH_SIZE = 32\n",
    "REPLACE_TARGET = 100\n",
    "SAVE_DIR = './progress'\n",
    "RESULTS_DIR = './results'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TRAINING ENVIRONMENT CartPole-v1---\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3674be8a670e4cd09bb713b88599f4ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=100000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_mean_100: 266.16, current_mean: 249.85,     best_eval_score: 475.7, current_eval_mean 475.743\n",
      "\n",
      "---GOAL REACHED AFTER 397 EPISODES---\n",
      "\n",
      "\n",
      "\n",
      "--- TRAINING ENVIRONMENT MountainCar-v0---\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c215b0543774465aea54409e78891bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=100000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_mean_100: -153.89, current_mean: -161.25,     best_eval_score: -109.97, current_eval_mean -109.97\n",
      "\n",
      "---GOAL REACHED AFTER 719 EPISODES---\n",
      "\n",
      "\n",
      "\n",
      "--- TRAINING ENVIRONMENT LunarLander-v2---\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33c16c020f7b4763bd9c71dc2e72d499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=100000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_mean_100: 77.97775699555334, current_mean: 60.2079115026465,     best_eval_score: 200.52001041486616, current_eval_mean 200.520010414866163374682\n",
      "\n",
      "---GOAL REACHED AFTER 513 EPISODES---\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Main loop\n",
    "for env in ENVS:\n",
    "\n",
    "    NAME = env['NAME']\n",
    "    UNWRAPPED = env['UNWRAPPED']\n",
    "    SOLVED_REWARD = env['SOLVED_REWARD']\n",
    "    \n",
    "    print(f'--- TRAINING ENVIRONMENT {NAME}---\\n')\n",
    "    \n",
    "    if UNWRAPPED:  \n",
    "        ENV = gym.make(NAME).unwrapped\n",
    "    else: \n",
    "        ENV = gym.make(NAME)\n",
    "    ACTION_SIZE = ENV.action_space.n\n",
    "    INPUT_SIZE = ENV.observation_space.shape\n",
    "\n",
    "    agent = DQN(INPUT_SIZE, HIDDEN_SIZES, ACTION_SIZE, \n",
    "                min_epsilon=MIN_EPSILON, max_epsilon=MAX_EPSILON, epsilon_decay=EPSILON_DECAY,\n",
    "                gamma=GAMMA, lr=LEARNING_RATE, \n",
    "                mem_size=MEMORY_SIZE, batch_size=BATCH_SIZE, \n",
    "                save_dir=SAVE_DIR, name=NAME)\n",
    "\n",
    "\n",
    "    reward_tracking = []\n",
    "    eval_tracking = []\n",
    "    best_mean = -1000\n",
    "    best_eval_score = -1000\n",
    "    reward_mean = -1000\n",
    "    eval_mean = -1000\n",
    "    counter = 0\n",
    "    for episode in tqdm(range(EPISODES)):\n",
    "        obs, done = ENV.reset(), False\n",
    "        reward_sum = 0\n",
    "        while not done:\n",
    "            action = agent.epsilon_greedy(obs)\n",
    "            next_obs, reward, done, info = ENV.step(action)\n",
    "            is_truncated = 'TimeLimit.truncated' in info and info['TimeLimit.truncated']\n",
    "            terminal = done and (not is_truncated)\n",
    "            reward_sum += reward\n",
    "            agent.add_memory(obs, action, reward, next_obs, terminal)\n",
    "            obs = next_obs\n",
    "            agent.learn()\n",
    "            if(counter + 1) % REPLACE_TARGET == 0:\n",
    "                agent.replace_target_network()\n",
    "                counter = 0\n",
    "            counter += 1\n",
    "        reward_tracking.append(reward_sum)\n",
    "\n",
    "\n",
    "        #testing agent\n",
    "        obs, done = ENV.reset(), False\n",
    "        eval_score = 0\n",
    "        while not done:\n",
    "            action = agent.greedy(obs)\n",
    "            next_obs, reward, done, _ = ENV.step(action)\n",
    "            eval_score += reward\n",
    "            obs = next_obs\n",
    "\n",
    "            # this is necessary for Mountain Car, as in earlier episodes the policy might be stuck during evaluation\n",
    "            # this is the case because the evaluation policy is purely greedy and the environment is unwrapped\n",
    "            if eval_score <= -200:\n",
    "                done = True\n",
    "        eval_tracking.append(eval_score)\n",
    "\n",
    "        # OUTPUT INFO\n",
    "        if (episode > 100):\n",
    "            reward_mean = np.array(reward_tracking[-100:]).mean()\n",
    "            eval_mean = np.array(eval_tracking[-100:]).mean()\n",
    "            if reward_mean > best_mean:\n",
    "                best_mean = reward_mean\n",
    "            if eval_mean > best_eval_score:\n",
    "                best_eval_score = eval_mean\n",
    "                agent.save()\n",
    "\n",
    "            print(f'best_mean_100: {best_mean}, current_mean: {reward_mean}, \\\n",
    "    best_eval_score: {best_eval_score}, current_eval_mean {eval_mean}', end='\\r')\n",
    "\n",
    "            if eval_mean >= SOLVED_REWARD:\n",
    "                print('\\n', flush=True)\n",
    "                print(f'---GOAL REACHED AFTER {episode} EPISODES---')\n",
    "                print('\\n')\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(eval_tracking)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play a game with the best agent and save the results as gif\n",
    "for env in ENVS:\n",
    "    frames = []\n",
    "    \n",
    "    NAME = env['NAME']\n",
    "    \n",
    "    ENV = gym.make(NAME)\n",
    "    ACTION_SIZE = ENV.action_space.n\n",
    "    INPUT_SIZE = ENV.observation_space.shape\n",
    "    \n",
    "    agent = DQN(INPUT_SIZE, HIDDEN_SIZES, ACTION_SIZE, \n",
    "                min_epsilon=MIN_EPSILON, max_epsilon=MAX_EPSILON, epsilon_decay=EPSILON_DECAY,\n",
    "                gamma=GAMMA, lr=LEARNING_RATE, \n",
    "                mem_size=MEMORY_SIZE, batch_size=BATCH_SIZE, save_dir=SAVE_DIR, name=NAME)\n",
    "    agent.load()\n",
    "    ENV = gym.make(NAME)\n",
    "#     ENV = wrappers.Monitor(ENV, \"./gym-results\", force=True)\n",
    "    obs, done = ENV.reset(), False\n",
    "    frames.append(Image.fromarray(ENV.render(mode='rgb_array')))\n",
    "    eval_score = 0\n",
    "    while not done:\n",
    "        action = agent.greedy(obs)\n",
    "        next_obs, reward, done, _ = ENV.step(action)\n",
    "        eval_score += reward\n",
    "        obs = next_obs\n",
    "        frames.append(Image.fromarray(ENV.render(mode='rgb_array')))\n",
    "    ENV.close()\n",
    "    \n",
    "    path = os.path.join(RESULTS_DIR, NAME+'.gif')\n",
    "    with open(path, 'wb') as f:\n",
    "        im = Image.new('RGB', frames[0].size)\n",
    "        im.save(f, save_all=True, append_images=frames, loop=0, duration=25) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SegmentLocal](results/CartPole-v1.gif \"segment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./results/MountainCar-v0.gif\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython import display\n",
    "display.HTML('<img src=\"{}\">'.format('./results/MountainCar-v0.gif'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./results/LunarLander-v2.gif\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython import display\n",
    "display.HTML('<img src=\"{}\">'.format('./results/LunarLander-v2.gif'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
