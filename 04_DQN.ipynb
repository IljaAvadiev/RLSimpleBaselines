{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network (DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook is based on \"Human-Level Control through deep reinforcement learning\" by Mnih et. al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from itertools import count\n",
    "import os\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replay buffer for memory replay\n",
    "class ReplayBuffer():\n",
    "    '''\n",
    "    Stores memories up to a maximum of mem_size. The memories can be batched to use in training.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, input_size, mem_size=10000, batch_size=64):\n",
    "        self.mem_size = mem_size\n",
    "        self.index = 0\n",
    "        self.batch_size=batch_size\n",
    "        \n",
    "        self.obs_memory = np.empty((self.mem_size, *input_size), dtype=np.float32)\n",
    "        self.action_memory = np.empty((self.mem_size), dtype=np.int64)\n",
    "        self.reward_memory = np.empty((self.mem_size), dtype=np.float32)\n",
    "        self.next_obs_memory = np.empty((self.mem_size, *input_size), dtype=np.float32)\n",
    "        self.terminal_memory = np.empty((self.mem_size), dtype=np.bool)\n",
    "        \n",
    "    def add_memory(self, obs, action, reward, next_obs, done):\n",
    "        self.obs_memory[self.index] = obs\n",
    "        self.action_memory[self.index] = action\n",
    "        self.reward_memory[self.index] = reward\n",
    "        self.next_obs_memory[self.index] = next_obs\n",
    "        self.terminal_memory[self.index] = done\n",
    "        self.index += 1\n",
    "        self.index %= self.mem_size\n",
    "\n",
    "    def get_memory_batch(self):\n",
    "        idxs = np.random.choice(len(self), self.batch_size, replace=False)\n",
    "        \n",
    "        obss = self.obs_memory[idxs]\n",
    "        actions = self.action_memory[idxs]\n",
    "        rewards = self.reward_memory[idxs]\n",
    "        next_obss = self.next_obs_memory[idxs]\n",
    "        dones = self.terminal_memory[idxs]\n",
    "        \n",
    "        return obss, actions, rewards, next_obss, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return min(self.index, self.mem_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prioritized experience replay\n",
    "class PrioritizedExperienceReplay():\n",
    "    '''\n",
    "    Prioritized Experience Replay by Tom Schaul, John Quan, Ioannis Antonoglou and David Silver.\n",
    "    https://arxiv.org/pdf/1511.05952.pdf\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, input_size, mem_size=10000, batch_size=64, alpha=0.5, beta=0.4, beta_factor=0.9999):\n",
    "        self.mem_size = mem_size\n",
    "        self.index = 0\n",
    "        self.batch_size=batch_size\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.beta_factor = beta_factor\n",
    "        self.idxs = None\n",
    "        \n",
    "        self.obs_memory = np.empty((self.mem_size, *input_size), dtype=np.float32)\n",
    "        self.action_memory = np.empty((self.mem_size), dtype=np.int64)\n",
    "        self.reward_memory = np.empty((self.mem_size), dtype=np.float32)\n",
    "        self.next_obs_memory = np.empty((self.mem_size, *input_size), dtype=np.float32)\n",
    "        self.terminal_memory = np.empty((self.mem_size), dtype=np.bool)\n",
    "        self.priority_memory = np.empty((self.mem_size), dtype=np.float32)\n",
    "        \n",
    "    def add_memory(self, obs, action, reward, next_obs, done):\n",
    "        if self.index == 0: \n",
    "            priority = 1\n",
    "        else:\n",
    "            # ensure that first time experience is drawn with the highest probability\n",
    "            priority = self.priority_memory[:len(self)].max()\n",
    "            \n",
    "        self.obs_memory[self.index] = obs\n",
    "        self.action_memory[self.index] = action\n",
    "        self.reward_memory[self.index] = reward\n",
    "        self.next_obs_memory[self.index] = next_obs\n",
    "        self.terminal_memory[self.index] = done\n",
    "        self.priority_memory[self.index] = priority\n",
    "        self.index += 1\n",
    "        self.index %= self.mem_size\n",
    "\n",
    "    def get_memory_batch(self):\n",
    "        numerator = self.priority_memory[:len(self)] ** self.alpha\n",
    "        denominator = numerator.sum()\n",
    "        \n",
    "        p = numerator / denominator\n",
    "        self.idxs = np.random.choice(len(self), self.batch_size, p=p, replace=False)\n",
    "        \n",
    "        weights = (p[self.idxs] * len(self))**(-self.beta)\n",
    "        max_weight = weights.max()\n",
    "        weights /= max_weight\n",
    "        \n",
    "        obss = self.obs_memory[self.idxs]\n",
    "        actions = self.action_memory[self.idxs]\n",
    "        rewards = self.reward_memory[self.idxs]\n",
    "        next_obss = self.next_obs_memory[self.idxs]\n",
    "        dones = self.terminal_memory[self.idxs]\n",
    "        \n",
    "        self.beta *= self.beta_factor\n",
    "        \n",
    "        return obss, actions, rewards, next_obss, dones, weights\n",
    "    \n",
    "    def update_memory_batch(self, priorities):\n",
    "        self.priority_memory[self.idxs] = np.abs(priorities).squeeze()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return min(self.index, self.mem_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function approximator of the Q-Function\n",
    "class QNN(nn.Module):\n",
    "    '''\n",
    "    A PyTorch based neural network approximator of the Q-function.\n",
    "    The network can include a flexible number of input, hidden and output nodes.\n",
    "    No CNN layers are included in this architecture.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, input_size, hidden_sizes, action_size, lr, save_dir, name):\n",
    "        super(QNN, self).__init__()\n",
    "        self.output_file = os.path.join(save_dir, name)\n",
    "        self.input_layer = nn.Linear(*input_size, hidden_sizes[0])\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]) \n",
    "                                            for i in range(len(hidden_sizes)-1)])\n",
    "        self.output_layer = nn.Linear(hidden_sizes[-1], action_size)\n",
    "        \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.loss = nn.MSELoss()\n",
    "        self.optimizer = optim.RMSprop(self.parameters(), lr=lr)\n",
    "        self.to(self.device)\n",
    "        \n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        x = F.relu(self.input_layer(x))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = F.relu(hidden_layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "        \n",
    "    \n",
    "    def save(self):\n",
    "        torch.save(self.state_dict(), self.output_file)\n",
    "    \n",
    "    def load(self):\n",
    "        self.load_state_dict(torch.load(self.output_file))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duelling function approximator separating the Q function into the V and the A part\n",
    "class Duell_QNN(nn.Module):\n",
    "    '''\n",
    "    A PyTorch based neural network approximator of the Q-function.\n",
    "    The network can include a flexible number of input, hidden and output nodes.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, input_size, hidden_sizes, action_size, lr, save_dir, name):\n",
    "        super(Duell_QNN, self).__init__()\n",
    "        self.output_file = os.path.join(save_dir, name)\n",
    "        self.input_layer = nn.Linear(*input_size, hidden_sizes[0])\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]) \n",
    "                                            for i in range(len(hidden_sizes)-1)])\n",
    "        \n",
    "        \n",
    "        self.v = nn.Linear(hidden_sizes[-1], 1)\n",
    "        self.a = nn.Linear(hidden_sizes[-1], action_size)\n",
    "                \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.loss = nn.MSELoss()\n",
    "        self.optimizer = optim.RMSprop(self.parameters(), lr=lr)\n",
    "        self.to(self.device)\n",
    "        \n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        x = F.relu(self.input_layer(x))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = F.relu(hidden_layer(x))\n",
    "        \n",
    "        v = self.v(x)\n",
    "        a = self.a(x)\n",
    "        a_mean = a.mean(axis=1, keepdim=True)\n",
    "              \n",
    "        x = v + (a - a_mean)\n",
    "        return x\n",
    "        \n",
    "    \n",
    "    def save(self):\n",
    "        torch.save(self.state_dict(), self.output_file)\n",
    "    \n",
    "    def load(self):\n",
    "        self.load_state_dict(torch.load(self.output_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    '''\n",
    "    DQN Agent combining a memory buffer and separate online and target neural networks\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, net_type, duelling, memory_type, input_size, hidden_sizes, action_size, \n",
    "                 min_epsilon, max_epsilon, epsilon_decay, \n",
    "                 gamma, lr, mem_size, batch_size, save_dir, name):\n",
    "        self.memory_type = memory_type\n",
    "        \n",
    "        if self.memory_type == 'UNIFORM':\n",
    "            self.replay_buffer = ReplayBuffer(input_size, mem_size, batch_size)\n",
    "        elif self.memory_type == 'PER':\n",
    "            self.replay_buffer = PrioritizedExperienceReplay(input_size, mem_size, batch_size)\n",
    "        else:\n",
    "            raise ValueError('Invalid memory type for replay buffer')\n",
    "        \n",
    "        if duelling:\n",
    "            self.qnn_target = Duell_QNN(input_size, hidden_sizes, action_size, lr, save_dir, \n",
    "                                  name=name+net_type+'duelling'+'_target.pt')\n",
    "            self.qnn_online = Duell_QNN(input_size, hidden_sizes, action_size, lr, save_dir, \n",
    "                                  name=name+net_type+'duelling'+'_online.pt')\n",
    "        else:\n",
    "            self.qnn_target = QNN(input_size, hidden_sizes, action_size, lr, save_dir, \n",
    "                                  name=name+net_type+'_target.pt')\n",
    "            self.qnn_online = QNN(input_size, hidden_sizes, action_size, lr, save_dir, \n",
    "                                  name=name+net_type+'_online.pt')\n",
    "        \n",
    "        self.replace_target_network()\n",
    "        \n",
    "        self.net_type = net_type\n",
    "        self.epsilon = max_epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.gamma = gamma\n",
    "        self.action_size = action_size\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def epsilon_greedy(self, obs):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            action = self.greedy(obs)\n",
    "        else:\n",
    "            action = np.random.choice(self.action_size)\n",
    "        return action\n",
    "    \n",
    "    def greedy(self, obs):\n",
    "        with torch.no_grad():\n",
    "            obs = torch.from_numpy(obs).to(self.qnn_online.device).float().unsqueeze(0)\n",
    "            action = np.argmax(self.qnn_online.forward(obs).detach().numpy())\n",
    "            return action\n",
    "    \n",
    "    def decrement_epsilon(self):\n",
    "        if self.epsilon <= self.min_epsilon:\n",
    "            return\n",
    "        \n",
    "        epsilon = self.epsilon - self.epsilon_decay\n",
    "        self.epsilon = max(epsilon, self.min_epsilon)\n",
    "    \n",
    "    def add_memory(self, obs, action, reward, next_obs, done):\n",
    "        self.replay_buffer.add_memory(obs, action, reward, next_obs, done)\n",
    "    \n",
    "    def get_memory_batch(self):\n",
    "        if self.memory_type == 'UNIFORM':\n",
    "            obss, actions, rewards, next_obss, dones = self.replay_buffer.get_memory_batch()\n",
    "        elif self.memory_type == 'PER':\n",
    "            obss, actions, rewards, next_obss, dones, weights = self.replay_buffer.get_memory_batch()\n",
    "        device = self.qnn_online.device\n",
    "        obss = torch.from_numpy(obss).to(device)\n",
    "        actions = torch.from_numpy(actions).to(device)\n",
    "        rewards = torch.from_numpy(rewards).to(device)\n",
    "        next_obss = torch.from_numpy(next_obss).to(device)\n",
    "        dones = torch.from_numpy(dones).to(device)\n",
    "        if self.memory_type == 'UNIFORM':\n",
    "            return obss, actions, rewards, next_obss, dones\n",
    "        elif self.memory_type == 'PER':\n",
    "            weights = torch.from_numpy(weights).to(device)\n",
    "            return obss, actions, rewards, next_obss, dones, weights\n",
    "        \n",
    "    def learn(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        self.qnn_online.optimizer.zero_grad()\n",
    "        if self.memory_type == 'UNIFORM':\n",
    "            obss, actions, rewards, next_obss, dones = self.get_memory_batch()\n",
    "        if self.memory_type == 'PER':\n",
    "            obss, actions, rewards, next_obss, dones, weights = self.get_memory_batch()\n",
    "        with torch.no_grad():\n",
    "            if self.net_type == 'DQN':\n",
    "#                 target = rewards + self.gamma * torch.max(self.qnn_target.forward(next_obss).detach(), dim=1)[0] \\\n",
    "#                     * torch.logical_not(dones)\n",
    "                indices = torch.max(self.qnn_target.forward(next_obss).detach(), dim=1)[1]\n",
    "            elif self.net_type == 'DDQN':\n",
    "                indices = torch.max(self.qnn_online.forward(next_obss).detach(), dim=1)[1]\n",
    "            else:\n",
    "                raise ValueError('net_type should be DQN or DDQN')\n",
    "            \n",
    "\n",
    "            next_values = self.qnn_target.forward(next_obss)[np.arange(self.batch_size), indices]\n",
    "            target = rewards + self.gamma * next_values * torch.logical_not(dones)\n",
    "            \n",
    "        target = target.unsqueeze(1)\n",
    "        online = self.qnn_online.forward(obss).gather(dim=1, index=actions.unsqueeze(1))\n",
    "\n",
    "        if self.memory_type == 'UNIFORM':\n",
    "            loss = self.qnn_online.loss(online, target)\n",
    "        elif self.memory_type == 'PER':\n",
    "            error = target - online\n",
    "            loss = (weights*error).pow(2).mul(0.5).mean() \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.qnn_online.parameters(), 1.0)\n",
    "        \n",
    "        self.qnn_online.optimizer.step()\n",
    "        self.decrement_epsilon()\n",
    "        \n",
    "        if self.memory_type == 'PER':\n",
    "            self.replay_buffer.update_memory_batch(error.detach().cpu().numpy())\n",
    "    \n",
    "    def save(self):\n",
    "        self.qnn_online.save()\n",
    "    \n",
    "    def load(self):\n",
    "        self.qnn_online.load()\n",
    "        \n",
    "    def replace_target_network(self):\n",
    "        self.qnn_target.load_state_dict(self.qnn_online.state_dict())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environments config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVS = [\n",
    "    {\n",
    "        'NAME': 'CartPole-v1',\n",
    "        'UNWRAPPED': False,\n",
    "        'SOLVED_REWARD': 475.0,\n",
    "        'NET_TYPE': 'DQN',\n",
    "        'RESULTS': [],\n",
    "        'DUELLING': False,\n",
    "        'MEMORY_TYPE': 'UNIFORM'\n",
    "    },\n",
    "    {\n",
    "        'NAME': 'CartPole-v1',\n",
    "        'UNWRAPPED': False,\n",
    "        'SOLVED_REWARD': 475.0,\n",
    "        'NET_TYPE': 'DDQN',\n",
    "        'RESULTS': [],\n",
    "        'DUELLING': False,\n",
    "        'MEMORY_TYPE': 'UNIFORM'\n",
    "    },\n",
    "    {\n",
    "        'NAME': 'CartPole-v1',\n",
    "        'UNWRAPPED': False,\n",
    "        'SOLVED_REWARD': 475.0,\n",
    "        'NET_TYPE': 'DDQN',\n",
    "        'RESULTS': [],\n",
    "        'DUELLING': True,\n",
    "        'MEMORY_TYPE': 'UNIFORM'\n",
    "    },\n",
    "    {\n",
    "        'NAME': 'CartPole-v1',\n",
    "        'UNWRAPPED': False,\n",
    "        'SOLVED_REWARD': 475.0,\n",
    "        'NET_TYPE': 'DDQN',\n",
    "        'RESULTS': [],\n",
    "        'DUELLING': True,\n",
    "        'MEMORY_TYPE': 'PER'\n",
    "    },\n",
    "#     {\n",
    "#         'NAME': 'CartPole-v1',\n",
    "#         'UNWRAPPED': False,\n",
    "#         'SOLVED_REWARD': 475.0,\n",
    "#         'NET_TYPE': 'DQN',\n",
    "#         'RESULTS': []\n",
    "#     },\n",
    "#     {\n",
    "#         'NAME': 'CartPole-v1',\n",
    "#         'UNWRAPPED': False,\n",
    "#         'SOLVED_REWARD': 475.0,\n",
    "#         'NET_TYPE': 'DDQN',\n",
    "#         'RESULTS': []\n",
    "#     },\n",
    "#     {\n",
    "#         'NAME': 'MountainCar-v0',\n",
    "#         'UNWRAPPED': True,\n",
    "#         'SOLVED_REWARD': -110.0,\n",
    "#         'NET_TYPE': 'DQN',\n",
    "#         'RESULTS': []\n",
    "#     },\n",
    "#     {\n",
    "#         'NAME': 'MountainCar-v0',\n",
    "#         'UNWRAPPED': True,\n",
    "#         'SOLVED_REWARD': -110.0,\n",
    "#         'NET_TYPE': 'DDQN',\n",
    "#         'RESULTS': []\n",
    "#     },\n",
    "#     {\n",
    "#         'NAME': 'LunarLander-v2',\n",
    "#         'UNWRAPPED': False,\n",
    "#         'SOLVED_REWARD': 200,\n",
    "#         'NET_TYPE': 'DQN',\n",
    "#         'RESULTS': []\n",
    "#     },\n",
    "#     {\n",
    "#         'NAME': 'LunarLander-v2',\n",
    "#         'UNWRAPPED': False,\n",
    "#         'SOLVED_REWARD': 200,\n",
    "#         'NET_TYPE': 'DDQN',\n",
    "#         'RESULTS': []\n",
    "#     }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These parameters are used for all the environments defined above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZES = (512, 128)\n",
    "EPISODES = 2000\n",
    "MIN_EPSILON=0.3\n",
    "MAX_EPSILON=1\n",
    "EPSILON_DECAY=0.0001\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.00025\n",
    "MEMORY_SIZE = 1000000\n",
    "BATCH_SIZE = 32\n",
    "REPLACE_TARGET = 1000\n",
    "SAVE_DIR = './progress'\n",
    "RESULTS_DIR = './results'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TRAINING ENVIRONMENT CartPole-v1---\n",
      "\n",
      "--- MODEL TYPE --- DQN---\n",
      "\n",
      "--- DUELLING --- False---\n",
      "\n",
      "--- MEMORY TYPE --- UNIFORM---\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e59aec81447046e68fa196656e048113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 177477, best_mean_100: 229.85, current_mean: 167.13,     best_eval_100: 476.83, eval_100 476.83, eval_10 500.0\n",
      "\n",
      "---GOAL REACHED AFTER 1542 EPISODES---\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "--- TRAINING ENVIRONMENT CartPole-v1---\n",
      "\n",
      "--- MODEL TYPE --- DDQN---\n",
      "\n",
      "--- DUELLING --- False---\n",
      "\n",
      "--- MEMORY TYPE --- UNIFORM---\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7e448a057fa4ccbb208e343d8652e73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 98705, best_mean_100: 150.55, current_mean: 74.4,     best_eval_100: 476.2, eval_100 476.2, eval_10 500.00.00\n",
      "\n",
      "---GOAL REACHED AFTER 1142 EPISODES---\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "--- TRAINING ENVIRONMENT CartPole-v1---\n",
      "\n",
      "--- MODEL TYPE --- DDQN---\n",
      "\n",
      "--- DUELLING --- True---\n",
      "\n",
      "--- MEMORY TYPE --- UNIFORM---\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a43585d1128a42f8819c335eedc023d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 182059, best_mean_100: 170.77, current_mean: 72.45,     best_eval_100: 457.73, eval_100 242.18, eval_10 226.67\n",
      "----------------------------------------------------------------------------------------------------\n",
      "--- TRAINING ENVIRONMENT CartPole-v1---\n",
      "\n",
      "--- MODEL TYPE --- DDQN---\n",
      "\n",
      "--- DUELLING --- True---\n",
      "\n",
      "--- MEMORY TYPE --- PER---\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2770793256904ba4ba65fc4fbe57c0dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 125657, best_mean_100: 219.48, current_mean: 212.29,     best_eval_100: 477.05, eval_100 477.05, eval_10 500.0\n",
      "\n",
      "---GOAL REACHED AFTER 1164 EPISODES---\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Main loop\n",
    "for env in ENVS:\n",
    "    \n",
    "    NAME = env['NAME']\n",
    "    UNWRAPPED = env['UNWRAPPED']\n",
    "    SOLVED_REWARD = env['SOLVED_REWARD']\n",
    "    NET_TYPE = env['NET_TYPE']\n",
    "    DUELLING = env['DUELLING']\n",
    "    MEMORY_TYPE = env['MEMORY_TYPE']\n",
    "    \n",
    "    print(f'--- TRAINING ENVIRONMENT {NAME}---\\n')\n",
    "    print(f'--- MODEL TYPE --- {NET_TYPE}---\\n')\n",
    "    print(f'--- DUELLING --- {DUELLING}---\\n')\n",
    "    print(f'--- MEMORY TYPE --- {MEMORY_TYPE}---\\n')\n",
    "    \n",
    "    if UNWRAPPED:  \n",
    "        ENV = gym.make(NAME).unwrapped\n",
    "    else: \n",
    "        ENV = gym.make(NAME)\n",
    "    \n",
    "    ENV.seed(42)\n",
    "    ACTION_SIZE = ENV.action_space.n\n",
    "    INPUT_SIZE = ENV.observation_space.shape\n",
    "\n",
    "    agent = DQN(NET_TYPE, DUELLING, MEMORY_TYPE, INPUT_SIZE, HIDDEN_SIZES, ACTION_SIZE, \n",
    "                min_epsilon=MIN_EPSILON, max_epsilon=MAX_EPSILON, epsilon_decay=EPSILON_DECAY,\n",
    "                gamma=GAMMA, lr=LEARNING_RATE, \n",
    "                mem_size=MEMORY_SIZE, batch_size=BATCH_SIZE, \n",
    "                save_dir=SAVE_DIR, name=NAME)\n",
    "\n",
    "\n",
    "    reward_tracking = []\n",
    "    eval_tracking = env['RESULTS']\n",
    "    best_mean = -1000\n",
    "    best_eval_score = -1000\n",
    "    reward_mean = -1000\n",
    "    eval_mean = -1000\n",
    "    counter = 0\n",
    "    for episode in tqdm(range(EPISODES)):\n",
    "        obs, done = ENV.reset(), False\n",
    "        reward_sum = 0\n",
    "        while not done:\n",
    "            action = agent.epsilon_greedy(obs)\n",
    "            next_obs, reward, done, info = ENV.step(action)\n",
    "            is_truncated = 'TimeLimit.truncated' in info and info['TimeLimit.truncated']\n",
    "            terminal = done and (not is_truncated)\n",
    "            reward_sum += reward\n",
    "            agent.add_memory(obs, action, reward, next_obs, terminal)\n",
    "            obs = next_obs\n",
    "            agent.learn()\n",
    "            if(counter + 1) % REPLACE_TARGET == 0:\n",
    "                agent.replace_target_network()\n",
    "#                 counter = 0\n",
    "            counter += 1\n",
    "        reward_tracking.append(reward_sum)\n",
    "\n",
    "\n",
    "        #testing agent\n",
    "        obs, done = ENV.reset(), False\n",
    "        eval_score = 0\n",
    "        while not done:\n",
    "            action = agent.greedy(obs)\n",
    "            next_obs, reward, done, _ = ENV.step(action)\n",
    "            eval_score += reward\n",
    "            obs = next_obs\n",
    "\n",
    "            # this is necessary for Mountain Car, as in earlier episodes the policy might be stuck during evaluation\n",
    "            # this is the case because the evaluation policy is purely greedy and the environment is unwrapped\n",
    "            if eval_score <= -200:\n",
    "                done = True\n",
    "        eval_tracking.append(eval_score)\n",
    "\n",
    "        # OUTPUT INFO\n",
    "        if (episode > 100):\n",
    "            reward_mean = np.array(reward_tracking[-100:]).mean()\n",
    "            eval_mean_10 = np.array(eval_tracking[-10:]).mean()\n",
    "            eval_mean_100 = np.array(eval_tracking[-100:]).mean()\n",
    "            if reward_mean > best_mean:\n",
    "                best_mean = reward_mean\n",
    "            if eval_mean_100 > best_eval_score:\n",
    "                best_eval_score = eval_mean_100\n",
    "                agent.save()\n",
    "\n",
    "            print(f'step: {counter}, best_mean_100: {best_mean}, current_mean: {reward_mean}, \\\n",
    "    best_eval_100: {best_eval_score}, eval_100 {eval_mean_100}, eval_10 {eval_mean_10}', end='\\r')\n",
    "\n",
    "            if eval_mean_100 >= SOLVED_REWARD:\n",
    "                print('\\n', flush=True)\n",
    "                print(f'---GOAL REACHED AFTER {episode} EPISODES---')\n",
    "                print('\\n')\n",
    "                break\n",
    "                \n",
    "    print('-'*100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(800, 600))\n",
    "# for idx, env in enumerate(ENVS):\n",
    "#     NAME = env['NAME']\n",
    "#     NET_TYPE = env['NET_TYPE']\n",
    "#     RESULTS = env['RESULTS']\n",
    "    \n",
    "#     fig.add_axes([idx%2, idx//2, 1, 1]).set_title(NAME+'_'+NET_TYPE)\n",
    "#     plt.plot(RESULTS)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play a game with the best agent and save the results as gif\n",
    "for env in ENVS:\n",
    "    frames = []\n",
    "    \n",
    "    NAME = env['NAME']\n",
    "    NET_TYPE = env['NET_TYPE']\n",
    "    DUELLING = env['DUELLING']\n",
    "    \n",
    "    ENV = gym.make(NAME)\n",
    "    ACTION_SIZE = ENV.action_space.n\n",
    "    INPUT_SIZE = ENV.observation_space.shape\n",
    "    \n",
    "    agent = DQN(NET_TYPE, DUELLING, INPUT_SIZE, HIDDEN_SIZES, ACTION_SIZE, \n",
    "                min_epsilon=MIN_EPSILON, max_epsilon=MAX_EPSILON, epsilon_decay=EPSILON_DECAY,\n",
    "                gamma=GAMMA, lr=LEARNING_RATE, \n",
    "                mem_size=MEMORY_SIZE, batch_size=BATCH_SIZE, save_dir=SAVE_DIR, name=NAME)\n",
    "    agent.load()\n",
    "    ENV = gym.make(NAME)\n",
    "    ENV = wrappers.Monitor(ENV, \"./gym-results\", force=True)\n",
    "    obs, done = ENV.reset(), False\n",
    "    frames.append(Image.fromarray(ENV.render(mode='rgb_array')))\n",
    "    eval_score = 0\n",
    "    while not done:\n",
    "        action = agent.greedy(obs)\n",
    "        next_obs, reward, done, _ = ENV.step(action)\n",
    "        eval_score += reward\n",
    "        obs = next_obs\n",
    "        frames.append(Image.fromarray(ENV.render(mode='rgb_array')))\n",
    "    ENV.close()\n",
    "    \n",
    "    path = os.path.join(RESULTS_DIR, NAME+NET_TYPE+'.gif')\n",
    "    with open(path, 'wb') as f:\n",
    "        im = Image.new('RGB', frames[0].size)\n",
    "        im.save(f, save_all=True, append_images=frames, loop=0, duration=25) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SegmentLocal](results/CartPole-v1DDQN.gif \"segment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./results/MountainCar-v0DDQN.gif\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython import display\n",
    "display.HTML('<img src=\"{}\">'.format('./results/MountainCar-v0DDQN.gif'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./results/LunarLander-v2DDQN.gif\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython import display\n",
    "display.HTML('<img src=\"{}\">'.format('./results/LunarLander-v2DDQN.gif'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
