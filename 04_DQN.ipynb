{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network (DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook is based on \"Human-Level Control through deep reinforcement learning\" by Mnih et. al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from itertools import count\n",
    "import os\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replay buffer for memory replay\n",
    "class ReplayBuffer():\n",
    "    '''\n",
    "    Stores memories up to a maximum of mem_size. The memories can be batched to use in training.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, input_size, mem_size=10000, batch_size=64):\n",
    "        self.mem_size = mem_size\n",
    "        self.index = 0\n",
    "        self.batch_size=batch_size\n",
    "        \n",
    "        self.obs_memory = np.empty((self.mem_size, *input_size), dtype=np.float32)\n",
    "        self.action_memory = np.empty((self.mem_size), dtype=np.int64)\n",
    "        self.reward_memory = np.empty((self.mem_size), dtype=np.float32)\n",
    "        self.next_obs_memory = np.empty((self.mem_size, *input_size), dtype=np.float32)\n",
    "        self.terminal_memory = np.empty((self.mem_size), dtype=np.bool)\n",
    "        \n",
    "    def add_memory(self, obs, action, reward, next_obs, done):\n",
    "        self.obs_memory[self.index] = obs\n",
    "        self.action_memory[self.index] = action\n",
    "        self.reward_memory[self.index] = reward\n",
    "        self.next_obs_memory[self.index] = next_obs\n",
    "        self.terminal_memory[self.index] = done\n",
    "        self.index += 1\n",
    "        self.index %= self.mem_size\n",
    "\n",
    "    def get_memory_batch(self):\n",
    "        idxs = np.random.choice(len(self), self.batch_size, replace=False)\n",
    "        \n",
    "        obss = self.obs_memory[idxs]\n",
    "        actions = self.action_memory[idxs]\n",
    "        rewards = self.reward_memory[idxs]\n",
    "        next_obss = self.next_obs_memory[idxs]\n",
    "        dones = self.terminal_memory[idxs]\n",
    "        \n",
    "        return obss, actions, rewards, next_obss, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return min(self.index, self.mem_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function approximator of the Q-Function\n",
    "class QNN(nn.Module):\n",
    "    '''\n",
    "    A PyTorch based neural network approximator of the Q-function.\n",
    "    The network can include a flexible number of input, hidden and output nodes.\n",
    "    No CNN layers are included in this architecture.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, input_size, hidden_sizes, action_size, lr, save_dir, name):\n",
    "        super(QNN, self).__init__()\n",
    "        self.output_file = os.path.join(save_dir, name)\n",
    "        self.input_layer = nn.Linear(*input_size, hidden_sizes[0])\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]) \n",
    "                                            for i in range(len(hidden_sizes)-1)])\n",
    "        self.output_layer = nn.Linear(hidden_sizes[-1], action_size)\n",
    "        \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.loss = nn.MSELoss()\n",
    "        self.optimizer = optim.RMSprop(self.parameters(), lr=lr)\n",
    "        self.to(self.device)\n",
    "        \n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        x = F.relu(self.input_layer(x))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = F.relu(hidden_layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "        \n",
    "    \n",
    "    def save(self):\n",
    "        torch.save(self.state_dict(), self.output_file)\n",
    "    \n",
    "    def load(self):\n",
    "        self.load_state_dict(torch.load(self.output_file))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    '''\n",
    "    DQN Agent combining a memory buffer and separate online and target neural networks\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, net_type, input_size, hidden_sizes, action_size, min_epsilon, max_epsilon, epsilon_decay, \n",
    "                 gamma, lr, mem_size, batch_size, save_dir, name):\n",
    "        self.replay_buffer = ReplayBuffer(input_size, mem_size, batch_size)\n",
    "        self.qnn_target = QNN(input_size, hidden_sizes, action_size, lr, save_dir, name=name+net_type+'_target.pt')\n",
    "        self.qnn_online = QNN(input_size, hidden_sizes, action_size, lr, save_dir, name=name+net_type+'_online.pt')\n",
    "        self.replace_target_network()\n",
    "        \n",
    "        self.net_type = net_type\n",
    "        self.epsilon = max_epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.gamma = gamma\n",
    "        self.action_size = action_size\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def epsilon_greedy(self, obs):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            action = self.greedy(obs)\n",
    "        else:\n",
    "            action = np.random.choice(self.action_size)\n",
    "        return action\n",
    "    \n",
    "    def greedy(self, obs):\n",
    "        with torch.no_grad():\n",
    "            obs = torch.from_numpy(obs).to(self.qnn_online.device).float()\n",
    "            action = np.argmax(self.qnn_online.forward(obs).detach().numpy())\n",
    "            return action\n",
    "    \n",
    "    def decrement_epsilon(self):\n",
    "        if self.epsilon <= self.min_epsilon:\n",
    "            return\n",
    "        \n",
    "        epsilon = self.epsilon - self.epsilon_decay\n",
    "        self.epsilon = max(epsilon, self.min_epsilon)\n",
    "    \n",
    "    def add_memory(self, obs, action, reward, next_obs, done):\n",
    "        self.replay_buffer.add_memory(obs, action, reward, next_obs, done)\n",
    "    \n",
    "    def get_memory_batch(self):\n",
    "        obss, actions, rewards, next_obss, dones = self.replay_buffer.get_memory_batch()\n",
    "        device = self.qnn_online.device\n",
    "        obss = torch.from_numpy(obss).to(device)\n",
    "        actions = torch.from_numpy(actions).to(device)\n",
    "        rewards = torch.from_numpy(rewards).to(device)\n",
    "        next_obss = torch.from_numpy(next_obss).to(device)\n",
    "        dones = torch.from_numpy(dones).to(device)\n",
    "        return obss, actions, rewards, next_obss, dones\n",
    "        \n",
    "    def learn(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        self.qnn_online.optimizer.zero_grad()\n",
    "        obss, actions, rewards, next_obss, dones = self.get_memory_batch()\n",
    "        with torch.no_grad():\n",
    "            if self.net_type == 'DQN':\n",
    "#                 target = rewards + self.gamma * torch.max(self.qnn_target.forward(next_obss).detach(), dim=1)[0] \\\n",
    "#                     * torch.logical_not(dones)\n",
    "                indices = torch.max(self.qnn_target.forward(next_obss).detach(), dim=1)[1]\n",
    "            elif self.net_type == 'DDQN':\n",
    "                indices = torch.max(self.qnn_online.forward(next_obss).detach(), dim=1)[1]\n",
    "            else:\n",
    "                raise ValueError('net_type should be DQN or DDQN')\n",
    "            \n",
    "\n",
    "            next_values = self.qnn_target.forward(next_obss)[np.arange(self.batch_size), indices]\n",
    "            target = rewards + self.gamma * next_values * torch.logical_not(dones)\n",
    "            \n",
    "        target = target.unsqueeze(1)\n",
    "        online = self.qnn_online.forward(obss).gather(dim=1, index=actions.unsqueeze(1))\n",
    "\n",
    "        loss = self.qnn_online.loss(online, target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.qnn_online.parameters(), 1.0)\n",
    "        \n",
    "        self.qnn_online.optimizer.step()\n",
    "        self.decrement_epsilon()\n",
    "    \n",
    "    def save(self):\n",
    "        self.qnn_online.save()\n",
    "    \n",
    "    def load(self):\n",
    "        self.qnn_online.load()\n",
    "        \n",
    "    def replace_target_network(self):\n",
    "        self.qnn_target.load_state_dict(self.qnn_online.state_dict())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environments config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVS = [\n",
    "    {\n",
    "        'NAME': 'CartPole-v1',\n",
    "        'UNWRAPPED': False,\n",
    "        'SOLVED_REWARD': 475.0,\n",
    "        'NET_TYPE': 'DQN',\n",
    "        'RESULTS': []\n",
    "    },\n",
    "    {\n",
    "        'NAME': 'CartPole-v1',\n",
    "        'UNWRAPPED': False,\n",
    "        'SOLVED_REWARD': 475.0,\n",
    "        'NET_TYPE': 'DDQN',\n",
    "        'RESULTS': []\n",
    "    },\n",
    "    {\n",
    "        'NAME': 'MountainCar-v0',\n",
    "        'UNWRAPPED': True,\n",
    "        'SOLVED_REWARD': -110.0,\n",
    "        'NET_TYPE': 'DQN',\n",
    "        'RESULTS': []\n",
    "    },\n",
    "    {\n",
    "        'NAME': 'MountainCar-v0',\n",
    "        'UNWRAPPED': True,\n",
    "        'SOLVED_REWARD': -110.0,\n",
    "        'NET_TYPE': 'DDQN',\n",
    "        'RESULTS': []\n",
    "    },\n",
    "    {\n",
    "        'NAME': 'LunarLander-v2',\n",
    "        'UNWRAPPED': False,\n",
    "        'SOLVED_REWARD': 200,\n",
    "        'NET_TYPE': 'DQN',\n",
    "        'RESULTS': []\n",
    "    },\n",
    "    {\n",
    "        'NAME': 'LunarLander-v2',\n",
    "        'UNWRAPPED': False,\n",
    "        'SOLVED_REWARD': 200,\n",
    "        'NET_TYPE': 'DDQN',\n",
    "        'RESULTS': []\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These parameters are used for all the environments defined above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZES = (512, 128)\n",
    "EPISODES = 2000\n",
    "MIN_EPSILON=0.3\n",
    "MAX_EPSILON=1\n",
    "EPSILON_DECAY=0.0001\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.00025\n",
    "MEMORY_SIZE = 1000000\n",
    "BATCH_SIZE = 32\n",
    "REPLACE_TARGET = 1000\n",
    "SAVE_DIR = './progress'\n",
    "RESULTS_DIR = './results'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TRAINING ENVIRONMENT CartPole-v1---\n",
      "\n",
      "--- MODEL TYPE --- DQN---\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "880df72a822d42c4855c17b3953a1009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_mean_100: 176.72, current_mean: 56.85,     best_eval_score: 475.03, current_eval_mean 475.033\n",
      "\n",
      "---GOAL REACHED AFTER 1558 EPISODES---\n",
      "\n",
      "\n",
      "\n",
      "--- TRAINING ENVIRONMENT CartPole-v1---\n",
      "\n",
      "--- MODEL TYPE --- DDQN---\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51daeb7a9e8f42758564f714b35edfeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_mean_100: 153.44, current_mean: 105.49,     best_eval_score: 476.76, current_eval_mean 476.76\n",
      "\n",
      "---GOAL REACHED AFTER 1547 EPISODES---\n",
      "\n",
      "\n",
      "\n",
      "--- TRAINING ENVIRONMENT MountainCar-v0---\n",
      "\n",
      "--- MODEL TYPE --- DQN---\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d9ec7256e764b098b7915b2a112a389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_mean_100: -168.79, current_mean: -171.86,     best_eval_score: -109.97, current_eval_mean -109.97\n",
      "\n",
      "---GOAL REACHED AFTER 1025 EPISODES---\n",
      "\n",
      "\n",
      "\n",
      "--- TRAINING ENVIRONMENT MountainCar-v0---\n",
      "\n",
      "--- MODEL TYPE --- DDQN---\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76eae4099224474696cc8561690b1ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_mean_100: -171.19, current_mean: -182.26,     best_eval_score: -109.99, current_eval_mean -109.99\n",
      "\n",
      "---GOAL REACHED AFTER 1041 EPISODES---\n",
      "\n",
      "\n",
      "\n",
      "--- TRAINING ENVIRONMENT LunarLander-v2---\n",
      "\n",
      "--- MODEL TYPE --- DQN---\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3297be90f32e4cf599f4ed0225b5e3a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_mean_100: 96.66262768494447, current_mean: 92.91748777481132,     best_eval_score: 201.48904051003353, current_eval_mean 201.48904051003353468312\n",
      "\n",
      "---GOAL REACHED AFTER 312 EPISODES---\n",
      "\n",
      "\n",
      "\n",
      "--- TRAINING ENVIRONMENT LunarLander-v2---\n",
      "\n",
      "--- MODEL TYPE --- DDQN---\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98427e462ff84b869c264b027421e197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_mean_100: 91.5492443116732, current_mean: 88.13932703068465,     best_eval_score: 200.76046875672407, current_eval_mean 200.760468756724074186643\n",
      "\n",
      "---GOAL REACHED AFTER 251 EPISODES---\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Main loop\n",
    "for env in ENVS:\n",
    "    \n",
    "    NAME = env['NAME']\n",
    "    UNWRAPPED = env['UNWRAPPED']\n",
    "    SOLVED_REWARD = env['SOLVED_REWARD']\n",
    "    NET_TYPE = env['NET_TYPE']\n",
    "    \n",
    "    print(f'--- TRAINING ENVIRONMENT {NAME}---\\n')\n",
    "    print(f'--- MODEL TYPE --- {NET_TYPE}---\\n')\n",
    "    \n",
    "    if UNWRAPPED:  \n",
    "        ENV = gym.make(NAME).unwrapped\n",
    "    else: \n",
    "        ENV = gym.make(NAME)\n",
    "    ACTION_SIZE = ENV.action_space.n\n",
    "    INPUT_SIZE = ENV.observation_space.shape\n",
    "\n",
    "    agent = DQN(NET_TYPE, INPUT_SIZE, HIDDEN_SIZES, ACTION_SIZE, \n",
    "                min_epsilon=MIN_EPSILON, max_epsilon=MAX_EPSILON, epsilon_decay=EPSILON_DECAY,\n",
    "                gamma=GAMMA, lr=LEARNING_RATE, \n",
    "                mem_size=MEMORY_SIZE, batch_size=BATCH_SIZE, \n",
    "                save_dir=SAVE_DIR, name=NAME)\n",
    "\n",
    "\n",
    "    reward_tracking = []\n",
    "    eval_tracking = env['RESULTS']\n",
    "    best_mean = -1000\n",
    "    best_eval_score = -1000\n",
    "    reward_mean = -1000\n",
    "    eval_mean = -1000\n",
    "    counter = 0\n",
    "    for episode in tqdm(range(EPISODES)):\n",
    "        obs, done = ENV.reset(), False\n",
    "        reward_sum = 0\n",
    "        while not done:\n",
    "            action = agent.epsilon_greedy(obs)\n",
    "            next_obs, reward, done, info = ENV.step(action)\n",
    "            is_truncated = 'TimeLimit.truncated' in info and info['TimeLimit.truncated']\n",
    "            terminal = done and (not is_truncated)\n",
    "            reward_sum += reward\n",
    "            agent.add_memory(obs, action, reward, next_obs, terminal)\n",
    "            obs = next_obs\n",
    "            agent.learn()\n",
    "            if(counter + 1) % REPLACE_TARGET == 0:\n",
    "                agent.replace_target_network()\n",
    "                counter = 0\n",
    "            counter += 1\n",
    "        reward_tracking.append(reward_sum)\n",
    "\n",
    "\n",
    "        #testing agent\n",
    "        obs, done = ENV.reset(), False\n",
    "        eval_score = 0\n",
    "        while not done:\n",
    "            action = agent.greedy(obs)\n",
    "            next_obs, reward, done, _ = ENV.step(action)\n",
    "            eval_score += reward\n",
    "            obs = next_obs\n",
    "\n",
    "            # this is necessary for Mountain Car, as in earlier episodes the policy might be stuck during evaluation\n",
    "            # this is the case because the evaluation policy is purely greedy and the environment is unwrapped\n",
    "            if eval_score <= -200:\n",
    "                done = True\n",
    "        eval_tracking.append(eval_score)\n",
    "\n",
    "        # OUTPUT INFO\n",
    "        if (episode > 100):\n",
    "            reward_mean = np.array(reward_tracking[-100:]).mean()\n",
    "            eval_mean = np.array(eval_tracking[-100:]).mean()\n",
    "            if reward_mean > best_mean:\n",
    "                best_mean = reward_mean\n",
    "            if eval_mean > best_eval_score:\n",
    "                best_eval_score = eval_mean\n",
    "                agent.save()\n",
    "\n",
    "            print(f'best_mean_100: {best_mean}, current_mean: {reward_mean}, \\\n",
    "    best_eval_score: {best_eval_score}, current_eval_mean {eval_mean}', end='\\r')\n",
    "\n",
    "            if eval_mean >= SOLVED_REWARD:\n",
    "                print('\\n', flush=True)\n",
    "                print(f'---GOAL REACHED AFTER {episode} EPISODES---')\n",
    "                print('\\n')\n",
    "                break\n",
    "                \n",
    "    # After model is finished save the list for \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(800, 600))\n",
    "for idx, env in enumerate(ENVS):\n",
    "    NAME = env['NAME']\n",
    "    NET_TYPE = env['NET_TYPE']\n",
    "    RESULTS = env['RESULTS']\n",
    "    \n",
    "    fig.add_axes([idx%2, idx//2, 1, 1]).set_title(NAME+'_'+NET_TYPE)\n",
    "    plt.plot(RESULTS)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play a game with the best agent and save the results as gif\n",
    "for env in ENVS:\n",
    "    frames = []\n",
    "    \n",
    "    NAME = env['NAME']\n",
    "    NET_TYPE = env['NET_TYPE']\n",
    "    \n",
    "    ENV = gym.make(NAME)\n",
    "    ACTION_SIZE = ENV.action_space.n\n",
    "    INPUT_SIZE = ENV.observation_space.shape\n",
    "    \n",
    "    agent = DQN(NET_TYPE, INPUT_SIZE, HIDDEN_SIZES, ACTION_SIZE, \n",
    "                min_epsilon=MIN_EPSILON, max_epsilon=MAX_EPSILON, epsilon_decay=EPSILON_DECAY,\n",
    "                gamma=GAMMA, lr=LEARNING_RATE, \n",
    "                mem_size=MEMORY_SIZE, batch_size=BATCH_SIZE, save_dir=SAVE_DIR, name=NAME)\n",
    "    agent.load()\n",
    "    ENV = gym.make(NAME)\n",
    "    ENV = wrappers.Monitor(ENV, \"./gym-results\", force=True)\n",
    "    obs, done = ENV.reset(), False\n",
    "    frames.append(Image.fromarray(ENV.render(mode='rgb_array')))\n",
    "    eval_score = 0\n",
    "    while not done:\n",
    "        action = agent.greedy(obs)\n",
    "        next_obs, reward, done, _ = ENV.step(action)\n",
    "        eval_score += reward\n",
    "        obs = next_obs\n",
    "        frames.append(Image.fromarray(ENV.render(mode='rgb_array')))\n",
    "    ENV.close()\n",
    "    \n",
    "    path = os.path.join(RESULTS_DIR, NAME+NET_TYPE+'.gif')\n",
    "    with open(path, 'wb') as f:\n",
    "        im = Image.new('RGB', frames[0].size)\n",
    "        im.save(f, save_all=True, append_images=frames, loop=0, duration=25) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SegmentLocal](results/CartPole-v1DQN.gif \"segment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "display.HTML('<img src=\"{}\">'.format('./results/MountainCar-v0.gif'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "display.HTML('<img src=\"{}\">'.format('./results/LunarLander-v2.gif'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
