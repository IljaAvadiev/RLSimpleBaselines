{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Critic Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributions as distributions\n",
    "import torch.multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_dims, hidden_dims, output_dims, lr):\n",
    "        super(Actor, self).__init__()\n",
    "        self.input_layer = nn.Linear(input_dims, hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList(\n",
    "            [nn.Linear(hidden_dims[i], hidden_dims[i+1]) for i in range(len(hidden_dims) - 1)]\n",
    "        )\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], output_dims)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "        x = F.relu(self.input_layer(x))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = F.relu(hidden_layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "    def step(self, state):\n",
    "        logits = self.forward(state)\n",
    "        distribution = distributions.Categorical(logits=logits)\n",
    "        actions = distribution.sample()\n",
    "        log_probs = distribution.log_prob(actions)\n",
    "        entropies = distribution.entropy()\n",
    "        return actions, log_probs, entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dims, hidden_dims, lr):\n",
    "        super(Critic, self).__init__()\n",
    "        self.input_layer = nn.Linear(input_dims, hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList(\n",
    "            [nn.Linear(hidden_dims[i], hidden_dims[i+1]) for i in range(len(hidden_dims) - 1)]\n",
    "        )\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], 1)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "        x = F.relu(self.input_layer(x))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = F.relu(hidden_layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "    def calculate_value(self, state):\n",
    "        with torch.no_grad():\n",
    "            x = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "            x = F.relu(self.input_layer(x))\n",
    "            for hidden_layer in self.hidden_layers:\n",
    "                x = F.relu(hidden_layer(x))\n",
    "            x = self.output_layer(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A combined actor-critic network\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, input_dims, hidden_dims, output_dims, lr):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.input_layer = nn.Linear(input_dims, hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList(\n",
    "            [nn.Linear(hidden_dims[i], hidden_dims[i+1]) for i in range(len(hidden_dims) - 1)]\n",
    "        )\n",
    "        self.actor_output_layer = nn.Linear(hidden_dims[-1], output_dims)\n",
    "        self.critic_output_layer = nn.Linear(hidden_dims[-1], 1)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.to(device)\n",
    "    \n",
    "    def forward(self, state):    \n",
    "        x = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "        x = F.relu(self.input_layer(x))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = F.relu(hidden_layer(x))\n",
    "        actor_output = self.actor_output_layer(x)\n",
    "        critic_output = self.critic_output_layer(x)\n",
    "        return actor_output, critic_output\n",
    "    \n",
    "    def step(self, state):\n",
    "        logits, value = self.forward(state)\n",
    "        distribution = distributions.Categorical(logits=logits)\n",
    "        action = distribution.sample()\n",
    "        log_prob = distribution.log_prob(action)\n",
    "        entropy = distribution.entropy()\n",
    "        return action.cpu().item(), log_prob, entropy, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChildEnv(gym.Env):\n",
    "    def __init__(self, env_name, seed, conn):\n",
    "        self.conn = conn\n",
    "        self.env = gym.make(env_name)\n",
    "        self.env.seed(seed)\n",
    "\n",
    "    def listen(self):\n",
    "        while True:\n",
    "            cmd, action = self.conn.recv()\n",
    "            if cmd == 'reset':\n",
    "                self.conn.send(self.reset())\n",
    "                \n",
    "            if cmd == 'step':\n",
    "                self.conn.send((self.step(action)))\n",
    "                \n",
    "            if cmd == 'close':\n",
    "                self.close()\n",
    "                break\n",
    "\n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "    \n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "        del self.env\n",
    "        self.conn.close()\n",
    "        del self.conn\n",
    "        \n",
    "        \n",
    "class ParentEnv(gym.Env):\n",
    "    def __init__(self, env_name, n_processes):\n",
    "        self.n_processes = n_processes\n",
    "        \n",
    "        self.connections = [mp.Pipe() for process in range(n_processes)]\n",
    "        self.children = [\n",
    "            mp.Process(target=self.control_child, args=(env_name, process, self.connections[process][1])) \\\n",
    "                for process in range(n_processes)]\n",
    "        \n",
    "        for child in self.children:\n",
    "            child.start()\n",
    "            \n",
    "    def control_child(self, env_name, seed, conn):\n",
    "        child = ChildEnv(env_name, seed, conn)\n",
    "        child.listen()\n",
    "    \n",
    "    def step(self, actions):\n",
    "        results = []\n",
    "        for seed, connection in enumerate(self.connections):\n",
    "            connection[0].send(('step', actions[seed]))\n",
    "            \n",
    "        for connection in self.connections:\n",
    "            obs, reward, done, info = connection[0].recv()\n",
    "            if done == True:\n",
    "                is_truncated = 'TimeLimit.truncated' in info and info['TimeLimit.truncated']\n",
    "                connection[0].send(('reset', ''))\n",
    "                obc = connection[0].recv()\n",
    "                if is_truncated:\n",
    "                    done = False\n",
    "            results.append([obs, reward, done, info])\n",
    "            \n",
    "        return [np.vstack(part) for part in np.array(results).T]\n",
    "    \n",
    "    def close(self):\n",
    "        for connection in self.connections:\n",
    "            connection[0].send(('close', ''))\n",
    "    \n",
    "    def reset(self):\n",
    "        observations = []\n",
    "        for connection in self.connections:\n",
    "            connection[0].send(('reset', ''))\n",
    "        \n",
    "        for connection in self.connections:\n",
    "            observation = connection[0].recv()\n",
    "            observations.append(observation)\n",
    "            \n",
    "        return np.array(observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, \n",
    "                 state_dims, \n",
    "                 actor_hidden_dims, \n",
    "                 critic_hidden_dims, \n",
    "                 action_dims, \n",
    "                 gamma, \n",
    "                 beta,\n",
    "                 tau,\n",
    "                 actor_lr,\n",
    "                 critic_lr):\n",
    "        self.actor = Actor(\n",
    "            input_dims=state_dims, \n",
    "            hidden_dims=actor_hidden_dims, \n",
    "            output_dims=action_dims, \n",
    "            lr=actor_lr)\n",
    "        self.critic = Critic(\n",
    "            input_dims=state_dims,\n",
    "            hidden_dims=critic_hidden_dims,\n",
    "            lr=critic_lr\n",
    "        )\n",
    "        self.gamma = gamma\n",
    "        self.beta = beta\n",
    "        self.tau = tau\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.log_probs = []\n",
    "        self.entropies = []\n",
    "    \n",
    "    def learn(self):\n",
    "        trajectory_len = len(self.rewards)\n",
    "        gammas = np.array([self.gamma**i for i in range(trajectory_len)], dtype=np.float32)\n",
    "        taus = np.array([(self.gamma * self.tau)**i for i in range(trajectory_len)], dtype=np.float32) \n",
    "        \n",
    "        returns = [[np.sum(np.array(self.rewards)[t:, worker] * gammas[:trajectory_len - t]) \n",
    "                   for t in range(trajectory_len)]\n",
    "                   for worker in range(len(self.rewards[0]))]\n",
    "                \n",
    "                \n",
    "        #shape (workers (batches), n-steps)\n",
    "        #print(returns.shape)\n",
    "        returns = torch.tensor(returns, dtype=torch.float32).to(device)\n",
    "        log_probs = torch.vstack(self.log_probs).squeeze(1).T\n",
    "        values = torch.vstack(self.values).squeeze(1).T\n",
    "        entropies = torch.vstack(self.entropies).squeeze(1).T\n",
    "        \n",
    "        advantages = returns + self.gamma * values[:,1:] - values[:, :-1]\n",
    "        gaes = [[torch.sum(advantages[t:, worker] * taus[:trajectory_len - t - 1]) \n",
    "                for t in range(trajectory_len-1)]\n",
    "                for worker in range(len(self.rewards[0]))]\n",
    "        \n",
    "        print(gaes.shape)\n",
    "                \n",
    "#         actor_loss = -(advantages.detach() * log_probs + self.beta * entropies).mean()\n",
    "        \n",
    "#         self.actor.optimizer.zero_grad()\n",
    "#         actor_loss.backward()\n",
    "#         self.actor.optimizer.step()\n",
    "        \n",
    "#         critic_advantages = returns[:-1] - values\n",
    "#         critic_loss = critic_advantages.pow(2).mul(0.5).mean()\n",
    "#         self.critic.optimizer.zero_grad()\n",
    "#         critic_loss.backward()\n",
    "#         self.critic.optimizer.step()\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def add_rewards(self, rewards):\n",
    "        self.rewards.append(rewards)\n",
    "    \n",
    "    def act(self, states):\n",
    "        actions, log_probs, entropies = self.actor.step(states)\n",
    "        self.log_probs.append(log_probs)\n",
    "        self.entropies.append(entropies)\n",
    "        self.values.append(self.critic(states).squeeze(1))\n",
    "        return actions.cpu().numpy()\n",
    "    \n",
    "    \n",
    "    def values_from_state(self, states):\n",
    "        self.values.append(self.critic.calculate_value(states).squeeze(1))\n",
    "    \n",
    "    \n",
    "    def choose_action(self, states):\n",
    "        with torch.no_grad():\n",
    "            actions, _, _ = self.actor.step(states)\n",
    "            return actions.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make(env_name)\n",
    "# state_dims = env.observation_space.shape[0]\n",
    "# action_dims = env.action_space.n\n",
    "\n",
    "# env = ParentEnv(env_name, 8)\n",
    "\n",
    "# agent = Agent(state_dims, (2, 2), (2,2), action_dims, 1, 0.001, 0.001, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.reset().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actions = agent.act(obs)\n",
    "# obs, rewards, _, _ = env.step(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(20):\n",
    "#     actions = agent.act(obs)\n",
    "#     obs, rewards, _, _ = env.step(actions)\n",
    "#     agent.add_rewards(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(agent.rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "env_name = 'CartPole-v1'\n",
    "steps = 100000\n",
    "actor_alpha = 0.0005\n",
    "critic_alpha = 0.0007\n",
    "beta = 0.001\n",
    "gamma = 0.99\n",
    "tau = 0.5\n",
    "actor_hidden_dims = (128, 64)\n",
    "critic_hidden_dims = (256, 128)\n",
    "win_condition = 475\n",
    "n_steps = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-a36c8de3725c>:64: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return [np.vstack(part) for part in np.array(results).T]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-f1a3c8a2936d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mn_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues_from_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_obss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-8e13418a0b30>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0madvantages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madvantages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         gaes = [[torch.sum(advantages[t:, worker] * taus[:trajectory_len - t - 1]) \n\u001b[0m\u001b[1;32m     53\u001b[0m                 for t in range(trajectory_len-1)]\n\u001b[1;32m     54\u001b[0m                 for worker in range(len(self.rewards[0]))]\n",
      "\u001b[0;32m<ipython-input-40-8e13418a0b30>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0madvantages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madvantages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         gaes = [[torch.sum(advantages[t:, worker] * taus[:trajectory_len - t - 1]) \n\u001b[0m\u001b[1;32m     53\u001b[0m                 for t in range(trajectory_len-1)]\n\u001b[1;32m     54\u001b[0m                 for worker in range(len(self.rewards[0]))]\n",
      "\u001b[0;32m<ipython-input-40-8e13418a0b30>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0madvantages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madvantages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         gaes = [[torch.sum(advantages[t:, worker] * taus[:trajectory_len - t - 1]) \n\u001b[0m\u001b[1;32m     53\u001b[0m                 for t in range(trajectory_len-1)]\n\u001b[1;32m     54\u001b[0m                 for worker in range(len(self.rewards[0]))]\n",
      "\u001b[0;32m~/Documents/01_programming/04_rl/github/ReinforcementLearning/venv/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    628\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelevant_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "# main loop\n",
    "eval_env = gym.make(env_name)\n",
    "state_dims = eval_env.observation_space.shape[0]\n",
    "action_dims = eval_env.action_space.n\n",
    "\n",
    "env = ParentEnv(env_name, 8)\n",
    "agent = Agent(state_dims, \n",
    "                  actor_hidden_dims, \n",
    "                  critic_hidden_dims, \n",
    "                  action_dims, \n",
    "                  gamma, \n",
    "                  beta,\n",
    "                  tau,\n",
    "                  actor_alpha,\n",
    "                  critic_alpha)\n",
    "track_rewards_mean = []\n",
    "track_rewards = []\n",
    "best_mean = 0\n",
    "episode = 0\n",
    "obss = env.reset()\n",
    "for step in range(steps):\n",
    "\n",
    "    action = agent.act(obss)\n",
    "    new_obss, rewards, dones, _ = env.step(action)\n",
    "    agent.add_rewards(rewards)\n",
    "    obss = new_obss\n",
    "        \n",
    "    if (step + 1) % n_steps == 0 or dones.sum():\n",
    "        agent.values_from_state(new_obss)\n",
    "        agent.learn()\n",
    "    \n",
    "        if step > 1000:\n",
    "            eval_obs, eval_done = eval_env.reset(), False\n",
    "            reward_sum = 0\n",
    "            while not eval_done:\n",
    "                eval_action = agent.choose_action(eval_obs)\n",
    "                new_eval_obs, eval_reward, eval_done, _ = eval_env.step(eval_action)\n",
    "                eval_obs = new_eval_obs\n",
    "                reward_sum += eval_reward\n",
    "            track_rewards.append(reward_sum)\n",
    "            episode += 1\n",
    "            if episode > 100:\n",
    "                mean = np.mean(track_rewards[-100:])\n",
    "                if mean > best_mean:\n",
    "                    best_mean = mean\n",
    "                track_rewards_mean.append(mean)\n",
    "                print(f'Step: {step}, Episode: {episode}, Current Rewards: {reward_sum}, Reward Mean: {mean}, Best Mean: {best_mean}', end='\\r')\n",
    "                if mean > win_condition:\n",
    "                    env.close()\n",
    "                    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reward_mean)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
