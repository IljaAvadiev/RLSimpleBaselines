{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from time import sleep\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# helper functions from a local file\n",
    "from utils.helper import epsilon_greedy\n",
    "from utils.helper import generate_episode\n",
    "from utils.helper import linear_decay\n",
    "from utils.helper import print_policy, print_state_value_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moncte Carlo First Visit Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_fv_control(env, eps_start, eps_end, alpha_start, alpha_end, decay_duration, gamma=0.99, num_episodes=10000):\n",
    "    '''\n",
    "    Monte Carlo First Visit control function. \n",
    "    The function calculates an estimation of the optimal Q-function, V-function and policy\n",
    "\n",
    "    Args: \n",
    "        env:            OpenAI gym environment to interract with\n",
    "        eps_start:      The starting value for epsilon using epsilon greedy\n",
    "        eps_end:        The final value for epsilon using epsilon greedy\n",
    "        alpha_start:    The starting value for the learning rate\n",
    "        aplpha_end:     The final value for the learning rate\n",
    "        decay_duration: The number of episodes epsilon and alpha are decayed\n",
    "        gamma:          Discounting rate for the bellman equation\n",
    "        num_episodes:   The number of episodes to interract with the environment\n",
    "\n",
    "    Returns:\n",
    "        Q:              The estimate of the optimal action value function\n",
    "        V:              The estimate of the optimal state value function\n",
    "        pi:             The estimate of an optimal strategy\n",
    "    '''\n",
    "    \n",
    "    nS = env.observation_space.n\n",
    "    nA = env.action_space.n\n",
    "    epsilons = linear_decay(eps_start, eps_end, decay_duration, num_episodes)\n",
    "    alphas = linear_decay(alpha_start, alpha_end, decay_duration, num_episodes)\n",
    "    \n",
    "    Q = np.zeros((nS, nA), dtype=np.float32)\n",
    "    \n",
    "    # generates a policy based on the current Q and epsilon \n",
    "    def create_pi(Q, epsilon):\n",
    "        def pi(state):\n",
    "            return epsilon_greedy(Q, state, epsilon)\n",
    "        return pi\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        visited = np.zeros_like(Q, dtype=np.bool)\n",
    "        \n",
    "        \n",
    "        pi = create_pi(Q, epsilons[episode])\n",
    "        generated_episode = generate_episode(env, pi)\n",
    "        \n",
    "        for t, (obs, action, _, _, _) in enumerate(generated_episode):\n",
    "            if visited[obs][action] == True:\n",
    "                continue\n",
    "            visited[obs][action] = True\n",
    "            \n",
    "            remaining_rewards = np.array(generated_episode, dtype=np.object)[t:, 2]\n",
    "            remaining_steps = len(remaining_rewards)\n",
    "            \n",
    "            # calculate discounting\n",
    "            bases = [gamma for _ in range(remaining_steps)]\n",
    "            exponents = [x for x in range(remaining_steps)]\n",
    "            discounts = np.power(bases, exponents)\n",
    "            \n",
    "            #calculate discounted return\n",
    "            mc_target = np.sum(remaining_rewards * discounts)\n",
    "            \n",
    "            \n",
    "            Q[obs][action] = Q[obs][action] + alphas[episode] * (mc_target - Q[obs][action])\n",
    "            \n",
    "    V = np.max(Q, axis=1)\n",
    "    pi = lambda s: {state: action for state, action in enumerate(np.argmax(Q, axis=1))}[s]\n",
    "    \n",
    "    return Q, V, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA (TD On Policy learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env, eps_start, eps_end, alpha_start, alpha_end, decay_duration, gamma=0.99, num_episodes=10000):\n",
    "    '''\n",
    "    SARSA control function (On-Policy TD). \n",
    "    The function calculates an estimation of the optimal Q-function, V-function and policy\n",
    "\n",
    "    Args: \n",
    "        env:            OpenAI gym environment to interract with\n",
    "        eps_start:      The starting value for epsilon using epsilon greedy\n",
    "        eps_end:        The final value for epsilon using epsilon greedy\n",
    "        alpha_start:    The starting value for the learning rate\n",
    "        aplpha_end:     The final value for the learning rate\n",
    "        decay_duration: The number of episodes epsilon and alpha are decayed\n",
    "        gamma:          Discounting rate for the bellman equation\n",
    "        num_episodes:   The number of episodes to interract with the environment\n",
    "\n",
    "    Returns:\n",
    "        Q:              The estimate of the optimal action value function\n",
    "        V:              The estimate of the optimal state value function\n",
    "        pi:             The estimate of an optimal strategy\n",
    "    '''\n",
    "    nS = env.observation_space.n\n",
    "    nA = env.action_space.n\n",
    "    epsilons = linear_decay(eps_start, eps_end, decay_duration, num_episodes)\n",
    "    alphas = linear_decay(alpha_start, alpha_end, decay_duration, num_episodes)\n",
    "    \n",
    "    Q = np.zeros((nS, nA), dtype=np.float32)\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        obs, done = env.reset(), False\n",
    "        action = epsilon_greedy(Q, obs, epsilons[episode])\n",
    "        while not done:\n",
    "            next_obs, reward, done, _ = env.step(action)\n",
    "            next_action = epsilon_greedy(Q, next_obs, epsilons[episode])\n",
    "            sarsa_target = reward + gamma * Q[next_obs][next_action] * (not done)\n",
    "            Q[obs][action] = Q[obs][action] + alphas[episode] * (sarsa_target - Q[obs][action])\n",
    "            obs, action = next_obs, next_action\n",
    "    \n",
    "    V = np.max(Q, axis=1)\n",
    "    pi = lambda s: {state: action for state, action in enumerate(np.argmax(Q, axis=1))}[s]\n",
    "    \n",
    "    return Q, V, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning (TD Off Policy learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, eps_start, eps_end, alpha_start, alpha_end, decay_duration, gamma=0.99, num_episodes=10000):\n",
    "    '''\n",
    "    Q-Learning control function (Off-Policy TD). \n",
    "    The function calculates an estimation of the optimal Q-function, V-function and policy\n",
    "\n",
    "    Args: \n",
    "        env:            OpenAI gym environment to interract with\n",
    "        eps_start:      The starting value for epsilon using epsilon greedy\n",
    "        eps_end:        The final value for epsilon using epsilon greedy\n",
    "        alpha_start:    The starting value for the learning rate\n",
    "        aplpha_end:     The final value for the learning rate\n",
    "        decay_duration: The number of episodes epsilon and alpha are decayed\n",
    "        gamma:          Discounting rate for the bellman equation\n",
    "        num_episodes:   The number of episodes to interract with the environment\n",
    "\n",
    "    Returns:\n",
    "        Q:              The estimate of the optimal action value function\n",
    "        V:              The estimate of the optimal state value function\n",
    "        pi:             The estimate of an optimal strategy\n",
    "    '''\n",
    "    nS = env.observation_space.n\n",
    "    nA = env.action_space.n\n",
    "    epsilons = linear_decay(eps_start, eps_end, decay_duration, num_episodes)\n",
    "    alphas = linear_decay(alpha_start, alpha_end, decay_duration, num_episodes)\n",
    "    \n",
    "    Q = np.zeros((nS, nA), dtype=np.float32)\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        obs, done = env.reset(), False\n",
    "        while not done:\n",
    "            action = epsilon_greedy(Q, obs, epsilons[episode])\n",
    "            next_obs, reward, done, _ = env.step(action)\n",
    "            next_action = np.argmax(Q[next_obs])\n",
    "            sarsa_target = reward + gamma * Q[next_obs][next_action] * (not done)\n",
    "            Q[obs][action] = Q[obs][action] + alphas[episode] * (sarsa_target - Q[obs][action])\n",
    "            obs = next_obs\n",
    "            \n",
    "    V = np.max(Q, axis=1)\n",
    "    pi = lambda s: {state: action for state, action in enumerate(np.argmax(Q, axis=1))}[s]\n",
    "    \n",
    "    return Q, V, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double Q-Learning (reducing maximization bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_q_learning(env, eps_start, eps_end, alpha_start, alpha_end, decay_duration, gamma=0.99, num_episodes=10000):\n",
    "    '''\n",
    "    Double Q-Learning control function (Off-Policy TD). \n",
    "    The function calculates an estimation of the optimal Q-function, V-function and policy\n",
    "\n",
    "    Args: \n",
    "        env:            OpenAI gym environment to interract with\n",
    "        eps_start:      The starting value for epsilon using epsilon greedy\n",
    "        eps_end:        The final value for epsilon using epsilon greedy\n",
    "        alpha_start:    The starting value for the learning rate\n",
    "        aplpha_end:     The final value for the learning rate\n",
    "        decay_duration: The number of episodes epsilon and alpha are decayed\n",
    "        gamma:          Discounting rate for the bellman equation\n",
    "        num_episodes:   The number of episodes to interract with the environment\n",
    "\n",
    "    Returns:\n",
    "        Q:              The estimate of the optimal action value function\n",
    "        V:              The estimate of the optimal state value function\n",
    "        pi:             The estimate of an optimal strategy\n",
    "    '''\n",
    "    nS = env.observation_space.n\n",
    "    nA = env.action_space.n\n",
    "    epsilons = linear_decay(eps_start, eps_end, decay_duration, num_episodes)\n",
    "    alphas = linear_decay(alpha_start, alpha_end, decay_duration, num_episodes)\n",
    " \n",
    "    Q_1 = np.zeros((nS, nA), dtype=np.float32)\n",
    "    Q_2 = np.zeros((nS, nA), dtype=np.float32)\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        obs, done = env.reset(), False\n",
    "        while not done:\n",
    "            action = epsilon_greedy((Q_1 +  Q_2) / 2, obs, epsilons[episode])\n",
    "            next_obs, reward, done, _ = env.step(action)\n",
    "            if np.random.random() < 0.5:\n",
    "                next_action = np.argmax(Q_1[next_obs])\n",
    "                sarsa_target = reward + gamma * Q_2[next_obs][next_action] * (not done)\n",
    "                Q_1[obs][action] = Q_1[obs][action] + alphas[episode] * (sarsa_target - Q_1[obs][action])\n",
    "                obs = next_obs\n",
    "            else:\n",
    "                next_action = np.argmax(Q_2[next_obs])\n",
    "                sarsa_target = reward + gamma * Q_1[next_obs][next_action] * (not done)\n",
    "                Q_2[obs][action] = Q_2[obs][action] + alphas[episode] * (sarsa_target - Q_2[obs][action])\n",
    "                obs = next_obs\n",
    "                \n",
    "    Q = (Q_1 + Q_2) / 2\n",
    "    V = np.max(Q, axis=1)\n",
    "    pi = lambda s: {state: action for state, action in enumerate(np.argmax(Q, axis=1))}[s]\n",
    "    \n",
    "    return Q, V, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frozen Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma=0.99, delta=1e-10):\n",
    "    '''\n",
    "    Finds an optimal policy\n",
    "    \n",
    "    Args: \n",
    "        env:     openai gym environment\n",
    "        gamma:   discount factor\n",
    "        delta:   threshhold value to interrupt the policy evaluation\n",
    "    \n",
    "    Returns: \n",
    "        Optimal policy and value function\n",
    "    ''' \n",
    "    nS = env.observation_space.n\n",
    "    nA = env.action_space.n\n",
    "    V = np.zeros(nS, dtype=np.float64)\n",
    "    P = env.env.P\n",
    "    \n",
    "    while True:\n",
    "        V_old = V.copy()\n",
    "        Q = np.zeros(shape=(nS, nA), dtype=np.float64)\n",
    "        for state in range(nS):\n",
    "            for action in range(nA):\n",
    "                for pr, next_state, reward, done in P[state][action]:\n",
    "                    Q[state][action] += pr * (reward + gamma * V[next_state] * (not done))\n",
    "        V = np.max(Q, axis=1)\n",
    "        max_diff = np.max(np.abs(V_old - V))\n",
    "        if max_diff < delta:\n",
    "            break\n",
    "\n",
    "    strategy = {s: a for s, a in enumerate(np.argmax(Q, axis=1))}\n",
    "    def pi(s):\n",
    "        return strategy[s]\n",
    "    \n",
    "    return pi, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi, V = value_iteration(env, gamma=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1mFrozenLake optimal policy\u001b[0m\n",
      "\n",
      "\n",
      "         ←          ↑          ↑          ↑\n",
      "         ←          ■          ←          ■\n",
      "         ↑          ↓          ←          ■\n",
      "         ■          →          ↓          ■\n"
     ]
    }
   ],
   "source": [
    "print_policy(pi, 16, 4, name='FrozenLake optimal policy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1mFrozenLake optimal value function\u001b[0m\n",
      "\n",
      "\n",
      "0.54203 0.49880 0.47070 0.45685\n",
      "0.55845 0.00000 0.35835 0.00000\n",
      "0.59180 0.64308 0.61521 0.00000\n",
      "0.00000 0.74172 0.86284 0.00000\n"
     ]
    }
   ],
   "source": [
    "print_state_value_func(V, 4, name='FrozenLake optimal value function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Frozen Lake control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "num_episodes = 10000\n",
    "decay_duration = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, V, pi = mc_fv_control(env, eps_start=1.0, eps_end=0.01, alpha_start=0.05, alpha_end=0.01, \\\n",
    "                     decay_duration=decay_duration, num_episodes=num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1mFrozenLake mc optimal policy estimation\u001b[0m\n",
      "\n",
      "\n",
      "         ↓          ↑          ↓          ↑\n",
      "         ←          ■          ←          ■\n",
      "         ↑          ↓          ←          ■\n",
      "         ■          →          ↓          ■\n"
     ]
    }
   ],
   "source": [
    "print_policy(pi, 16, 4, name='FrozenLake mc optimal policy estimation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1mFrozenLake mc optimal value function estimation\u001b[0m\n",
      "\n",
      "\n",
      "0.37898 0.32744 0.28837 0.26887\n",
      "0.41985 0.00000 0.27365 0.00000\n",
      "0.49916 0.56090 0.55292 0.00000\n",
      "0.00000 0.67714 0.82830 0.00000\n"
     ]
    }
   ],
   "source": [
    "print_state_value_func(V, 4, name='FrozenLake mc optimal value function estimation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA Frozen Lake control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, V, pi = sarsa(env, eps_start=1.0, eps_end=0.01, alpha_start=0.05, alpha_end=0.01, \\\n",
    "             decay_duration=decay_duration, num_episodes=num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1mFrozenLake mc optimal policy estimation\u001b[0m\n",
      "\n",
      "\n",
      "         ←          ↑          ←          ↑\n",
      "         ←          ■          ←          ■\n",
      "         ↑          ↓          ↓          ■\n",
      "         ■          →          ↓          ■\n"
     ]
    }
   ],
   "source": [
    "print_policy(pi, 16, 4, name='FrozenLake SARSA optimal policy estimation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1mFrozenLake SARSA optimal value function estimation\u001b[0m\n",
      "\n",
      "\n",
      "0.39224 0.24000 0.13246 0.01958\n",
      "0.40921 0.00000 0.13352 0.00000\n",
      "0.44517 0.50703 0.43842 0.00000\n",
      "0.00000 0.64681 0.81653 0.00000\n"
     ]
    }
   ],
   "source": [
    "print_state_value_func(V, 4, name='FrozenLake SARSA optimal value function estimation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning Frozen Lake control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, V, pi = q_learning(env, eps_start=1.0, eps_end=0.01, alpha_start=0.5, alpha_end=0.01, \\\n",
    "                  decay_duration=decay_duration, num_episodes=num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1mFrozenLake Q-Learning optimal policy estimation\u001b[0m\n",
      "\n",
      "\n",
      "         ←          ↑          ↑          ↑\n",
      "         ←          ■          ←          ■\n",
      "         ↑          ↓          ←          ■\n",
      "         ■          →          ↓          ■\n"
     ]
    }
   ],
   "source": [
    "print_policy(pi, 16, 4, name='FrozenLake Q-Learning optimal policy estimation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1mFrozenLake Q-Learning optimal value function estimation\u001b[0m\n",
      "\n",
      "\n",
      "0.49982 0.44215 0.41037 0.39833\n",
      "0.51633 0.00000 0.31295 0.00000\n",
      "0.54115 0.60409 0.60304 0.00000\n",
      "0.00000 0.71231 0.84676 0.00000\n"
     ]
    }
   ],
   "source": [
    "print_state_value_func(V, 4, name='FrozenLake Q-Learning optimal value function estimation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Q-Learning Frozen Lake control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, V, pi = double_q_learning(env, eps_start=1.0, eps_end=0.01, alpha_start=0.5, alpha_end=0.01, \\\n",
    "                         decay_duration=decay_duration, num_episodes=num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1mFrozenLake Double Q-Learning optimal policy estimation\u001b[0m\n",
      "\n",
      "\n",
      "         ←          ↑          ←          ↑\n",
      "         ←          ■          ←          ■\n",
      "         ↑          ↓          ←          ■\n",
      "         ■          →          ↓          ■\n"
     ]
    }
   ],
   "source": [
    "print_policy(pi, 16, 4, name='FrozenLake Double Q-Learning optimal policy estimation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1mFrozenLake Double Q-Learning optimal value function estimation\u001b[0m\n",
      "\n",
      "\n",
      "0.50423 0.43319 0.36527 0.23953\n",
      "0.52129 0.00000 0.30889 0.00000\n",
      "0.55442 0.60340 0.57968 0.00000\n",
      "0.00000 0.72071 0.83441 0.00000\n"
     ]
    }
   ],
   "source": [
    "print_state_value_func(V, 4, name='FrozenLake Double Q-Learning optimal value function estimation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxi environment with Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, V, pi = q_learning(env, eps_start=1.0, eps_end=0.01, alpha_start=0.5, alpha_end=0.01, \\\n",
    "                  decay_duration=decay_duration, num_episodes=num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (Dropoff)\n"
     ]
    }
   ],
   "source": [
    "# Play an episode of taxi\n",
    "obs, done = env.reset(), False\n",
    "env.render()\n",
    "while not done:\n",
    "    sleep(0.5)\n",
    "    clear_output(wait=True)\n",
    "    action = pi(obs)\n",
    "    next_obs, reward, done, _ = env.step(action)\n",
    "    obs = next_obs\n",
    "    env.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
