{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Visit Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(env, pi):\n",
    "    '''\n",
    "    Generates an episode following a policy pi.\n",
    "    An episode is a succession of experiences until the terminal state. \n",
    "    A tuple of state, action, reward, next_state and done constitute an exprience.\n",
    "    \n",
    "    Args: \n",
    "        env: OpenAI gym environment to interract with\n",
    "        pi:  Policy that is used to create an episode\n",
    "    \n",
    "    Returns:\n",
    "        episode\n",
    "    '''\n",
    "    episode = []\n",
    "    obs, done = env.reset(), False\n",
    "    while not done:\n",
    "        action = pi(obs)\n",
    "        new_obs, reward, done, _ = env.step(action)\n",
    "        experience = (obs, action, reward, new_obs, done)\n",
    "        episode.append(experience)\n",
    "        obs = new_obs\n",
    "    \n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_fv_prediction(env, pi, alpha=0.001, gamma=0.99, num_episodes=10000):\n",
    "    '''\n",
    "    Calculates the state value function of a policy pi using first visit monte carlo\n",
    "    \n",
    "    Args:\n",
    "        env:           OpenAI gym environment to interract with\n",
    "        pi:            Policy that is used to select actions\n",
    "        alpha:         Learning rate\n",
    "        gamma:         Discounting rate\n",
    "        num_episodes:  Number of episodes to play\n",
    "    \n",
    "    Returns:\n",
    "        V: Value function\n",
    "    '''\n",
    "    nS = env.observation_space.n\n",
    "    V = np.zeros(nS, dtype=np.float64)\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        generated_episode = generate_episode(env, pi)\n",
    "        visited = np.zeros_like(V, dtype=np.bool)\n",
    "    \n",
    "        for t, (obs, _, _, _, _) in enumerate(generated_episode):\n",
    "            if visited[obs] == True:\n",
    "                continue\n",
    "            visited[obs] = True\n",
    "            \n",
    "            remaining_rewards = np.array(generated_episode, dtype=np.object)[t:, 2]\n",
    "            remaining_steps = len(remaining_rewards)\n",
    "            \n",
    "            # calculate discounting\n",
    "            bases = [gamma for _ in range(remaining_steps)]\n",
    "            exponents = [x for x in range(remaining_steps)]\n",
    "            discounts = np.power(bases, exponents)\n",
    "            \n",
    "            #calculate discounted return\n",
    "            mc_target = np.sum(remaining_rewards * discounts)\n",
    "            \n",
    "            #update value function\n",
    "            V[obs] = V[obs] + alpha * (mc_target - V[obs])\n",
    "            \n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD (Temporal Difference) Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_prediction(env, pi, alpha=0.001, gamma=0.99, num_episodes=10000):\n",
    "    '''\n",
    "    Calculates the state value function of a policy pi using temporal difference\n",
    "    \n",
    "    Args:\n",
    "        env:           OpenAI gym environment to interract with\n",
    "        pi:            Policy that is used to select actions\n",
    "        alpha:         Learning rate\n",
    "        gamma:         Discounting rate\n",
    "        num_episodes:  Number of episodes to play\n",
    "    \n",
    "    Returns:\n",
    "        V: Value function\n",
    "    '''\n",
    "    nS = env.observation_space.n\n",
    "    V = np.zeros(nS, dtype=np.float64)\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        obs, done = env.reset(), False\n",
    "        while not done:\n",
    "            action = pi(obs)\n",
    "            next_obs, reward, done, _ = env.step(action)\n",
    "            V[obs] = V[obs] + alpha * (reward + gamma * V[next_obs] * (not done) - V[obs])\n",
    "            obs = next_obs\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n - step Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD (Lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEFT = 0\n",
    "# DOWN = 1\n",
    "# RIGHT = 2\n",
    "# UP = 3\n",
    "\n",
    "#strategy = {s: 0 for s in range(env.observation_space.n)}    \n",
    "strategy = {0: 2, 1: 2, 2: 1, 3: 0, 4: 1, 5:0, 6: 1, 7: 0, 8: 2, 9: 2, 10: 1, 11: 0, 12:0, 13:2, 14: 2, 15: 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pi(s):\n",
    "    return strategy[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = monte_carlo_fv_prediction(env, pi, num_episodes=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03517821, 0.0290139 , 0.05970268, 0.02996512, 0.04660632,\n",
       "       0.        , 0.11588832, 0.        , 0.09765313, 0.25246365,\n",
       "       0.3186096 , 0.        , 0.        , 0.45735877, 0.68128761,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = td_prediction(env, pi, num_episodes=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03105021, 0.0202269 , 0.04051933, 0.02002985, 0.04384035,\n",
       "       0.        , 0.08993763, 0.        , 0.08579095, 0.21719199,\n",
       "       0.27030225, 0.        , 0.        , 0.39716272, 0.62556548,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
