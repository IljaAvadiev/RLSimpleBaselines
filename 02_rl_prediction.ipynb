{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from itertools import count\n",
    "from collections import deque\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# utility functions from local folder\n",
    "from utils.helper import create_random_policy\n",
    "from utils.helper import print_state_value_func, print_policy\n",
    "from utils.helper import generate_episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Visit Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_fv_predict(env, pi, alpha=0.001, gamma=0.99, num_episodes=10000):\n",
    "    '''\n",
    "    Calculates the state value function of a policy pi using first visit monte carlo\n",
    "    \n",
    "    Args:\n",
    "        env:           OpenAI gym environment to interract with\n",
    "        pi:            Policy that is used to select actions\n",
    "        alpha:         Learning rate\n",
    "        gamma:         Discounting rate\n",
    "        num_episodes:  Number of episodes to play\n",
    "    \n",
    "    Returns:\n",
    "        V: Value function\n",
    "    '''\n",
    "    nS = env.observation_space.n\n",
    "    V = np.zeros(nS, dtype=np.float64)\n",
    "    \n",
    "    for episode in tqdm(range(num_episodes)):\n",
    "        generated_episode = generate_episode(env, pi)\n",
    "        visited = np.zeros_like(V, dtype=np.bool)\n",
    "    \n",
    "        for t, (obs, _, _, _, _) in enumerate(generated_episode):\n",
    "            if visited[obs] == True:\n",
    "                continue\n",
    "            visited[obs] = True\n",
    "            \n",
    "            remaining_rewards = np.array(generated_episode, dtype=np.object)[t:, 2]\n",
    "            remaining_steps = len(remaining_rewards)\n",
    "            \n",
    "            # calculate discounting\n",
    "            bases = [gamma for _ in range(remaining_steps)]\n",
    "            exponents = [x for x in range(remaining_steps)]\n",
    "            discounts = np.power(bases, exponents)\n",
    "            \n",
    "            #calculate discounted return\n",
    "            mc_target = np.sum(remaining_rewards * discounts)\n",
    "            \n",
    "            #update value function\n",
    "            V[obs] = V[obs] + alpha * (mc_target - V[obs])\n",
    "            \n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD (Temporal Difference) Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_predict(env, pi, alpha=0.001, gamma=0.99, num_episodes=10000):\n",
    "    '''\n",
    "    Calculates the state value function of a policy pi using temporal difference\n",
    "    \n",
    "    Args:\n",
    "        env:           OpenAI gym environment to interract with\n",
    "        pi:            Policy that is used to select actions\n",
    "        alpha:         Learning rate\n",
    "        gamma:         Discounting rate\n",
    "        num_episodes:  Number of episodes to play\n",
    "    \n",
    "    Returns:\n",
    "        V: Value function\n",
    "    '''\n",
    "    nS = env.observation_space.n\n",
    "    V = np.zeros(nS, dtype=np.float64)\n",
    "    \n",
    "    for episode in tqdm(range(num_episodes)):\n",
    "        obs, done = env.reset(), False\n",
    "        while not done:\n",
    "            action = pi(obs)\n",
    "            next_obs, reward, done, _ = env.step(action)\n",
    "            V[obs] = V[obs] + alpha * (reward + gamma * V[next_obs] * (not done) - V[obs])\n",
    "            obs = next_obs\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n - step Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_step_predict(env, pi, n=2, alpha=0.001, gamma=0.99, num_episodes=10000):\n",
    "    '''\n",
    "    Calculates the state value function of a policy pi using n_step temporal difference\n",
    "    \n",
    "    Args:\n",
    "        env:           OpenAI gym environment to interract with\n",
    "        pi:            Policy that is used to select actions\n",
    "        n:             Number of steps (intermediate value between TD and full Monte Carlo)\n",
    "        alpha:         Learning rate\n",
    "        gamma:         Discounting rate\n",
    "        num_episodes:  Number of episodes to play\n",
    "    \n",
    "    Returns:\n",
    "        V: Value function\n",
    "    '''\n",
    "    \n",
    "    nS = env.observation_space.n\n",
    "    V = np.zeros(nS, dtype=np.float64)\n",
    "    \n",
    "    for episode in tqdm(range(num_episodes)):\n",
    "        experiences = deque(maxlen=n)\n",
    "        obs, done = env.reset(), False\n",
    "        for t in count():\n",
    "            if not done:\n",
    "                action = pi(obs)\n",
    "                next_obs, reward, done, _ = env.step(action)\n",
    "                experiences.append((obs, action, reward, next_obs, done))\n",
    "                obs = next_obs\n",
    "            \n",
    "            if t < n - 1 and not done:\n",
    "                continue\n",
    "            \n",
    "            adjust_state = experiences[0][0]\n",
    "            last_next_state = experiences[-1][3]\n",
    "            \n",
    "            #calculating the target value\n",
    "            target = 0\n",
    "            for i, (_, _, reward, _, _) in enumerate(experiences):\n",
    "                target += reward * gamma**(len(experiences)-i-1)\n",
    "            target += V[last_next_state] * gamma**len(experiences) * (not experiences[-1][4])\n",
    "            \n",
    "            experiences.popleft()\n",
    "                \n",
    "            V[adjust_state] = V[adjust_state] + alpha * (target - V[adjust_state])\n",
    "            \n",
    "            if len(experiences) > 0:\n",
    "                obs = experiences[-1][3]\n",
    "                done = experiences[-1][4]\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "            \n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD (Lambda) with eligibility traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_lambda_predict(env, pi, elig_lambda=0.5, alpha=0.001, gamma=0.99, num_episodes=10000):\n",
    "    \n",
    "    '''\n",
    "    Calculates the state value function of a policy pi using temporal difference with eligibility traces\n",
    "    \n",
    "    Args:\n",
    "        env:           OpenAI gym environment to interract with\n",
    "        pi:            Policy that is used to select actions\n",
    "        elig_lambda:   Factor for discounting eligibility traces\n",
    "        alpha:         Learning rate\n",
    "        gamma:         Discounting rate\n",
    "        num_episodes:  Number of episodes to play\n",
    "    \n",
    "    Returns:\n",
    "        V: Value function\n",
    "    '''\n",
    "    nS = env.observation_space.n\n",
    "    V = np.zeros(nS, dtype=np.float64)\n",
    "    \n",
    "    for episode in tqdm(range(num_episodes)):\n",
    "        obs, done = env.reset(), False\n",
    "        eligibility_traces = np.zeros(nS, dtype=np.float64)\n",
    "        while not done:\n",
    "            eligibility_traces[obs] += 1\n",
    "            action = pi(obs)\n",
    "            next_obs, reward, done, _ = env.step(action)\n",
    "            target = reward + gamma * V[next_obs] * (not done)\n",
    "            error = target - V[obs]\n",
    "            \n",
    "            V = V + alpha * error * eligibility_traces\n",
    "            eligibility_traces *= gamma * elig_lambda\n",
    "            obs = next_obs\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frozen Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nS = env.observation_space.n\n",
    "nA = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = create_random_policy(nS, nA, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1mRandom Policy\u001b[0m\n",
      "\n",
      "\n",
      "         ↑          ↑          ←          →\n",
      "         ↑          ■          →          ■\n",
      "         →          ↓          ↓          ■\n",
      "         ■          ←          →          ■\n"
     ]
    }
   ],
   "source": [
    "print_policy(pi, nS, 4, name='Random Policy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13988adb6d844aa797f7c2b7e9f3e28d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "V = mc_fv_predict(env, pi, num_episodes=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09235 0.09504 0.10056 0.00000\n",
      "0.00885 0.00000 0.10983 0.00000\n",
      "0.03353 0.10363 0.24142 0.00000\n",
      "0.00000 0.03315 0.53212 0.00000\n"
     ]
    }
   ],
   "source": [
    "print_state_value_func(V, 4, name='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f404d34df14644c99123f3b491a2a166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "V = td_predict(env, pi, num_episodes=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04787 0.05113 0.05820 0.00000\n",
      "0.00334 0.00000 0.07137 0.00000\n",
      "0.00543 0.04562 0.16876 0.00000\n",
      "0.00000 0.00776 0.51077 0.00000\n"
     ]
    }
   ],
   "source": [
    "print_state_value_func(V, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ac3d7f7b2ea40b09ec69a419a54c358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "V= n_step_predict(env, pi, n=3, num_episodes=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09234 0.09566 0.10431 0.00000\n",
      "0.01162 0.00000 0.11249 0.00000\n",
      "0.02423 0.09897 0.24186 0.00000\n",
      "0.00000 0.03344 0.60675 0.00000\n"
     ]
    }
   ],
   "source": [
    "print_state_value_func(V, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7877d98d761e4c6ebf4d7258eff97a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "V = td_lambda_predict(env, pi, elig_lambda=.5, num_episodes=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08022 0.08210 0.08663 0.00000\n",
      "0.00921 0.00000 0.09414 0.00000\n",
      "0.01747 0.07470 0.20643 0.00000\n",
      "0.00000 0.02157 0.57060 0.00000\n"
     ]
    }
   ],
   "source": [
    "print_state_value_func(V, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
