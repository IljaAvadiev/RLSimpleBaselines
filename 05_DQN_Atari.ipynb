{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network: Atari Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from itertools import count\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import Wrapper, ObservationWrapper, RewardWrapper\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "class MaxFrame(ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(MaxFrame, self).__init__(env)\n",
    "        self.frames = deque(maxlen=2)\n",
    "\n",
    "    def reset(self):\n",
    "        observation = self.env.reset()\n",
    "        if self.env.unwrapped.get_action_meanings()[1] == 'FIRE':\n",
    "            obs, _, _, _ = self.env.step(1)\n",
    "        for _ in range(2):\n",
    "            self.frames.append(np.zeros(observation.shape))\n",
    "        return observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        self.frames.append(observation)\n",
    "        max_frames_values = np.maximum(self.frames[0], self.frames[1])\n",
    "        return max_frames_values\n",
    "\n",
    "\n",
    "# repeat action\n",
    "# frame skipping\n",
    "class RepeatAction(Wrapper):\n",
    "    def __init__(self, env, repeat=4):\n",
    "        super(RepeatAction, self).__init__(env)\n",
    "        self.repeat = repeat\n",
    "\n",
    "        # to trigger done when you lose a life\n",
    "        self.ale = env.unwrapped.ale\n",
    "        self.lives = 0\n",
    "\n",
    "    def step(self, action):\n",
    "        sum_reward = 0\n",
    "        for _ in range(self.repeat):\n",
    "            observation, reward, done, info = self.env.step(action)\n",
    "            sum_reward += reward\n",
    "\n",
    "            # if you lose a life trigger done\n",
    "            new_lives = self.ale.lives()\n",
    "            done = done or new_lives < self.lives\n",
    "            self.lives = new_lives\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        return observation, sum_reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        observation = self.env.reset()\n",
    "        self.lives = self.ale.lives()\n",
    "        return observation\n",
    "\n",
    "\n",
    "# remove y channel\n",
    "# rescale to size between 0 and 1\n",
    "# and rescale to 84x84\n",
    "class PreprocessImage(ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super(PreprocessImage, self).__init__(env)\n",
    "\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            0.0, 1.0, shape=shape, dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = cv2.cvtColor(observation, cv2.COLOR_RGB2GRAY)\n",
    "        observation = cv2.resize(\n",
    "            observation, self.observation_space.shape[1:], interpolation=cv2.INTER_AREA)\n",
    "        observation = observation.reshape(\n",
    "            self.observation_space.shape).astype('float32')\n",
    "        observation = observation / self.observation_space.high\n",
    "        return observation\n",
    "\n",
    "\n",
    "# stack n frames (4 was used)\n",
    "class StackFrames(ObservationWrapper):\n",
    "    def __init__(self, env, maxlen=4):\n",
    "        super(StackFrames, self).__init__(env)\n",
    "        self.maxlen = maxlen\n",
    "        self.frames = deque(maxlen=maxlen)\n",
    "        low = self.env.observation_space.low.repeat(maxlen, axis=0)\n",
    "        high = self.env.observation_space.high.repeat(maxlen, axis=0)\n",
    "        dtype = self.env.observation_space.dtype\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=low, high=high, dtype=dtype)\n",
    "\n",
    "    def reset(self):\n",
    "        observation = self.env.reset()\n",
    "        for _ in range(self.maxlen):\n",
    "            self.frames.append(observation)\n",
    "\n",
    "        observation = np.vstack(self.frames)\n",
    "        return observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        self.frames.append(observation)\n",
    "        observation = np.vstack(self.frames)\n",
    "        return observation\n",
    "\n",
    "\n",
    "def apply_wrappers(env):\n",
    "    env = MaxFrame(env)\n",
    "    env = RepeatAction(env)\n",
    "    env = PreprocessImage(env, shape=(1, 88, 88))\n",
    "    env = StackFrames(env)\n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replay buffer for memory replay\n",
    "class ReplayBuffer():\n",
    "    '''\n",
    "    Stores memories up to a maximum of mem_size. The memories can be batched to use in training.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, input_size, mem_size=10000, batch_size=64):\n",
    "        self.mem_size = mem_size\n",
    "        self.index = 0\n",
    "        self.batch_size=batch_size\n",
    "        \n",
    "        self.obs_memory = np.empty((self.mem_size, *input_size), dtype=np.float32)\n",
    "        self.action_memory = np.empty((self.mem_size), dtype=np.int64)\n",
    "        self.reward_memory = np.empty((self.mem_size), dtype=np.float32)\n",
    "        self.next_obs_memory = np.empty((self.mem_size, *input_size), dtype=np.float32)\n",
    "        self.terminal_memory = np.empty((self.mem_size), dtype=np.bool)\n",
    "        \n",
    "    def add_memory(self, obs, action, reward, next_obs, done):\n",
    "        self.obs_memory[self.index] = obs\n",
    "        self.action_memory[self.index] = action\n",
    "        self.reward_memory[self.index] = reward\n",
    "        self.next_obs_memory[self.index] = next_obs\n",
    "        self.terminal_memory[self.index] = done\n",
    "        self.index += 1\n",
    "        self.index %= self.mem_size\n",
    "\n",
    "    def get_memory_batch(self):\n",
    "        idxs = np.random.choice(len(self), self.batch_size, replace=False)\n",
    "        \n",
    "        obss = self.obs_memory[idxs]\n",
    "        actions = self.action_memory[idxs]\n",
    "        rewards = self.reward_memory[idxs]\n",
    "        next_obss = self.next_obs_memory[idxs]\n",
    "        dones = self.terminal_memory[idxs]\n",
    "        \n",
    "        return obss, actions, rewards, next_obss, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return min(self.index, self.mem_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function approximator of the Q-Function\n",
    "class CNN_QNN(nn.Module):\n",
    "    '''\n",
    "    A PyTorch based neural network approximator of the Q-function.\n",
    "    The network can include a flexible number of input, hidden and output nodes.\n",
    "    The model assumes images (e.g. atari screen) as input\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, input_size, action_size, lr, save_dir, name):\n",
    "        super(CNN_QNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_file = os.path.join(save_dir, name)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(input_size[0], 32, 8, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, 2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, 1)\n",
    "        \n",
    "        fc_dims = self.num_flat_features(input_size)\n",
    "        \n",
    "        self.fc1 = nn.Linear(fc_dims, 512)\n",
    "        self.fc2 = nn.Linear(512, action_size)\n",
    "        \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.loss = nn.MSELoss()\n",
    "        self.optimizer = optim.RMSprop(self.parameters(), lr=lr)\n",
    "        self.to(self.device)\n",
    "    \n",
    "    def num_flat_features(self, input_size):\n",
    "        x = torch.zeros(1, *input_size)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        shape = x.shape[1:]\n",
    "        num_features = 1\n",
    "        for i in shape:\n",
    "            num_features *= i\n",
    "        return num_features\n",
    "    \n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def save(self):\n",
    "        torch.save(self.state_dict(), self.output_file)\n",
    "    \n",
    "    def load(self):\n",
    "        self.load_state_dict(torch.load(self.output_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    '''\n",
    "    DQN Agent combining a memory buffer and separate online and target neural networks\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, input_size, action_size, min_epsilon, max_epsilon, epsilon_decay, \n",
    "                 gamma, lr, mem_size, batch_size, save_dir, name):\n",
    "        self.replay_buffer = ReplayBuffer(input_size, mem_size, batch_size)\n",
    "        self.qnn_target = CNN_QNN(input_size, action_size, lr, save_dir, name=name+'_target.pt')\n",
    "        self.qnn_online = CNN_QNN(input_size, action_size, lr, save_dir, name=name+'_online.pt')\n",
    "        self.replace_target_network()\n",
    "        \n",
    "        self.epsilon = max_epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.gamma = gamma\n",
    "        self.action_size = action_size\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def epsilon_greedy(self, obs):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            action = self.greedy(obs)\n",
    "        else:\n",
    "            action = np.random.choice(self.action_size)\n",
    "        return action\n",
    "    \n",
    "    def greedy(self, obs):\n",
    "        with torch.no_grad():\n",
    "            obs = np.expand_dims(obs, axis=0)\n",
    "            obs = torch.from_numpy(obs).to(self.qnn_online.device).float()\n",
    "            action = np.argmax(self.qnn_online.forward(obs).detach().cpu().numpy())\n",
    "            return action\n",
    "    \n",
    "    def decrement_epsilon(self):\n",
    "        if self.epsilon <= self.min_epsilon:\n",
    "            return\n",
    "        \n",
    "        epsilon = self.epsilon - self.epsilon_decay\n",
    "        self.epsilon = max(epsilon, self.min_epsilon)\n",
    "    \n",
    "    def add_memory(self, obs, action, reward, next_obs, done):\n",
    "        self.replay_buffer.add_memory(obs, action, reward, next_obs, done)\n",
    "    \n",
    "    def get_memory_batch(self):\n",
    "        obss, actions, rewards, next_obss, dones = self.replay_buffer.get_memory_batch()\n",
    "        device = self.qnn_online.device\n",
    "        obss = torch.from_numpy(obss).to(device)\n",
    "        actions = torch.from_numpy(actions).to(device)\n",
    "        rewards = torch.from_numpy(rewards).to(device)\n",
    "        next_obss = torch.from_numpy(next_obss).to(device)\n",
    "        dones = torch.from_numpy(dones).to(device)\n",
    "        return obss, actions, rewards, next_obss, dones\n",
    "        \n",
    "    def learn(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        self.qnn_online.optimizer.zero_grad()\n",
    "        obss, actions, rewards, next_obss, dones = self.get_memory_batch()\n",
    "        with torch.no_grad():\n",
    "            target = rewards + self.gamma * torch.max(self.qnn_target.forward(next_obss).detach(), dim=1)[0] \\\n",
    "                * torch.logical_not(dones)\n",
    "        target = target.unsqueeze(1)\n",
    "        online = self.qnn_online.forward(obss).gather(dim=1, index=actions.unsqueeze(1))\n",
    "\n",
    "        loss = self.qnn_online.loss(online, target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.qnn_online.parameters(), 1.0)\n",
    "        \n",
    "        self.qnn_online.optimizer.step()\n",
    "        self.decrement_epsilon()\n",
    "    \n",
    "    def save(self):\n",
    "        self.qnn_online.save()\n",
    "    \n",
    "    def load(self):\n",
    "        self.qnn_online.load()\n",
    "        \n",
    "    def replace_target_network(self):\n",
    "        self.qnn_target.load_state_dict(self.qnn_online.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVS = [\n",
    "    {\n",
    "        'NAME': 'PongNoFrameskip-v4',\n",
    "        'UNWRAPPED': False,\n",
    "        'SOLVED_REWARD': 15\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 500\n",
    "MIN_EPSILON=0.1\n",
    "MAX_EPSILON=1\n",
    "EPSILON_DECAY=1e-5\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.0001\n",
    "MEMORY_SIZE = 50000\n",
    "BATCH_SIZE = 32\n",
    "REPLACE_TARGET = 1000\n",
    "SAVE_DIR = './progress'\n",
    "RESULTS_DIR = './results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TRAINING ENVIRONMENT BreakoutNoFrameskip-v4---\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a29051a79fd4ceba0eb6a60f536e049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5est_mean: 0.1, current_mean: 0.1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-b68bd7aef842>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_obs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mREPLACE_TARGET\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace_target_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-1ff91399ee3c>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mobss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_obss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqnn_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_obss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m                 \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_not\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-2c2a8704b5f2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/01_programming/04_rl/github/ReinforcementLearning/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/01_programming/04_rl/github/ReinforcementLearning/venv/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/01_programming/04_rl/github/ReinforcementLearning/venv/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    417\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 419\u001b[0;31m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0m\u001b[1;32m    420\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Main loop\n",
    "for env in ENVS:\n",
    "\n",
    "    NAME = env['NAME']\n",
    "    UNWRAPPED = env['UNWRAPPED']\n",
    "    SOLVED_REWARD = env['SOLVED_REWARD']\n",
    "    \n",
    "    print(f'--- TRAINING ENVIRONMENT {NAME}---\\n')\n",
    "    \n",
    "    if UNWRAPPED:  \n",
    "        ENV = gym.make(NAME).unwrapped\n",
    "    else: \n",
    "        ENV = gym.make(NAME)\n",
    "    ENV = apply_wrappers(ENV)\n",
    "    ACTION_SIZE = ENV.action_space.n\n",
    "    INPUT_SIZE = ENV.observation_space.shape\n",
    "\n",
    "    agent = DQN(INPUT_SIZE, ACTION_SIZE, \n",
    "                min_epsilon=MIN_EPSILON, max_epsilon=MAX_EPSILON, epsilon_decay=EPSILON_DECAY,\n",
    "                gamma=GAMMA, lr=LEARNING_RATE, \n",
    "                mem_size=MEMORY_SIZE, batch_size=BATCH_SIZE, \n",
    "                save_dir=SAVE_DIR, name=NAME)\n",
    "\n",
    "\n",
    "    reward_tracking = []\n",
    "    best_mean = -1000\n",
    "    reward_mean = -1000\n",
    "    counter = 0\n",
    "    for episode in tqdm(range(EPISODES)):\n",
    "        obs, done = ENV.reset(), False\n",
    "        reward_sum = 0\n",
    "        while not done:\n",
    "            action = agent.epsilon_greedy(obs)\n",
    "            next_obs, reward, done, info = ENV.step(action)\n",
    "#             is_truncated = 'TimeLimit.truncated' in info and info['TimeLimit.truncated']\n",
    "            terminal = done and (not is_truncated)\n",
    "            reward_sum += reward\n",
    "            agent.add_memory(obs, action, reward, next_obs, terminal)\n",
    "            obs = next_obs\n",
    "            agent.learn()\n",
    "            if(counter + 1) % REPLACE_TARGET == 0:\n",
    "                agent.replace_target_network()\n",
    "                counter = 0\n",
    "            counter += 1\n",
    "            ENV.render()\n",
    "        reward_tracking.append(reward_sum)\n",
    "\n",
    "        # OUTPUT INFO\n",
    "        if (episode > 10):\n",
    "            reward_mean = np.array(reward_tracking[-10:]).mean()\n",
    "            if reward_mean > best_mean:\n",
    "                best_mean = reward_mean\n",
    "                agent.save()\n",
    "\n",
    "            print(f'best_mean: {best_mean}, current_mean: {reward_mean}', end='\\r')\n",
    "\n",
    "            if best_mean >= SOLVED_REWARD:\n",
    "                print('\\n', flush=True)\n",
    "                print(f'---GOAL REACHED AFTER {episode} EPISODES---')\n",
    "                print('\\n')\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for env in ENVS:\n",
    "#     frames = []\n",
    "    \n",
    "    NAME = env['NAME']\n",
    "    \n",
    "    ENV = gym.make(NAME)\n",
    "    ENV = apply_wrappers(ENV)\n",
    "    ACTION_SIZE = ENV.action_space.n\n",
    "    INPUT_SIZE = ENV.observation_space.shape\n",
    "    \n",
    "    agent = DQN(INPUT_SIZE, ACTION_SIZE, \n",
    "                min_epsilon=MIN_EPSILON, max_epsilon=MAX_EPSILON, epsilon_decay=EPSILON_DECAY,\n",
    "                gamma=GAMMA, lr=LEARNING_RATE, \n",
    "                mem_size=MEMORY_SIZE, batch_size=BATCH_SIZE, \n",
    "                save_dir=SAVE_DIR, name=NAME)\n",
    "    agent.load()\n",
    "    \n",
    "\n",
    "    obs, done = ENV.reset(), False\n",
    "    \n",
    "#     frames.append(Image.fromarray(ENV.render(mode='rgb_array')))\n",
    "    eval_score = 0\n",
    "    count = 0\n",
    "    while not done:\n",
    "        action = agent.greedy(obs)\n",
    "        next_obs, reward, done, _ = ENV.step(action)\n",
    "        ENV.render()\n",
    "        eval_score += reward\n",
    "        obs = next_obs\n",
    "        count += 1\n",
    "        print(count, end='\\r')\n",
    "#         frames.append(Image.fromarray(ENV.render(mode='rgb_array')))\n",
    "    ENV.close()\n",
    "    \n",
    "#     path = os.path.join(RESULTS_DIR, NAME+'.gif')\n",
    "#     with open(path, 'wb') as f:\n",
    "#         im = Image.new('RGB', frames[0].size)\n",
    "#         im.save(f, save_all=True, append_images=frames, loop=0, duration=25) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
