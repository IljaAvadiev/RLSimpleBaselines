{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcment Learning Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(Q, state, epsilon):\n",
    "    '''\n",
    "    Selects an action using epsillon-greedy policy\n",
    "    \n",
    "    Args: \n",
    "        Q:       Action Value function of a policy pi\n",
    "        state:   State in the MDP context\n",
    "        epsilon: Probability with which a random action is selected\n",
    "        \n",
    "    Returns:\n",
    "        Action\n",
    "    '''\n",
    "    \n",
    "    if np.random.rand() > epsilon:\n",
    "        action = np.argmax(Q[state])\n",
    "    else:\n",
    "        action = np.random.randint(len(Q[state]))\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(env, pi, Q, epsilon):\n",
    "    '''\n",
    "    Generates an episode following a policy pi.\n",
    "    An episode is a succession of experiences until the terminal state. \n",
    "    A tuple of state, action, reward, next_state and done constitute an exprience.\n",
    "    \n",
    "    Args: \n",
    "        env: OpenAI gym environment to interract with\n",
    "        pi:  Policy that is used to create an episode\n",
    "    \n",
    "    Returns:\n",
    "        episode\n",
    "    '''\n",
    "    episode = []\n",
    "    obs, done = env.reset(), False\n",
    "    while not done:\n",
    "        action = pi(Q, obs, epsilon)\n",
    "        new_obs, reward, done, _ = env.step(action)\n",
    "        experience = (obs, action, reward, new_obs, done)\n",
    "        episode.append(experience)\n",
    "        obs = new_obs\n",
    "    \n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_decay(start_value, end_value, decay_duration, full_duration):\n",
    "    '''\n",
    "    Generates a list of float values that decrease from a start_value to the end_value in a linear fashion\n",
    "    \n",
    "    Args:\n",
    "        start_value:      The starting value for a float value\n",
    "        end_value:        The lowest value that is allowed\n",
    "        decay_duration:   Number of episodes the value is decreasing\n",
    "        full_duration:    Number of episodes the value is needed\n",
    "        \n",
    "    Returns:\n",
    "        decay_values:     Numpy array of decaying values\n",
    "    '''\n",
    "    \n",
    "    decay_values = np.arange(decay_duration)[::-1]\n",
    "    decay_values = decay_values / np.max(decay_values)\n",
    "    \n",
    "    values_range = start_value - end_value\n",
    "    decay_values = decay_values * values_range + end_value\n",
    "    \n",
    "    decay_values = np.pad(decay_values, (0, full_duration - decay_duration), mode='edge')\n",
    "    \n",
    "    return decay_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moncte Carlo First Visit Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_fv_control(env, eps_start, eps_end, alpha_start, alpha_end, decay_duration, gamma=0.99, num_episodes=20000):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    nS = env.observation_space.n\n",
    "    nA = env.action_space.n\n",
    "    epsilons = linear_decay(eps_start, eps_end, decay_duration, num_episodes)\n",
    "    alphas = linear_decay(alpha_start, alpha_end, decay_duration, num_episodes)\n",
    "    \n",
    "    Q = np.zeros((nS, nA), dtype=np.float32)\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        visited = np.zeros_like(Q, dtype=np.bool)\n",
    "        generated_episode = generate_episode(env, select_action, Q, epsilons[episode])\n",
    "        \n",
    "        for t, (obs, action, _, _, _) in enumerate(generated_episode):\n",
    "            if visited[obs][action] == True:\n",
    "                continue\n",
    "            visited[obs][action] = True\n",
    "            \n",
    "            remaining_rewards = np.array(generated_episode, dtype=np.object)[t:, 2]\n",
    "            remaining_steps = len(remaining_rewards)\n",
    "            \n",
    "            # calculate discounting\n",
    "            bases = [gamma for _ in range(remaining_steps)]\n",
    "            exponents = [x for x in range(remaining_steps)]\n",
    "            discounts = np.power(bases, exponents)\n",
    "            \n",
    "            #calculate discounted return\n",
    "            mc_target = np.sum(remaining_rewards * discounts)\n",
    "            \n",
    "            \n",
    "            Q[obs][action] = Q[obs][action] + alphas[episode] * (mc_target - Q[obs][action])\n",
    "    V = np.max(Q, axis=1)      \n",
    "    return Q, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA (TD On Policy learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env, eps_start, eps_end, alpha_start, alpha_end, decay_duration, gamma=0.99, num_episodes=20000):\n",
    "    '''\n",
    "    '''\n",
    "    nS = env.observation_space.n\n",
    "    nA = env.action_space.n\n",
    "    epsilons = linear_decay(eps_start, eps_end, decay_duration, num_episodes)\n",
    "    alphas = linear_decay(alpha_start, alpha_end, decay_duration, num_episodes)\n",
    "    \n",
    "    Q = np.zeros((nS, nA), dtype=np.float32)\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        obs, done = env.reset(), False\n",
    "        action = select_action(Q, obs, epsilons[episode])\n",
    "        while not done:\n",
    "            next_obs, reward, done, _ = env.step(action)\n",
    "            next_action = select_action(Q, next_obs, epsilons[episode])\n",
    "            sarsa_target = reward + gamma * Q[next_obs][next_action] * (not done)\n",
    "            Q[obs][action] = Q[obs][action] + alphas[episode] * (sarsa_target - Q[obs][action])\n",
    "            obs, action = next_obs, next_action\n",
    "    V = np.max(Q, axis=1)\n",
    "    return Q, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning (TD Off Policy learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, eps_start, eps_end, alpha_start, alpha_end, decay_duration, gamma=0.99, num_episodes=20000):\n",
    "    '''\n",
    "    '''\n",
    "    nS = env.observation_space.n\n",
    "    nA = env.action_space.n\n",
    "    epsilons = linear_decay(eps_start, eps_end, decay_duration, num_episodes)\n",
    "    alphas = linear_decay(alpha_start, alpha_end, decay_duration, num_episodes)\n",
    "    \n",
    "    Q = np.zeros((nS, nA), dtype=np.float32)\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        obs, done = env.reset(), False\n",
    "        while not done:\n",
    "            action = select_action(Q, obs, epsilons[episode])\n",
    "            next_obs, reward, done, _ = env.step(action)\n",
    "            next_action = np.argmax(Q[next_obs])\n",
    "            sarsa_target = reward + gamma * Q[next_obs][next_action] * (not done)\n",
    "            Q[obs][action] = Q[obs][action] + alphas[episode] * (sarsa_target - Q[obs][action])\n",
    "            obs = next_obs\n",
    "    V = np.max(Q, axis=1)\n",
    "    return Q, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double Q-Learning (reducing maximization bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_q_learning(env, eps_start, eps_end, alpha_start, alpha_end, decay_duration, gamma=0.99, num_episodes=20000):\n",
    "    '''\n",
    "    '''\n",
    "    nS = env.observation_space.n\n",
    "    nA = env.action_space.n\n",
    "    epsilons = linear_decay(eps_start, eps_end, decay_duration, num_episodes)\n",
    "    alphas = linear_decay(alpha_start, alpha_end, decay_duration, num_episodes)\n",
    " \n",
    "    Q_1 = np.zeros((nS, nA), dtype=np.float32)\n",
    "    Q_2 = np.zeros((nS, nA), dtype=np.float32)\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        obs, done = env.reset(), False\n",
    "        while not done:\n",
    "            action = select_action((Q_1 +  Q_2) / 2, obs, epsilons[episode])\n",
    "            next_obs, reward, done, _ = env.step(action)\n",
    "            if np.random.random() < 0.5:\n",
    "                next_action = np.argmax(Q_1[next_obs])\n",
    "                sarsa_target = reward + gamma * Q_2[next_obs][next_action] * (not done)\n",
    "                Q_1[obs][action] = Q_1[obs][action] + alphas[episode] * (sarsa_target - Q_1[obs][action])\n",
    "                obs = next_obs\n",
    "            else:\n",
    "                next_action = np.argmax(Q_2[next_obs])\n",
    "                sarsa_target = reward + gamma * Q_1[next_obs][next_action] * (not done)\n",
    "                Q_2[obs][action] = Q_2[obs][action] + alphas[episode] * (sarsa_target - Q_2[obs][action])\n",
    "                obs = next_obs\n",
    "    Q = (Q_1 + Q_2) / 2\n",
    "    V = np.max(Q, axis=1)\n",
    "    return Q, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma=0.99, delta=1e-10):\n",
    "    '''\n",
    "    Finds an optimal policy\n",
    "    \n",
    "    Args: \n",
    "        env:     openai gym environment\n",
    "        gamma:   discount factor\n",
    "        delta:   threshhold value to interrupt the policy evaluation\n",
    "    \n",
    "    Returns: \n",
    "        Optimal policy and value function\n",
    "    ''' \n",
    "    nS = env.observation_space.n\n",
    "    nA = env.action_space.n\n",
    "    V = np.zeros(nS, dtype=np.float64)\n",
    "    P = env.env.P\n",
    "    \n",
    "    while True:\n",
    "        V_old = V.copy()\n",
    "        Q = np.zeros(shape=(nS, nA), dtype=np.float64)\n",
    "        for state in range(nS):\n",
    "            for action in range(nA):\n",
    "                for pr, next_state, reward, done in P[state][action]:\n",
    "                    Q[state][action] += pr * (reward + gamma * V[next_state] * (not done))\n",
    "        V = np.max(Q, axis=1)\n",
    "        max_diff = np.max(np.abs(V_old - V))\n",
    "        if max_diff < delta:\n",
    "            break\n",
    "\n",
    "    strategy = {s: a for s, a in enumerate(np.argmax(Q, axis=1))}\n",
    "    def pi(s):\n",
    "        return strategy[s]\n",
    "    \n",
    "    return pi, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, V = value_iteration(env, gamma=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.54202593 0.49880319 0.47069569 0.4568517 ]\n",
      " [0.55845096 0.         0.35834807 0.        ]\n",
      " [0.59179874 0.64307982 0.61520756 0.        ]\n",
      " [0.         0.74172044 0.86283743 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(V.reshape(4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "num_episodes = 10000\n",
    "decay_duration = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, V = mc_fv_control(env, eps_start=1.0, eps_end=0.01, alpha_start=0.05, alpha_end=0.01, \\\n",
    "                     decay_duration=decay_duration, num_episodes=num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.48527676 0.2907662  0.19433592 0.08634876]\n",
      " [0.4999712  0.         0.2172153  0.        ]\n",
      " [0.52989995 0.57752734 0.5292619  0.        ]\n",
      " [0.         0.684992   0.8154243  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(V.reshape(4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, V = sarsa(env, eps_start=1.0, eps_end=0.01, alpha_start=0.05, alpha_end=0.01, \\\n",
    "             decay_duration=decay_duration, num_episodes=num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.38965628 0.3313999  0.2796144  0.03633502]\n",
      " [0.40628433 0.         0.2583913  0.        ]\n",
      " [0.4462176  0.5085071  0.49611053 0.        ]\n",
      " [0.         0.6055947  0.7751972  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(V.reshape(4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, V = q_learning(env, eps_start=1.0, eps_end=0.01, alpha_start=0.5, alpha_end=0.01, \\\n",
    "                  decay_duration=decay_duration, num_episodes=num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5226891  0.46078998 0.42943972 0.41381168]\n",
      " [0.54267126 0.         0.31804314 0.        ]\n",
      " [0.57574356 0.62868685 0.59484756 0.        ]\n",
      " [0.         0.73314536 0.84061    0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(V.reshape(4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, V = double_q_learning(env, eps_start=1.0, eps_end=0.01, alpha_start=0.5, alpha_end=0.01, \\\n",
    "                         decay_duration=decay_duration, num_episodes=num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49951607 0.43738    0.39781386 0.3816498 ]\n",
      " [0.5215821  0.         0.32810134 0.        ]\n",
      " [0.5581316  0.61367244 0.5924568  0.        ]\n",
      " [0.         0.7297714  0.856019   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(V.reshape(4, 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
