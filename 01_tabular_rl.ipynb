{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular Methods - Dynamic Programming and Tabular RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "\n",
    "from algorithms.tabular.dynamic_programming import policy_iteration, value_iteration\n",
    "from algorithms.tabular.monte_carlo import MCAgent\n",
    "from algorithms.tabular.td import SarsaAgent, QAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "model = env.env.P\n",
    "nS = env.observation_space.n\n",
    "nA = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_policy(nS, nA, seed=42):\n",
    "    '''\n",
    "    Generates a random policy for a gridworld\n",
    "    Args:\n",
    "        nS:     Number of states in a gridworld\n",
    "        nA:     Number of actions in a state\n",
    "        seed:   Seed for the random module\n",
    "    Returns:\n",
    "        random_policy: A function generating an action given a state\n",
    "    '''\n",
    "\n",
    "    random.seed(seed)\n",
    "    policy = {state: random.randint(0, nA-1) for state in range(nS)}\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = create_random_policy(nS, nA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "?value_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "V, pi = policy_iteration(pi, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "V, pi = value_iteration(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.54185998, 0.49858161, 0.47043461, 0.45657012, 0.55829709,\n",
       "        0.        , 0.35822941, 0.        , 0.59166815, 0.64298202,\n",
       "        0.6151213 , 0.        , 0.        , 0.74165099, 0.86280139,\n",
       "        0.        ]),\n",
       " {0: 0,\n",
       "  1: 3,\n",
       "  2: 3,\n",
       "  3: 3,\n",
       "  4: 0,\n",
       "  5: 0,\n",
       "  6: 0,\n",
       "  7: 0,\n",
       "  8: 3,\n",
       "  9: 1,\n",
       "  10: 0,\n",
       "  11: 0,\n",
       "  12: 0,\n",
       "  13: 2,\n",
       "  14: 1,\n",
       "  15: 0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Difference Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
